,"id, table_id",table_body,caption,references
0,S5.T1, Avg score Avg accuracy Method over scenes over pairs Zhang et al 31 blind 63 33 0 00 with global image features 71 03 23 13 with attention based image features 74 65 34 73 Graph VQA full model 74 94 39 1 1 Question no parsing graph with previous next edges 37 9 2 Question word embedding not pretrained 33 8 3 Scene no edge features e i j S superscript subscript e i j superscript S e ij prime mathsf S 1 1 1 36 8 4 Graph processing disabled for question x i Q subscript superscript x superscript Q i x prime prime mathsf Q i x i S subscript superscript x superscript S i x prime mathsf S i 37 1 5 Graph processing disabled for scene x i S subscript superscript x superscript S i x prime prime mathsf S i x i Q subscript superscript x superscript Q i x prime mathsf Q i 37 0 6 Graph processing disabled for question scene 35 7 7 Graph processing only 1 iteration for question T Q superscript T Q T mathsf Q 1 39 0 8 Graph processing only 1 iteration for scene T S superscript T S T mathsf S 1 37 9 9 Graph processing only 1 iteration for question scene 39 1 10 Uniform matching weights a i j subscript a i j a ij 1 1 1 24 4 ,Table 1 Results on the test set of the balanced dataset 31 in percents using balanced versions of both training and test sets Numbered rows report accuracy over pairs of complementary scenes for ablated versions of our method ,We compare our method against the three models proposed in 31 They all use an ensemble of models exploiting either an LSTM for processing the question or an elaborate set of hand designed rules to identify two objects as the focus of the question The visual features in the three models are respectively empty blind model global scene wide or focused on the two objects identified from the question These models are specifically designed for binary questions whereas ours is generally applicable Nevertheless we obtain significantly better accuracy than all three Table 1 Differences in performance are mostly visible in the pairs setting which we believe is more reliable as it discards ambiguous test questions on which human annotators disagreed We evaluated variants of our model to measure the impact of various design choices see numbered rows in Table 1 On the question side we evaluate row 1 our graph approach without syntactic parsing building question graphs with only two types of edges previous next and linking consecutive nodes This shows the advantage of using the graph method together with syntactic parsing Optimizing the word embeddings from scratch row 2 rather than from pretrained Glove vectors 21 produces a significant drop in performance On the scene side we removed the edge features row 3 by setting ei jS 1superscriptsubscripteijS1e ij mathsf S 1 It confirms that the model makes use of the spatial relations between objects encoded by the edges of the graph In rows 4 6 we disabled the recurrent graph processing xi xi subscriptsuperscriptx isubscriptsuperscriptx ix prime prime i x prime i for the either the question the scene or both We finally tested the model with uniform matching weights ai j 1subscriptaij1a ij 1 row 10 As expected it performed poorly Our weights act similarly to the attention mechanisms in other models e g 32 28 5 13 29 and our observations confirm that such mechanisms are crucial for good performance Number of recurrent iterations to update graph node representations TQsuperscriptTQT mathsf Q TSsuperscriptTST mathsf S 4 Anecdotally we observed that processing the scene graph benefits from more iterations than the question graph for which performance nearly saturates with 2 or more iterations As reported in the ablative evaluation Table 1 the use of at least a single iteration has a stronger influence than its exact number In Table 1 why is there a large improvement of the metric over balanced pairs of scenes but not of the metric over individual scenes 
1,S5.T2, Multiple choice Open ended Method Overall Yes no Other Number Overall Yes no Other Number LSTM blind 4 61 41 76 90 49 19 49 65 57 19 76 88 38 79 49 55 LSTM with global image features 4 69 21 77 46 66 65 52 90 65 02 77 45 56 41 52 54 Zhang et al 31 yes no only 35 25 79 14 35 25 79 14 Multimodal residual learning 14 67 99 79 08 61 99 52 57 62 56 79 10 48 90 51 60 U Tokyo MIL ensemble 23 1 71 18 79 59 67 93 56 19 69 73 80 70 62 08 58 82 Graph VQA full model 74 37 79 74 68 31 74 97 70 42 81 26 56 28 76 47 ,Table 2 Results on the test set of the abstract scenes dataset average scores in percents ,We report our results on the original abstract scenes dataset in Table 2 The evaluation is performed on an automated server that does not allow for an extensive ablative analysis Anecdotally performance on the validation set corroborates all findings presented above in particular the strong benefit of pre parsing pretrained word embeddings and graph processing with a GRU At the time of our submission our method occupies the top place on the leader board in both the open ended and multiple choice settings The advantage over existing method is most pronounced on the binary and the counting questions Refer to Fig 5 and to the supplementary for visualizations of the results 
2,S3.T1, Notation Meaning U U mathcal U of devices u i subscript u i u i i i i th device D i subscript D i D i Dataset of u i subscript u i u i c c c Server c k c k c k of base samples in the k k k th label at c c c p p p Maximum of blending samples per label N N N of target labels M M M of dummy labels n n n of samples per each target label m m m of samples in the remaining M dummy labels q c t a r g e t subscript superscript q t a r g e t c q target c Total required of samples at c c c q c u i t a r g e t subscript superscript q t a r g e t c subscript u i q target c u i of samples uploaded to c c c from u i subscript u i u i x u i t t a r g e t subscript superscript x t a r g e t subscript u i t x target u i t Raw sample of a target label at u i subscript u i u i x u i t d u m m y subscript superscript x d u m m y subscript u i t x dummy u i t Raw sample of a dummy label at u i subscript u i u i x c t d u m m y subscript superscript x d u m m y c t x dummy c t Base sample of a dummy label at c c c X u i c e n c subscript superscript X e n c subscript u i c X enc u i c Encoded sample at u i subscript u i u i transmitted to c c c X c d e c subscript superscript X d e c c X dec c Decoded sample from X u i c e n c subscript superscript X e n c subscript u i c X enc u i c at c c c ,Table 1 List of Notations,
3,S4.T2, Method 1 Target Label N 1 N 1 N 1 N 2 N 2 N 2 N 4 N 4 N 4 0 1 2 3 4 5 6 7 8 9 0 2 0 8 2 4 4 8 0 2 4 8 0 2 4 8 0 2 4 8 XorMixFL 94 44 95 83 95 28 93 99 94 72 94 17 94 58 93 43 94 65 91 54 92 36 91 03 92 61 91 28 88 48 MixFL 96 85 96 72 95 59 96 13 95 82 95 89 95 52 95 57 95 56 95 34 95 52 93 98 94 87 93 76 91 93 Vanilla FL 83 27 84 83 84 25 83 49 84 82 83 82 85 12 83 31 84 26 82 94 81 72 81 23 78 75 79 23 77 12 Standalone 89 12 88 62 89 34 91 13 89 83 88 31 89 28 90 11 91 22 87 43 86 41 88 16 87 19 86 58 84 82 ,Table 2 Test accuracy evaluation for different N 1 2 4 N124N in 1 2 4 p 1p1p 1 a 0 50 5 alpha 0 5 ,
4,S4.T3, Number of Averaging Samples p p p p 1 p 1 p 1 2 3 4 5 XorMixFL M 1 M 1 M 1 2116 97 335 85 2335 78 545 64 2520 88 687 77 2674 01 778 90 2781 09 838 49 M 2 M 2 M 2 2321 38 352 65 2409 72 643 56 2664 12 814 13 2821 37 900 01 2911 42 947 06 M 3 M 3 M 3 2406 62 365 61 2519 44 741 04 2800 24 895 33 2949 51 966 04 3030 43 1001 48 MixFL M 1 M 1 M 1 1248 46 156 20 1342 34 213 23 1382 11 311 44 1402 53 292 41 1496 32 349 25 M 2 M 2 M 2 1462 75 251 78 1523 10 320 13 1611 42 343 13 1683 45 391 39 1820 43 385 32 M 3 M 3 M 3 1521 13 225 39 1621 47 276 52 1850 32 403 72 1999 01 545 66 2095 57 634 79 ,Table 3 MDS comparison mean standard deviation between XorMixFL and MixFL target label 999 a 0 50 5 alpha 0 5 ,
5,S4.T4, 1 dummy label M 1 M 1 M 1 2 dummy labels M 2 M 2 M 2 3 dummy labels M 3 M 3 M 3 a alpha 0 25 0 5 0 95 0 25 0 5 0 95 0 25 0 5 0 95 XorMixFL p p p 1 Test Acc 92 33 92 54 92 05 95 29 95 10 94 47 91 60 91 51 90 82 Per label Acc 52 89 54 48 51 65 80 01 77 61 74 45 44 62 41 98 39 72 p p p 5 Test Acc 91 75 92 98 93 11 94 37 92 78 94 19 90 82 90 42 91 60 Per label Acc 46 16 56 55 60 28 69 59 56 23 68 95 34 92 33 42 43 68 MixFL p p p 1 Test Acc 94 49 94 49 95 96 87 85 90 65 91 98 87 44 90 31 91 86 Per label Acc 72 82 72 60 72 73 21 94 43 36 47 72 19 78 40 91 46 46 p p p 5 Test Acc 94 49 90 07 90 74 88 64 86 21 90 71 85 65 86 03 90 66 Per label Acc 42 08 32 82 35 67 26 83 24 71 34 76 15 42 24 30 34 72 ,Table 4 The average test accuracy of XorMixFL and MixFL when the number of dummy label and the value of a alpha for image blending procedure changes target label 999 dummy labels 333 and or 444 and or 555 ,
6,S6.T1, Metrics Real time After 2 hours After 4 hours After 6 hours GC LSTM 3 212 4 589 6 357 9 145 GRU 4 025 9 156 14 892 22 047 LSTM 4 217 9 635 15 068 21 624 AQNet 4 493 9 264 17 695 24 753 ,TABLE I Performance Comparsion of RMSE For GC LSTM GRU LSTM and AQNet,The results about RMSERMSE mathrm RMSE are presented in Table I which shows that GC LSTM s error is much smaller than other models This is because GC LSTM captures not only the spatial correlation of historical data on monitoring stations but also the temporal dependence between air quality data Moreover other models have ignored the topological correlation between the monitoring stations while we use this topological correlation to build a graph convolutional neural network in our work 
7,S2.E1, 120124 8290 Z 8290 119852 i m 8290 119852 i and Cov 8290 Z 8290 119852 i Z 8290 119852 j v 8290 119852 i 8290 v 8290 119852 j 8290 r 8290 h 8290 119852 i 119852 j formulae sequence 120124 delimited 119885 subscript 119852 119894 119898 subscript 119852 119894 and Cov 119885 subscript 119852 119894 119885 subscript 119852 119895 119907 subscript 119852 119894 119907 subscript 119852 119895 119903 8462 subscript 119852 119894 subscript 119852 119895 mathbbm E Z mathbf s i m mathbf s i quad text and quad mathrm Cov Z mathbf s i Z mathbf s j v mathbf s i v mathbf s j r big h mathbf s i mathbf s j big blackboard E italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic m bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and roman Cov italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic Z bold s start POSTSUBSCRIPT italic j end POSTSUBSCRIPT italic v bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic v bold s start POSTSUBSCRIPT italic j end POSTSUBSCRIPT italic r italic h bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold s start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 1 ,,Denote the HGP process defined in Equation 1 by HGP m v r HGPmvr textrm HGP m cdot v cdot r cdot HGP italic m italic v italic r The mean function m mm cdot italic m captures large scale variations usually through covariates The SD function v vv cdot italic v function accommodates both homoscedastic and hereroscedastic scenarios For instance with a log link and covariate w s wsw mathbf s italic w bold s at spatial unit ss mathbf s bold s a natural model is where z b subscriptzb mathbf z b bold z start POSTSUBSCRIPT italic b end POSTSUBSCRIPT is the bbbitalic b th MCMC sample of the spatial random effect and ms ssubscriptconditionalsuperscripts s mu mathbf s ast mid mathbf s italic m start POSTSUBSCRIPT bold s start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold s end POSTSUBSCRIPT and Ss ssubscriptSconditionalsuperscripts s Sigma mathbf s ast mid mathbf s roman S start POSTSUBSCRIPT bold s start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold s end POSTSUBSCRIPT are the conditional mean and covariance matrix induced by the HGP Equation 1 Next we sample y b subscriptsuperscripty b mathbf y ast b bold y start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT italic b end POSTSUBSCRIPT from p y b z b pconditionalsuperscripty subscriptbsubscriptsuperscriptz bp mathbf y ast mid bm theta b mathbf z ast b italic p bold y start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold italic th start POSTSUBSCRIPT italic b end POSTSUBSCRIPT bold z start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT italic b end POSTSUBSCRIPT where b subscriptb bm theta b bold italic th start POSTSUBSCRIPT italic b end POSTSUBSCRIPT is the bbbitalic b th MCMC sample of bm theta bold italic th This yields a Monte Carlo sample of the predictive posterior distribution of Y superscriptY mathbf Y ast bold Y start POSTSUPERSCRIPT end POSTSUPERSCRIPT allowing estimation of various quantities associated with the distribution 
8,S2.E2, log 8289 v 8290 119852 945 0 945 1 8290 w 8290 119852 119907 119852 subscript 120572 0 subscript 120572 1 119908 119852 log v mathbf s alpha 0 alpha 1 w mathbf s roman log italic v bold s italic 945 start POSTSUBSCRIPT 0 end POSTSUBSCRIPT italic 945 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic w bold s 2 ,,Equation 2 allows a distinction between point referenced and areal data a need in data fusion applications Wang and Furrer 2021 With A A mathcal A cdot caligraphic A being the function that returns the area of a spatial object A covariate w s 1 A s 0 ws1As0w mathbf s mathbbm 1 mathcal A mathbf s 0 italic w bold s blackboard 1 caligraphic A bold s 0 enables different variances between these data types Certainly care is necessary in specifying v s vsv mathbf s italic v bold s to ensure a valid GP Palacios and Steel 2006 Prior distributions for all the model parameters are to be specified to complete the Bayesian model specification For fixed effects regression coefficients bm beta bold italic b we employ uncorrelated zero mean Normal priors with marginal variances 102superscript10210 2 10 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT In the homoscedastic case on s sigmaitalic s we place a half t prior with 3333 degrees of freedom denoted t 3 subscriptt3t 3 italic t start POSTSUBSCRIPT end POSTSUBSCRIPT 3 Gelman 2006 For the hereroscedastic model 2 we set independent standard normal priors for a0subscript0 alpha 0 italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT and a1subscript1 alpha 1 italic a start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The PEC dependence is encoded through r n bm delta rho nu bold italic d italic r italic n In geostatistics smoothness parameters for correlation functions are typically fixed and their estimation properties are rarely reported Zhang 2004 Due to this and the scope of this paper we specify separate models for different values of n nuitalic n ensuring a positive definite PEC and select the best fit for inference The spatial dependence parameter r rhoitalic r is also inherently difficult Thus to avoid oversmoothing we employ an exponential prior for r rhoitalic r with mean lr r0 log pr subscriptsubscript0subscriptp lambda rho rho 0 log p rho italic l start POSTSUBSCRIPT italic r end POSTSUBSCRIPT italic r start POSTSUBSCRIPT 0 end POSTSUBSCRIPT roman log start ARG italic p start POSTSUBSCRIPT italic r end POSTSUBSCRIPT end ARG where r0subscript0 rho 0 italic r start POSTSUBSCRIPT 0 end POSTSUBSCRIPT is an upper bound set to four fifths of the largest observed Hausdorff distance and prsubscriptpp rho italic p start POSTSUBSCRIPT italic r end POSTSUBSCRIPT is the probability that r rhoitalic r exceeds r0subscript0 rho 0 italic r start POSTSUBSCRIPT 0 end POSTSUBSCRIPT The resulting prior concentrates its mass on lower values r rhoitalic r Building on previous concepts we introduce the notation for this application The response variable is the recorded PM2 5 noted Y si YsubscriptsiY mathbf s i italic Y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT at the spatial unit sisubscriptsi mathbf s i bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT which is either a point or a grid box We again resort to the GLMM model from Section 3 1 In this case there are no covariates the likelihood is Normal with variance t2superscript2 tau 2 italic t start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT and the link function is the identity A heteroscedastic HGP prior is placed on the random effect allowing for differing variances between point referenced and areal data through the SD function from Equation 2 with w s 1 A s 0 w mathbf s mathbbm 1 mathcal A mathbf s 0 italic w bold s blackboard 1 caligraphic A bold s 0 For interpretability we present results for s exp a0 subscript0 sigma exp alpha 0 italic s roman exp italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT and sa exp a0 a1 subscriptasubscript0subscript1 sigma a exp alpha 0 alpha 1 italic s start POSTSUBSCRIPT italic a end POSTSUBSCRIPT roman exp italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT italic a start POSTSUBSCRIPT 1 end POSTSUBSCRIPT instead of a0subscript0 alpha 0 italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT and a1subscript1 alpha 1 italic a start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The model was fit with n nuitalic n fixed at different values and we selected the model with best goodness of fit n 0 80 8 nu 0 8italic n 0 8 For more details see Table A 8 in the appendix The priors for model parameters are as in Section 4 2 The MCMC algorithm initialization is described in Section 3 2 We used four parallel chains each comprising a warm up period of 2000 samples and an additional 2000 samples for inference All model parameters exhibited split R R hat R over start ARG italic R end ARG values below indicating good convergence This is further supported by the trace plots presented in Section G 1 of the appendix 
9,S2.Ex1, r 8290 h 966 957 exp 8289 8722 h 957 966 957 119903 8462 120593 120584 superscript 8462 120584 superscript 120593 120584 r h varphi nu exp left frac h nu varphi nu right italic r italic h italic 966 italic 957 roman exp divide start ARG italic h start POSTSUPERSCRIPT italic 957 end POSTSUPERSCRIPT end ARG start ARG italic 966 start POSTSUPERSCRIPT italic 957 end POSTSUPERSCRIPT end ARG ,,
10,S3.E3, Y 119852 i 8739 119831 i Z 119852 i 8764 ind f 8901 8739 956 i 120632 Y mathbf s i mid mathbf X i Z mathbf s i overset rm ind sim f cdot mid mu i bm gamma italic Y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8739 bold X start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT overroman ind start ARG 8764 end ARG italic f 8901 8739 italic 956 start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold italic 947 3 ,,
11,S3.E4, 956 i 120124 8290 Y 8290 119852 i 8739 119831 i Z 8290 119852 i g 8722 1 8290 119831 i 8868 8290 120631 Z 8290 119852 i subscript 120583 119894 120124 delimited conditional 119884 subscript 119852 119894 subscript 119831 119894 119885 subscript 119852 119894 superscript 119892 1 subscript superscript 119831 top 119894 120631 119885 subscript 119852 119894 mu i mathbbm E Y mathbf s i mid mathbf X i Z mathbf s i g 1 mathbf X top i bm beta Z mathbf s i italic 956 start POSTSUBSCRIPT italic i end POSTSUBSCRIPT blackboard E italic Y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8739 bold X start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic g start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT bold X start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold italic 946 italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 4 ,,Predicting spatial phenomena accurately at unobserved locations is crucial in many applications especially in change of support settings Gelfand et al 2001 The HGP prior for spatial random effects in GLMMs offers a powerful solution For a set of mmmitalic m unobserved locations S s1 sm superscriptS subscriptsuperscripts 1 subscriptsuperscripts m mathcal S ast mathbf s ast 1 ldots mathbf s ast m caligraphic S start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold s start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 1 end POSTSUBSCRIPT bold s start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT italic m end POSTSUBSCRIPT we define an mmmitalic m dimensional variable Y superscriptY mathbf Y ast bold Y start POSTSUPERSCRIPT end POSTSUPERSCRIPT representing the response variable at these locations Assuming covariates from Equation 4 are available we can generate predictions accounting for uncertainty using the posterior predictive distribution of Y superscriptY mathbf Y ast bold Y start POSTSUPERSCRIPT end POSTSUPERSCRIPT 
12,S3.E5, 119833 8764 HGP 8290 0 v 8290 8901 120648 r 8290 8901 120633 similar to 119833 HGP 0 119907 8901 120648 119903 8901 120633 mathbf Z sim textrm HGP big 0 v cdot bm sigma r cdot bm delta big bold Z 8764 HGP 0 italic v 8901 bold italic 963 italic r 8901 bold italic 948 5 ,,
13,S3.E6, p 8290 119858 8739 119831 119859 8719 i 1 n f 8290 y 8290 119852 i 8739 956 i 120632 119901 conditional 119858 119831 119859 superscript subscript product 119894 1 119899 119891 conditional 119910 subscript 119852 119894 subscript 120583 119894 120632 p mathbf y mid mathbf X mathbf z prod i 1 n f y mathbf s i mid mu i bm gamma italic p bold y 8739 bold X bold z 8719 start POSTSUBSCRIPT italic i 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic n end POSTSUPERSCRIPT italic f italic y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8739 italic 956 start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold italic 947 6 ,,
14,S3.E7, 960 8290 120637 119859 8739 119858 8733 p 8290 119858 8739 119831 119859 120631 120632 8290 960 8290 119859 8739 120648 120633 8290 960 8290 120637 proportional to 120587 120637 conditional 119859 119858 119901 conditional 119858 119831 119859 120631 120632 120587 conditional 119859 120648 120633 120587 120637 pi bm theta mathbf z mid mathbf y propto p mathbf y mid mathbf X mathbf z bm beta bm gamma pi mathbf z mid bm sigma bm delta pi bm theta italic 960 bold italic 952 bold z 8739 bold y 8733 italic p bold y 8739 bold X bold z bold italic 946 bold italic 947 italic 960 bold z 8739 bold italic 963 bold italic 948 italic 960 bold italic 952 7 ,,The estimation of the spatial GLMM parameters is obtained from the posterior distribution defined in Equation 7 As the posterior distribution is intractable we use the No U Turn Hoffman and Gelman 2014 Markov Chain Monte Carlo MCMC sampler available in Stan Team 2023 to draw samples from it The No U Turn algorithm samples all the model parameters jointly and efficiently exploits the parameter space using automatic differentiation Additionally it eliminates the need for hand tuning making it a highly convenient sampler We initialize the parameters with samples from their respective prior distributions The latent random effects are initialized from a standard Gaussian distribution The number of samples and the warm up period of the MCMC algorithm are application dependent We assess the convergence of the chains using the split R R hat R over start ARG italic R end ARG diagnostic Vehtari et al 2021 Finally the parameters point estimates and 95 credible intervals CI are obtained as respectively the median and percentiles 2 52 52 52 5 and 97 597 597 597 5 unless otherwise stated of the marginal MCMC samples 
15,S3.E8, p 8290 119858 8727 8739 119858 8747 p 8290 119858 8727 8739 119859 8727 120637 8290 p 8290 119859 8727 8739 119859 120637 8290 960 8290 120637 8739 119858 8290 d 120637 119901 conditional superscript 119858 8727 119858 119901 conditional superscript 119858 8727 superscript 119859 8727 120637 119901 conditional superscript 119859 8727 119859 120637 120587 conditional 120637 119858 differential d 120637 p mathbf y ast mid mathbf y int p mathbf y ast mid mathbf z ast bm theta p mathbf z ast mid mathbf z bm theta pi bm theta mid mathbf y rm d bm theta italic p bold y start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8739 bold y 8747 italic p bold y start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8739 bold z start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT bold italic 952 italic p bold z start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8739 bold z bold italic 952 italic 960 bold italic 952 8739 bold y roman d bold italic 952 8 ,,We assess the quality of model predictions across different scenarios using four metrics Firstly point and interval predictions are derived from the posterior predictive distributions Equation 8 using their median and the 2 5 and 97 5 percentiles respectively The root mean square error of prediction RMSP is used to evaluate point estimates The remaining three metrics assess interval predictions the frequentist coverage percentage of prediction intervals denoted CPP their width and the interval score Gneiting and Raftery 2007 IS The latter is defined as Efficient sampling from Equation 8 utilizes the MCMC samples obtained during parameter estimation Section 3 2 For each MCMC sample bbbitalic b we generate a corresponding Monte Carlo sample from the posterior predictive distribution of Y superscriptY mathbf Y ast bold Y start POSTSUPERSCRIPT end POSTSUPERSCRIPT First using GP properties Diggle et al 1998 we sample from 
16,S4.E9, Z 8290 119852 i 119964 8290 119852 i 8722 1 8290 8747 119852 i 8290 Z 8290 119852 8290 d 119852 if 160 8290 119964 8290 119852 i gt 0 Z 8290 119852 i otherwise 119885 subscript 119852 119894 cases 119964 superscript subscript 119852 119894 1 subscript 119852 119894 119885 119852 119852 if 160 119964 subscript 119852 119894 0 119885 subscript 119852 119894 otherwise Z mathbf s i begin cases mathcal A mathbf s i 1 int mathbf s i Z mathbf s differential mathbf s amp text if mathcal A mathbf s i gt 0 Z mathbf s i amp text otherwise end cases italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start ROW start CELL caligraphic A bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 8747 bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic Z bold s start DIFFOP roman d end DIFFOP bold s end CELL start CELL if caligraphic A bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT gt 0 end CELL end ROW start ROW start CELL italic Z bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end CELL start CELL otherwise end CELL end ROW 9 ,,Inferences for the model induced by the spatial random effect from Equation 9 were performed using the integrated nested Laplace approximation and the stochastic partial differential equation Lindgren et al 2011 SPDE method The joint distribution induced by this AGP involve approximating stochastic integrals Practitioners must define a grid with a fixed number of points within each areal unit to approximate these integrals Wang and Furrer 2021 The SPDE uses a mesh to discretize the space to achieve such an approximation efficiently The number of grid points per polygon will depend on the resolution of the mesh We used two different meshes to show the inferences and consequently the predictions may change substantially depending on the accuracy of the approximation for the stochastic integrals in Equation 9 In the appendix Figure A 9 visualizes these meshes a sparse mesh left column and a fine resolution mesh second column with approximately 2 and 25 points per polygon respectively Henceforth we will differentiate the AGP models by mesh density denoting the sparse mesh model as AGP1 and the fine mesh model as AGP2 AGP model priors are detailed in Section E of the appendix 
17,S4.Ex2, IS 8290 y l u u 8722 l 2 945 8290 l 8722 y 8290 120793 8290 y lt l 2 945 8290 y 8722 u 8290 120793 8290 y gt u IS 119910 119897 119906 119906 119897 2 120572 119897 119910 1 119910 119897 2 120572 119910 119906 1 119910 119906 rm IS y l u u l frac 2 alpha l y mathbbm 1 y lt l frac 2 alpha y u mathbbm 1 y gt u roman IS italic y italic l italic u italic u italic l divide start ARG 2 end ARG start ARG italic 945 end ARG italic l italic y blackboard 1 italic y lt italic l divide start ARG 2 end ARG start ARG italic 945 end ARG italic y italic u blackboard 1 italic y gt italic u ,,
18,S4.T1.2, AGP HGP Spatial unit Fit RMSP CPP Width IS RMSP CPP Width IS Overall AGP 1 1 56 67 8 3 10 17 74 1 81 57 9 2 09 31 37 AGP 2 1 39 95 7 5 59 6 60 1 80 88 1 5 53 10 63 HGP 1 65 90 8 6 10 8 65 1 51 95 7 6 15 7 05 Polygons 3 2 superscript 3 2 3 2 3 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT AGP 1 0 78 75 8 1 91 5 79 0 89 72 5 1 94 6 96 AGP 2 0 60 96 3 2 50 2 87 0 80 89 7 2 68 4 14 HGP 0 83 97 9 3 78 4 00 0 74 96 4 3 00 3 36 Polygons 1 5 2 superscript 1 5 2 1 5 2 1 5 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT AGP 1 1 33 75 6 3 56 11 73 0 90 75 6 2 12 6 47 AGP 2 1 13 95 8 4 69 5 38 1 09 96 6 4 72 5 32 HGP 1 50 79 2 3 84 9 36 0 85 94 8 3 36 4 01 Points AGP 1 2 56 52 2 3 85 35 70 3 64 22 8 2 20 80 70 AGP 2 2 43 95 1 9 59 11 54 3 51 77 9 9 19 22 42 HGP 2 62 95 3 10 7 12 57 2 94 96 0 12 09 13 79 ,Table 1 Out of sample prediction for the three models under different data generating models Spatial unit indicates the geometry of the sample units for which predictions were assessed for polygons area is additionally specified within parenthesis Fit specifies the model fitted to the data RMSP is the root mean squared error of prediction CPP is the frequentist coverage percentage of the 95 prediction interval Width is the prediction interval width and IS is the interval score Double headers denote the true data generating model ,Table 1 presents the prediction assessment metrics under both data generating models at different types of spatial units Let us first consider the overall case that is ignoring the types When the AGP is the true model the fine mesh AGP model yields the most accurate point predictions lowest RMSP However the HGP model s performance is only marginally worse with a RMSP 18 higher For interval predictions the HGP model produces prediction intervals PI with near nominal CPP and widths comparable to the AGP2 model resulting in a competitive interval score IS Despite having the narrowest PIs on average the sparse mesh AGP model suffers from poor coverage and consequently a high IS As expected when the true data generating model is the HGP the HGP model s predictive performance surpasses that of its competitors The point predictions from both AGP models are remarkably similar with the fine mesh AGP exhibiting a RMSP only 19 higher than the correctly specified HGP This suggests the fine mesh AGP remains competitive in terms of point predictions despite model misspecification In terms of interval predictions while the AGP2 was competitive its sparse mesh counterpart delivered poor interval predictions The performance of the competing method varies considerably depending on the mesh resolution which can significantly impact prediction accuracy In contrast our model eliminates the need for arbitrary mesh selection and consistently delivers highly competitive predictions enhancing prediction accuracy Table 1 further reveals demonstrates a consistent decline in prediction accuracy for all models as the area of the spatial units decreased For AGP generated data the AGP2 model s RMSP for point referenced data was approximately four times higher than for the polygons with bigger area i e 32superscript323 2 3 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT Similarly the HGP model s RMSP increased by a factor of 3 15 for the same comparison point referenced vs largest polygon For both the HGP and AGP2 CPP remained near the nominal level for all the spatial units However HGP s CPP dropped below nominal level for smaller polygons area of 1 52superscript1 521 5 2 1 5 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT This is likely due to variance underestimation while the AGP generated data s variance is inversely proportional to spatial units the fitted HGP model only distinguish between points and polygons not their area Regarding the IS both our model and the AGP2 model showed IS values roughly three times higher for point referenced data compared to the largest polygons This aligns with the simulation parameters The AGP1 model s IS performance deteriorated substantially when decreasing the area of the spatial units Under the HGP data generating process the HGP model consistently outperformed competitors on all assessment metrics except PI width Although point predictions were broadly similar across models the HGP provided remarkably superior interval predictions for all spatial unit types mantaining CPP near the nominal level see Table 1 In contrast the AGP1 model exhibited a CPP of only 22 8 for point referenced data The poor coverage for both AGP models with point referenced data is further reflected in their IS which were 5 9 AGP1 and 1 6 AGP2 times higher than the HGP On the other hand the AGP2 model remained competitive with the HGP for polygons interval predictions while the sparse AGP performed poorly Table A 5 displays the same results as in Table 1 from the main paper but for a different sample size The polygons used to simulate data here are in a 15x15151515 times 1515 x 15 grid Everything else is the same as in Section 4 2 The results follow the same trend as observed in the main text That is in general the HGP does a good job quantifying uncertainty and seems to be more trustworthy under model misspecification 
19,S5.T2.15, HGP DAGAR BYM 946 0 subscript 120573 0 beta 0 italic 946 start POSTSUBSCRIPT 0 end POSTSUBSCRIPT 8722 0 21 8722 0 268 8722 0 139 8722 0 26 8722 0 450 8722 0 137 8722 0 22 8722 0 254 8722 0 184 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 0 33 0 284 0 368 0 31 0 258 0 370 0 33 0 285 0 366 963 120590 sigma italic 963 0 19 0 155 0 234 0 30 0 218 0 484 0 24 0 176 0 354 961 120588 rho italic 961 2 25 0 159 6 948 968 120595 psi italic 968 0 43 0 069 0 827 958 120585 xi italic 958 0 42 0 100 0 815 LOOIC 1081 0 1081 9 1089 3 WAIC 1038 0 1032 4 1039 6 ,Table 2 Parameter estimates and model comparison criteria for the three models fitted to the areal dataset Marginal posterior summary through median and 95 credible intervals ,We present the posterior medians and 95 CIs for the parameters from the HGP DAGAR and BYM models in Table 2 When considering the coefficient linked to the covariate representing income deprivation all three models yield similar posterior estimates The positive posterior estimates the respective parameter b1subscript1 beta 1 italic b start POSTSUBSCRIPT 1 end POSTSUBSCRIPT indicate a positive association between proportion of income deprived people and hospitalization rates for respiratory diseases Among these models DAGAR exhibits the broadest credible intervals for the intercept whereas the marginal posterior distribution for this parameter in the BYM model is highly concentrated around 0 220 22 0 22 0 22 Examining the spatial aspect the confidence interval for the practical range parameter r rhoitalic r in the HGP model implies that the distance at which the spatial correlation between random effects from two distinct spatial units decreases to 0 100 100 100 10 is at most 6 956 956 956 95 kilometers Conversely the 95 CI for the spatial dependence parameter ps psiitalic ps from the DAGAR model spans from 0 070 070 070 07 to 0 830 830 830 83 In other words this credible interval suggests that the clustering effect ranges from almost negligible to very strong A similar pattern is observed for the z zetaitalic z in the BYM model In Table 2 we also present two model comparison criteria described in Section 4 for the models fitted to the data These results suggest the HGP stands out as the best fit based on the LOOIC rendering the HGP prior a competitive fit for these data Despite the modest improvement in model fitting capacity the HGP possesses an advantage in terms of intuitive interpretability of its spatial dependence achieved in the last two paragraphs through the practical range parameter r rhoitalic r Among the specialized models for areal data the BYM was the least adequate in this application However none of the models delivered a remarkably better fit to this dataset This is further reinforced by looking at the maps of observed and smoothed SMR estimates presented in Figure A 10 from the appendix 
20,S5.T3.7, HGP AGP 1 subscript AGP 1 rm AGP 1 roman AGP start POSTSUBSCRIPT 1 end POSTSUBSCRIPT AGP 2 subscript AGP 2 rm AGP 2 roman AGP start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 946 120573 beta italic 946 5 605 4 688 6 449 6 219 2 16 10 10 6 187 5 879 6 478 961 120588 rho italic 961 13 826 7 817 23 608 13 163 5 140 30 244 0 632 0 456 0 832 964 120591 tau italic 964 0 178 0 073 0 312 1 394 1 242 1 563 0 539 0 394 0 719 963 120590 sigma italic 963 3 849 2 921 5 181 1 703 0 960 2 910 2 399 2 024 2 837 963 a subscript 120590 119886 sigma a italic 963 start POSTSUBSCRIPT italic a end POSTSUBSCRIPT 1 241 1 037 1 505 RMSP 1 05 1 45 1 64 Width 3 57 2 53 9 67 CPP 95 5 78 6 95 5 IS 4 80 15 11 13 65 ,Table 3 Parameter estimates and out of sample prediction assessment for the three models fitted to the fused dataset Marginal posterior summary through median and 95 credible intervals RMSP is the root mean squared error of prediction CPP is the frequentist coverage of the 95 prediction interval Width is the width of the prediction interval and IS is the interval score ,We present the posterior median and 95 CI of the model parameters in Table 3 When looking at the common parameters between the three models namely b0subscript0 beta 0 italic b start POSTSUBSCRIPT 0 end POSTSUBSCRIPT s sigmaitalic s r rhoitalic r and t tauitalic t the HGP acts as a compromise between AGP1 and AGP2 while eliminating the arbitrary choice of the level of discretization for approximating numerical integrals Unlike the aggregated models our method allows the data to tell us whether the variance of the underlying spatial parameter is smaller or larger than the same quantity for point referenced data Interestingly the estimated SD of the point referenced data is 3 09 times larger then the areal data 95 CI 2 43 4 12 The interpretation of r rhoitalic r presented in tens of kilometers in both models is carried out in a similar fashion because the Hausdorff distance between points is equivalent to the distance between points used by the AGP models Table 3 also presents the out of sample prediction performance of the three models assessed via 10 fold cross validation Using four metrics from our prior simulation study RMSP CPP prediction interval width IS we find the HGP offers the most reliable point predictions evidenced by its lowest RMSP The HGP also maintains nominal coverage for its prediction intervals While the AGP2 achieves similar coverage its prediction intervals are considerably wider resulting in inferior interval scores Further analysis by data type is detailed in Table A 9 of the appendix revealing the unreliability of competing methods for point referenced data where frequentist coverage can be as low as 3 
21,A3.Ex3, 119833 b 8727 8739 119833 119859 8290 b 8764 119977 8290 956 119852 8727 8739 119852 931 119852 8727 8739 119852 similar to conditional subscript superscript 119833 8727 119887 119833 119859 119887 119977 subscript 120583 conditional superscript 119852 8727 119852 subscript 931 conditional superscript 119852 8727 119852 big mathbf Z ast b mid mathbf Z mathbf z b big sim mathcal N big mu mathbf s ast mid mathbf s Sigma mathbf s ast mid mathbf s big bold Z start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT italic b end POSTSUBSCRIPT 8739 bold Z bold z italic b 8764 caligraphic N italic 956 start POSTSUBSCRIPT bold s start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8739 bold s end POSTSUBSCRIPT roman 931 start POSTSUBSCRIPT bold s start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8739 bold s end POSTSUBSCRIPT ,,
22,A4.T4.15, Scenario Parameter True Bias MAPE RMSE CP 15 215 15 15 15 15 times 15 15 215 15 grid 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 4 290 0 002 1 3 0 115 95 5 946 2 subscript 120573 2 beta 2 italic 946 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 0 330 0 004 8 9 0 058 95 0 961 120588 rho italic 961 1 654 0 439 45 6 1 536 95 0 963 120590 sigma italic 963 0 436 0 014 7 0 0 058 93 5 15 215 15 15 15 15 times 15 15 215 15 grid 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 4 290 8722 0 005 1 0 0 089 95 5 946 2 subscript 120573 2 beta 2 italic 946 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 0 330 0 002 6 4 0 044 95 5 961 120588 rho italic 961 1 971 0 398 31 1 1 318 95 0 963 120590 sigma italic 963 0 436 0 009 5 0 0 043 95 5 Map 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 4 390 0 001 1 2 0 104 95 5 946 2 subscript 120573 2 beta 2 italic 946 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 0 330 0 001 10 9 0 063 92 5 961 120588 rho italic 961 2 320 0 812 56 3 2 735 92 5 963 120590 sigma italic 963 0 436 0 012 7 4 0 061 95 0 ,Table A 4 Assessing parameter estimation for HGP model for areal data Scenario indicates the data configuration scenario True is the parameter used to simulate the datasets Bias is the bias of estimation MAPE is the mean absolute percentage error RMSE is the root mean square error of estimation and CP is the frequentist coverage percentage of the credible interval ,In the appendix Table A 4 we also evaluated the HGP s ability to recover true model parameters under correct specification We assessed estimation accuracy using Mean Absolute Percentage Error MAPE and the frequentist coverage percentage CP of 95 credible intervals All model parameters achieved CPs close to the nominal level with the lowest being 92 5 for r rhoitalic r which consistently proved the most challenging to estimate The second most difficult parameter s sigmaitalic s exhibited a maximum MAPE of only 7 In Table A 4 we present the bias of estimation root mean square error RMSE and frequentist coverage percentage of the credible intervals for the parameters in the HGP along with the parameters used to generate the datasets in the areal data simulation study Although r rhoitalic r is a harder parameter to estimate our model has delivered credible intervals with good coverage for this parameter 
23,A5.T5.4, DGM Fit MAPE RMSP IS CPP AGP HGP 8 35 1 84 9 41 90 4 AGP 1 8 26 1 73 16 23 74 4 AGP 2 7 33 1 57 7 03 92 2 HGP HGP 17 36 1 78 7 47 96 1 AGP 1 27 09 2 26 31 94 60 6 AGP 2 30 34 2 29 10 90 87 9 ,Table A 5 Out of sample prediction for the three models under different data generating models DGM stands for the data generation model MAPE is the mean absolute percentage error of prediction RMSP is the root mean square error of prediction IS is the interval score and CPP is the frequentist coverage of the 95 prediction interval ,Table A 5 displays the same results as in Table 1 from the main paper but for a different sample size The polygons used to simulate data here are in a 15x15151515 times 1515 x 15 grid Everything else is the same as in Section 4 2 The results follow the same trend as observed in the main text That is in general the HGP does a good job quantifying uncertainty and seems to be more trustworthy under model misspecification 
24,A5.T6.18, Map Parameter True Bias MAPE RMSE CP 15 215 15 15 15 15 times 15 15 215 15 grid 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 5 810 0 014 5 568 0 402 91 5 961 120588 rho italic 961 20 366 8722 0 967 12 351 3 221 99 0 963 a subscript 120590 119886 sigma a italic 963 start POSTSUBSCRIPT italic a end POSTSUBSCRIPT 1 311 8722 0 154 12 183 0 192 81 5 963 120590 sigma italic 963 4 442 8722 0 125 11 784 0 634 91 5 964 120591 tau italic 964 0 170 0 252 148 432 0 265 66 5 17 215 17 17 17 17 times 17 17 215 17 grid 946 1 subscript 120573 1 beta 1 italic 946 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 5 810 8722 0 022 4 903 0 358 90 0 961 120588 rho italic 961 27 934 0 003 10 927 3 836 99 5 963 a subscript 120590 119886 sigma a italic 963 start POSTSUBSCRIPT italic a end POSTSUBSCRIPT 1 311 8722 0 115 9 121 0 145 84 5 963 120590 sigma italic 963 4 442 8722 0 150 8 616 0 477 93 5 964 120591 tau italic 964 0 170 0 170 99 788 0 181 79 5 ,Table A 6 Assessing parameter estimation for HGP model for fused data Map indicates the size of the grid where the data was simulated True is the parameter used to simulate the datasets Bias is the bias of estimation MAPE is the mean absolute percentage error RMSE is the root mean square error of estimation and CP is the frequentist coverage percentage of the credible interval ,Table A 6 in the appendix presents the results for parameter estimation under the correct specification of the the HGP model We used the same metrics as in Section 4 1 to assess the parameters estimation The MAPE for r rhoitalic r was around 10 9 percent10 910 9 10 9 while for sasubscripta sigma a italic s start POSTSUBSCRIPT italic a end POSTSUBSCRIPT it was 9 21 percent9 219 21 9 21 Similarly to what is observed with geostatistical models the small scale standard deviation parameter namely t tauitalic t was the most challenging parameter to be estimated Unlike the areal simulation the coverage of some of the variance parameters was consistently below the nominal level of 95 percent9595 95 This results are expected as the HGP inherits the weak identifiability of these parameters from geostatistical models under a normal likelihood Zhang 2004 Predictions remain trustworthy despite this issue In Table A 6 we present the bias of estimation root mean square error RMSE and frequentist coverage percentage of the credible intervals for the parameters in the HGP along with the parameters used to generate the datasets in the fused data simulation study We present the parameter estimation results for two sizes of simulated maps Similarly to what we encounter in the geostatistics literature t tauitalic t is hard to estimate However as we increased the sample size the estimation of this parameter has improved considerably In addition despite the low bias when estimating the practical range parameter r rhoitalic r the coverage of the credible intervals and the RMSE indicate high uncertainty around the estimation of such parameter 
25,A6.T7.6, 957 0 6 120584 0 6 nu 0 6 italic 957 0 6 957 0 7 120584 0 7 nu 0 7 italic 957 0 7 957 0 8 120584 0 8 nu 0 8 italic 957 0 8 957 0 9 120584 0 9 nu 0 9 italic 957 0 9 LOOIC 1091 9 1081 5 1092 6 1090 8 WAIC 1039 3 1038 7 1039 3 1039 2 DIC 1049 5 1048 8 1049 4 1049 6 ,Table A 7 Different model comparison criteria for the homoscedastic HGP models fit with different values for n nuitalic n ,In this application Y si YsubscriptsiY mathbf s i italic Y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT is the number of admissions by respiratory disease registered at the iiiitalic i th IZ denoted sisubscriptsi mathbf s i bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT In the iiiitalic i th IZ define xisubscriptxix i italic x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT as the percentage of people who are defined to be income deprived and EisubscriptEiE i italic E start POSTSUBSCRIPT italic i end POSTSUBSCRIPT as the expected number of admissions We analyze the data using the GLMM model from Section 3 1 with a Poisson likelihood a log logroman log link function an intercept a covariate xisubscriptxix i italic x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and an offset EisubscriptEiE i italic E start POSTSUBSCRIPT italic i end POSTSUBSCRIPT We employed the homoscedastic HGP with a PEC function with smoothness n 0 70 7 nu 0 7italic n 0 7 as the prior for the random effects the choice of n 0 70 7 nu 0 7italic n 0 7 is justified in Table A 7 of the appendix The priors for the parameters in the HGP GLMM were set to be those presented in Section 3 2 Inference was performed using four parallel chains with 5000 MCMC samples thinned by 10 after discarding the initial 1000 iterations as the warm up period The initialization of the algorithm is described in Section 3 2 The highest split R R hat R over start ARG italic R end ARG was smaller than 1 011 011 011 01 providing evidence of convergence To choose the most appropriate value of n nuitalic n in our application we have fitted out model with n 0 6 0 7 0 8 0 9 0 60 70 80 9 nu in 0 6 0 7 0 8 0 9 italic n 0 6 0 7 0 8 0 9 These values were selected because they fill different regions of the parameter space where the PEC function is positive definite The goodness of fit metrics associated to each value of n nuitalic n are presented in Table A 7 
26,A7.T8.6, 957 0 6 120584 0 6 nu 0 6 italic 957 0 6 957 0 7 120584 0 7 nu 0 7 italic 957 0 7 957 0 8 120584 0 8 nu 0 8 italic 957 0 8 957 0 9 120584 0 9 nu 0 9 italic 957 0 9 LOOIC 511 0 506 2 505 1 511 5 WAIC 516 4 507 9 507 2 512 1 DIC 480 0 471 7 468 7 469 7 ,Table A 8 Different model comparison criteria for the heteroscedastic HGP models fit with different values for n nuitalic n ,Building on previous concepts we introduce the notation for this application The response variable is the recorded PM2 5 noted Y si YsubscriptsiY mathbf s i italic Y bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT at the spatial unit sisubscriptsi mathbf s i bold s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT which is either a point or a grid box We again resort to the GLMM model from Section 3 1 In this case there are no covariates the likelihood is Normal with variance t2superscript2 tau 2 italic t start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT and the link function is the identity A heteroscedastic HGP prior is placed on the random effect allowing for differing variances between point referenced and areal data through the SD function from Equation 2 with w s 1 A s 0 w mathbf s mathbbm 1 mathcal A mathbf s 0 italic w bold s blackboard 1 caligraphic A bold s 0 For interpretability we present results for s exp a0 subscript0 sigma exp alpha 0 italic s roman exp italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT and sa exp a0 a1 subscriptasubscript0subscript1 sigma a exp alpha 0 alpha 1 italic s start POSTSUBSCRIPT italic a end POSTSUBSCRIPT roman exp italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT italic a start POSTSUBSCRIPT 1 end POSTSUBSCRIPT instead of a0subscript0 alpha 0 italic a start POSTSUBSCRIPT 0 end POSTSUBSCRIPT and a1subscript1 alpha 1 italic a start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The model was fit with n nuitalic n fixed at different values and we selected the model with best goodness of fit n 0 80 8 nu 0 8italic n 0 8 For more details see Table A 8 in the appendix The priors for model parameters are as in Section 4 2 The MCMC algorithm initialization is described in Section 3 2 We used four parallel chains each comprising a warm up period of 2000 samples and an additional 2000 samples for inference All model parameters exhibited split R R hat R over start ARG italic R end ARG values below indicating good convergence This is further supported by the trace plots presented in Section G 1 of the appendix The most appropriate value of n nuitalic n in our application was chosen based on the goodness of fit when fitting our model with n 0 6 0 7 0 8 0 9 0 60 70 80 9 nu in 0 6 0 7 0 8 0 9 italic n 0 6 0 7 0 8 0 9 These values were selected because they fill different regions of the parameter space where the PEC function is positive definite The goodness of fit metrics associated to each value of n nuitalic n are presented in Table A 8 
27,A7.T9.4, Resolution Model RMSP CPP Width IS Point HGP 2 71 85 4 9 6 12 69 AGP 1 4 07 3 12 2 4 111 84 AGP 2 2 71 57 3 8 2 45 03 Polygon HGP 0 62 96 9 2 9 3 83 AGP 1 0 86 86 6 2 5 6 03 AGP 2 1 10 98 9 9 8 10 92 ,Table A 9 Metrics for assessment of out of sample predictions using 10 fold cross validation for different geometry resolutions RMSP is the root mean squared error of prediction CPP is the frequentist coverage of the 95 prediction interval Width is the width of the prediction interval and IS is the interval score ,Table 3 also presents the out of sample prediction performance of the three models assessed via 10 fold cross validation Using four metrics from our prior simulation study RMSP CPP prediction interval width IS we find the HGP offers the most reliable point predictions evidenced by its lowest RMSP The HGP also maintains nominal coverage for its prediction intervals While the AGP2 achieves similar coverage its prediction intervals are considerably wider resulting in inferior interval scores Further analysis by data type is detailed in Table A 9 of the appendix revealing the unreliability of competing methods for point referenced data where frequentist coverage can be as low as 3 Table A 9 presents the predictive performance of the HGP and aggregated models using a 10 fold cross validation It summarizes how each model performed specifically for the point referenced and areal polygon data In terms of point prediction HGP has shown a better performance regardless of the geometry type The same behavior was observed for interval predictions The AGP models tend to be overconfident in their interval predictions for point referenced data resulting in very low coverage 
28,S6.T1, Hyper parameter Exploration phase Exploitation phase c 1 subscript c 1 c 1 2 0187 3 6845 c 2 subscript c 2 c 2 0 1 5614 c 3 subscript c 3 c 3 3 2697 0 c 4 subscript c 4 c 4 0 3 6703 ,Table 1 Values of the acceleration coefficients for exploration and exploitation phases,The maximum speed at which vehicles can reach is 2 meters per second The value of the length scale of the GP is set to 10 taking into account the above mentioned by 8 the authors indicate that to have a good smoothness the value of the length scale should be 10 percent of the value of the search map Considering the works in 16 and 17 the range for l lambda to obtain a good performance of the monitoring system is 0 1 0 5 By setting the value of l lambda to 0 1 the ASVs will take more samples However the execution time will be higher On the contrary if the value of l lambda is 0 5 the monitoring system will take less time on the other hand the value of samples will be lower To balance these two factors the l lambda value was set to 0 3 the middle value of the range The values of the acceleration coefficients are different in the exploration and exploitation phase In the exploration phase the values of the coefficients obtained in 24 are used and in the exploitation phase the values of the coefficients obtained in 25 are used This is because in these works studies were carried out in which the behavior of the algorithm in exploration and exploitation was analyzed Data of the acceleration coefficients are shown in Table 1 These values are used for all fleet sizes After making the necessary adjustments the performance of the path planners is compared It is used as a basis algorithm developed by 43 for the Lawn mower path planner The settings for the compared path planners with respect to water sampling and GP are the same settings as shown in Sect 6 1 3 The maximum distance traveled by the vehicles is 20 000 meters To set the values of the hyper parameters in the Classic PSO 44 45 are taken into consideration c1 c2 2subscriptc1subscriptc22c 1 c 2 2 For the Enhanced GP based PSO Exploration the values of the coefficients are the same as those used in the exploration phase Table 1 Moreover the values of the coefficients used in the Enhanced GP based PSO Exploitation have the same values used in the exploitation phase Table 1 The best combination of parameters for the epsilon greedy method is shown in the Table 5 Finally the values of the coefficients for the AquaFeL PSO algorithm are those shown in the Table 1 
29,S6.T2, Number of ASVs r a d r a d rad m m a x p r t m a x p r t max prt 2 5 000 30 4 2 500 50 6 1 667 70 8 1 250 90 ,Table 2 Parameters of the action zones according to the number of ASVs in the fleet,The Table 2 shows the radius r a dradrad and maximum priority m a x p r tmax prtmax prt parameters of the action zones for each number of vehicles nA S V ssubscriptnASVsn ASVs The results were obtained by applying Eq 9 and Eq 10 respectively The length parameter l e n g t hlengthlength of the Eq 9 in the Ypacarai lake scenario has a value of 10 000 meters 
30,S6.T3, Phase distance MSE 5km MSE 10km MSE 15km MSE 20km MSE 25km MSE 30km Exploration 5km 0 01388 plus or minus pm 0 0265 plus or minus pm 0 00217 plus or minus pm 0 00209 plus or minus pm 0 00212 plus or minus pm 0 00195 plus or minus pm Exploitation 25km 0 02379 0 00702 0 00462 0 00444 0 00447 0 00459 Exploration 10km 0 01388 plus or minus pm 0 00427 plus or minus pm 0 00068 plus or minus pm 0 00045 plus or minus pm 0 00045 plus or minus pm 0 00046 plus or minus pm Exploitation 20km 0 02379 0 00509 0 00081 0 00054 0 00064 0 00066 Exploration 15km 0 01388 plus or minus pm 0 00427 plus or minus pm 0 00337 plus or minus pm 0 00167 plus or minus pm 0 00032 plus or minus pm 0 00029 plus or minus pm Exploitation 15km 0 02379 0 00509 0 00467 0 00446 0 00017 0 00027 Exploration 20km 0 01388 plus or minus pm 0 00427 plus or minus pm 0 00337 plus or minus pm 0 00299 plus or minus pm 0 00061 plus or minus pm 0 00025 plus or minus pm Exploitation 10km 0 02379 0 00509 0 00467 0 00418 0 00118 0 00033 Exploration 25km 0 01388 plus or minus pm 0 00427 plus or minus pm 0 00337 plus or minus pm 0 00299 plus or minus pm 0 00169 plus or minus pm 0 00045 plus or minus pm Exploitation 5km 0 02379 0 00509 0 00467 0 00418 0 00138 0 00137 Exploration 30km 0 01388 plus or minus pm 0 00427 plus or minus pm 0 00337 plus or minus pm 0 00299 plus or minus pm 0 00169 plus or minus pm 0 00119 plus or minus pm 0 02379 0 00509 0 00467 0 00418 0 00138 0 00119 ,Table 3 Comparison between the MSE of the distances traveled in the exploration phase and the exploitation phase for four vehicles,Table 3 shows the results of the mean MSE and the confidence interval for four vehicles obtained with different exploration and exploration distances in the estimation of the water quality model of the entire surface water resource When the maximum distance that ASVs can travel is 20km the best combination of exploration and exploration is 10 km and 10 km This means that the balance of phases to obtain a good model is to explore the surface of the water resource 50 percent of the total distance and 50 percent to monitor or exploit the contamination zones Considering the maximum distance equal to 30km the best result was obtained when the ASVs explored 20km and exploited 10km However the difference between the results obtained in the case Exploration 15km and Exploitation 15km case is very low Therefore it can be concluded that the best balance between exploration and exploitation is 50 percent 50 percent of the maximum distance with cases where the distance of the exploration phase can be increased to 67 percent of the maximum travel distance For more detailed results with different fleet sizes see the A The results obtained with four vehicles and a maximum distance of 20km are shown graphically in Fig 11 
31,S6.T4, Technique MSE Action Error Peaks MSE Model of the Mean of Time Zones Lake Consuming sec Centralized Learning 0 02304 plus or minus pm 0 05757 0 00155 plus or minus pm 0 00985 0 00038 plus or minus pm 0 00063 8 43176 plus or minus pm 2 67029 Federated Learning 0 02331 plus or minus pm 0 05749 0 00160 plus or minus pm 0 00919 0 00045 plus or minus pm 0 00055 8 29892 plus or minus pm 2 22732 ,Table 4 Comparison of MSE and error applying centralized learning and federated learning techniques,This subsection shows the comparison between the application of the FL technique and a centralized learning Using the centralized learning the samples taken by the fleet in the exploitation phase go directly to the main server and a single estimated water quality model is generated unlike the FL The results show in Table 4 demonstrate that there is very little difference between using a centralized learning or applying the FL technique However as an advantage with FL there is no excess data on the central server since the GP is adjusted at the nodes in each action zone 
32,S6.T5, Parameter Hyper parameter Value d 0 d subscript italic 0 d epsilon 0 m 6 500 d f d subscript italic f d epsilon f m 13 500 D D italic Delta epsilon 0 13 c 1 e x p l o r e subscript c 1 e x p l o r e c 1explore 2 0187 c 2 e x p l o r e subscript c 2 e x p l o r e c 2explore 0 c 3 e x p l o r e subscript c 3 e x p l o r e c 3explore 3 2697 c 4 e x p l o r e subscript c 4 e x p l o r e c 4explore 0 c 1 e x p l o i t subscript c 1 e x p l o i t c 1exploit 3 6845 c 2 e x p l o i t subscript c 2 e x p l o i t c 2exploit 1 5614 c 3 e x p l o i t subscript c 3 e x p l o i t c 3exploit 0 c 4 e x p l o i t subscript c 4 e x p l o i t c 4exploit 3 1262 ,Table 5 Parameter and hyper parameter values of the Epsilon Greedy method,After making the necessary adjustments the performance of the path planners is compared It is used as a basis algorithm developed by 43 for the Lawn mower path planner The settings for the compared path planners with respect to water sampling and GP are the same settings as shown in Sect 6 1 3 The maximum distance traveled by the vehicles is 20 000 meters To set the values of the hyper parameters in the Classic PSO 44 45 are taken into consideration c1 c2 2subscriptc1subscriptc22c 1 c 2 2 For the Enhanced GP based PSO Exploration the values of the coefficients are the same as those used in the exploration phase Table 1 Moreover the values of the coefficients used in the Enhanced GP based PSO Exploitation have the same values used in the exploitation phase Table 1 The best combination of parameters for the epsilon greedy method is shown in the Table 5 Finally the values of the coefficients for the AquaFeL PSO algorithm are those shown in the Table 1 
33,S6.T6, Path Planner MSE Action Error Peaks MSE Model of the Zones Lake Lawn mower 0 07988 plus or minus pm 0 30728 0 22435 plus or minus pm 0 64639 0 03225 plus or minus pm 0 07159 Classic PSO 0 04095 plus or minus pm 0 09118 0 14414 plus or minus pm 0 57829 0 01146 plus or minus pm 0 02812 Enhanced GP based PSO 0 02642 plus or minus pm 0 05389 0 10634 plus or minus pm 0 32627 0 00268 plus or minus pm 0 00388 Exploration Enhanced GP based PSO 0 04403 plus or minus pm 0 10310 0 15388 plus or minus pm 0 59155 0 01266 plus or minus pm 0 03218 Exploitation Epsilon Greedy Method 0 02648 plus or minus pm 0 05869 0 07599 plus or minus pm 0 31318 0 00246 plus or minus pm 0 00760 AquaFeL PSO 0 02331 plus or minus pm 0 05749 0 00160 plus or minus pm 0 00919 0 00045 plus or minus pm 0 00055 ,Table 6 Comparison of the MSE and the error of the path planners for four vehicles,The comparison of the mean MSE and errors together with the 95 percent confidence interval is shown in Table 6 The results of the table show that 1 in the three cases considered MSE of the action zone error of the peaks and MSE of the whole lake the proposed monitoring system has the best performance 2 the difference between the models obtained from the action zones of the AquaFeL PSO algorithm and the other path planners is not large however in the detection of the peak of each action zone there are large differences with the epsilon greedy being the second lowest error the error obtained with this path planner is more than 40 times greater than the error obtained with the AquaFeL PSO algorithm and 3 with respect to the estimated model of the whole lake the best MSE has the AquaFeL PSO algorithm and the second best is the MSE of the Enhanced GP based PSO based on the epsilon greedy method the difference between the proposed monitoring system and the epsilon greedy method is more than 400 percent and the result obtained with the lawn mower algorithm is approximately 70 times greater than the AquaFeL PSO algorithm 
34,A1.T7, Vs MSE 5km MSE 10km MSE 15km MSE 20km MSE 25km MSE 30km Exploration 5km 0 06180 plus or minus pm 0 04020 plus or minus pm 0 03476 plus or minus pm 0 03425 plus or minus pm 0 03403 plus or minus pm 0 03395 plus or minus pm Exploitation 25km 0 05052 0 03669 0 03538 0 03635 0 03646 0 03656 Exploration 10km 0 06180 plus or minus pm 0 02646 plus or minus pm 0 01610 plus or minus pm 0 00622 plus or minus pm 0 00574 plus or minus pm 0 00562 plus or minus pm Exploitation 20km 0 05052 0 03128 0 02942 0 02236 0 01538 0 01123 Exploration 15km 0 06180 plus or minus pm 0 02646 plus or minus pm 0 01762 plus or minus pm 0 01492 plus or minus pm 0 00679 plus or minus pm 0 00375 plus or minus pm Exploitation 15km 0 05052 0 03128 0 02716 0 02734 0 01484 0 01047 Exploration 20km 0 06180 plus or minus pm 0 02646 plus or minus pm 0 01762 plus or minus pm 0 01634 plus or minus pm 0 01231 plus or minus pm 0 00611 plus or minus pm Exploitation 10km 0 05052 0 03128 0 02716 0 02800 0 02511 0 01835 Exploration 25km 0 06180 plus or minus pm 0 02646 plus or minus pm 0 01762 plus or minus pm 0 01634 plus or minus pm 0 01253 plus or minus pm 0 00338 plus or minus pm Exploitation 5km 0 05052 0 03128 0 02716 0 02800 0 02214 0 00857 Exploration 30km 0 06180 plus or minus pm 0 02646 plus or minus pm 0 01762 plus or minus pm 0 01634 plus or minus pm 0 01253 plus or minus pm 0 01077 plus or minus pm 0 05052 0 03128 0 02716 0 02800 0 02214 0 01879 ,Table 7 Comparison between the MSE of the distances traveled in the exploration phase and the exploitation phase for two vehicles,This subsection details the results obtained for two Table 7 six Table 8 and eight vehicles Table 9 with respect to the duration of the exploration phase and the exploitation phase Table 7 shows the results obtained with two ASVs when the maximum distance that ASVs can travel is 20km the best combination of exploration and exploration is 10 km and 10 km This same result is obtained for six Table 8 and eight ASVs Table 9 Considering the maximum distance equal to 30km the best result with two vehicles Table 7 was obtained in the Exploration 25km and Exploitation 5km However by a small margin the second best was Exploration 15km and Exploitation 15km For eight ASVs Table 9 similar results were obtained with Exploration 10km and Exploitation 20km Exploration 15km and Exploitation 15km and Exploration 20km and Exploitation 10km It can be said that with the balance between exploration and exploitation equal to 50 percent 50 percent a good performance of the algorithm is obtained The best result with six ASVs Table 8 was obtained when the ASVs explored 20km and exploited 10km However the difference between the results obtained in the case Exploration 15km and Exploitation 15km case is very low Therefore it can be concluded that the best balance between exploration and exploitation is 50 percent 50 percent of the maximum distance with cases where the distance of the exploration phase can be increased to 67 percent of the maximum travel distance From the results obtained in Table 9 it can be concluded that having more vehicles the difference is minimal whether the ASVs travel 20km or 30km Therefore the ideal distance for eight ASVs is 20km since there is no significant difference in the results 
35,A1.T8, Phase distance MSE 5km MSE 10km MSE 15km MSE 20km MSE 25km MSE 30km Exploration 25km 0 00704 plus or minus pm 0 00142 plus or minus pm 0 00095 plus or minus pm 0 00094 plus or minus pm 0 00093 plus or minus pm 0 00093 plus or minus pm Exploitation 15km 0 00701 0 00329 0 00198 0 00197 0 00198 0 00198 Exploration 10km 0 00704 plus or minus pm 0 00276 plus or minus pm 0 00065 plus or minus pm 0 00042 plus or minus pm 0 00040 plus or minus pm 0 00040 plus or minus pm Exploitation 20km 0 00701 0 00628 0 00194 0 00097 0 00092 0 00094 Exploration 15km 0 00704 plus or minus pm 0 00276 plus or minus pm 0 00201 plus or minus pm 0 00060 plus or minus pm 0 00050 plus or minus pm 0 00039 plus or minus pm Exploitation 15km 0 00701 0 00628 0 00538 0 00162 0 00168 0 00128 Exploration 20km 0 00704 plus or minus pm 0 00276 plus or minus pm 0 00201 plus or minus pm 0 00136 plus or minus pm 0 00043 plus or minus pm 0 00033 plus or minus pm Exploitation 10km 0 00701 0 00628 0 00538 0 00328 0 00103 0 00085 Exploration 25km 0 00704 plus or minus pm 0 00276 plus or minus pm 0 00201 plus or minus pm 0 00136 plus or minus pm 0 00113 plus or minus pm 0 00052 plus or minus pm Exploitation 5km 0 00701 0 00628 0 00538 0 00328 0 00302 0 00149 Exploration 30km 0 00704 plus or minus pm 0 00276 plus or minus pm 0 00201 plus or minus pm 0 00136 plus or minus pm 0 00113 plus or minus pm 0 00092 plus or minus pm 0 00701 0 00628 0 00538 0 00328 0 00302 0 00299 ,Table 8 Comparison between the MSE of the distances traveled in the exploration phase and the exploitation phase for six vehicles,This subsection details the results obtained for two Table 7 six Table 8 and eight vehicles Table 9 with respect to the duration of the exploration phase and the exploitation phase Table 7 shows the results obtained with two ASVs when the maximum distance that ASVs can travel is 20km the best combination of exploration and exploration is 10 km and 10 km This same result is obtained for six Table 8 and eight ASVs Table 9 Considering the maximum distance equal to 30km the best result with two vehicles Table 7 was obtained in the Exploration 25km and Exploitation 5km However by a small margin the second best was Exploration 15km and Exploitation 15km For eight ASVs Table 9 similar results were obtained with Exploration 10km and Exploitation 20km Exploration 15km and Exploitation 15km and Exploration 20km and Exploitation 10km It can be said that with the balance between exploration and exploitation equal to 50 percent 50 percent a good performance of the algorithm is obtained The best result with six ASVs Table 8 was obtained when the ASVs explored 20km and exploited 10km However the difference between the results obtained in the case Exploration 15km and Exploitation 15km case is very low Therefore it can be concluded that the best balance between exploration and exploitation is 50 percent 50 percent of the maximum distance with cases where the distance of the exploration phase can be increased to 67 percent of the maximum travel distance From the results obtained in Table 9 it can be concluded that having more vehicles the difference is minimal whether the ASVs travel 20km or 30km Therefore the ideal distance for eight ASVs is 20km since there is no significant difference in the results 
36,A1.T9, Phase distance MSE 5km MSE 10km MSE 15km MSE 20km MSE 25km MSE 30km Exploration 5km 0 00137 plus or minus pm 0 00041 plus or minus pm 0 00029 plus or minus pm 0 00028 plus or minus pm 0 00028 plus or minus pm 0 00028 plus or minus pm Exploitation 25km 0 00196 0 00090 0 00056 0 00054 0 00054 0 00054 Exploration 10km 0 00137 plus or minus pm 0 00023 plus or minus pm 0 00006 plus or minus pm 0 00003 plus or minus pm 0 00002 plus or minus pm 0 00002 plus or minus pm Exploitation 20km 0 00196 0 00044 0 00013 0 00003 0 00003 0 00003 Exploration 15km 0 00137 plus or minus pm 0 00023 plus or minus pm 0 00011 plus or minus pm 0 00003 plus or minus pm 0 00002 plus or minus pm 0 00002 plus or minus pm Exploitation 15km 0 00196 0 00044 0 00035 0 00004 0 00003 0 00003 Exploration 20km 0 00137 plus or minus pm 0 00023 plus or minus pm 0 00011 plus or minus pm 0 00006 plus or minus pm 0 00005 plus or minus pm 0 00002 plus or minus pm Exploitation 10km 0 00196 0 00044 0 00035 0 00018 0 00019 0 00004 Exploration 25km 0 00137 plus or minus pm 0 00023 plus or minus pm 0 00011 plus or minus pm 0 00006 plus or minus pm 0 00005 plus or minus pm 0 00006 plus or minus pm Exploitation 5km 0 00196 0 00044 0 00035 0 00018 0 00018 0 00028 Exploration 30km 0 00137 plus or minus pm 0 00023 plus or minus pm 0 00011 plus or minus pm 0 00006 plus or minus pm 0 00005 plus or minus pm 0 00005 plus or minus pm 0 00196 0 00044 0 00035 0 00018 0 00018 0 00019 ,Table 9 Comparison between the MSE of the distances traveled in the exploration phase and the exploitation phase for eight vehicles,This subsection details the results obtained for two Table 7 six Table 8 and eight vehicles Table 9 with respect to the duration of the exploration phase and the exploitation phase Table 7 shows the results obtained with two ASVs when the maximum distance that ASVs can travel is 20km the best combination of exploration and exploration is 10 km and 10 km This same result is obtained for six Table 8 and eight ASVs Table 9 Considering the maximum distance equal to 30km the best result with two vehicles Table 7 was obtained in the Exploration 25km and Exploitation 5km However by a small margin the second best was Exploration 15km and Exploitation 15km For eight ASVs Table 9 similar results were obtained with Exploration 10km and Exploitation 20km Exploration 15km and Exploitation 15km and Exploration 20km and Exploitation 10km It can be said that with the balance between exploration and exploitation equal to 50 percent 50 percent a good performance of the algorithm is obtained The best result with six ASVs Table 8 was obtained when the ASVs explored 20km and exploited 10km However the difference between the results obtained in the case Exploration 15km and Exploitation 15km case is very low Therefore it can be concluded that the best balance between exploration and exploitation is 50 percent 50 percent of the maximum distance with cases where the distance of the exploration phase can be increased to 67 percent of the maximum travel distance From the results obtained in Table 9 it can be concluded that having more vehicles the difference is minimal whether the ASVs travel 20km or 30km Therefore the ideal distance for eight ASVs is 20km since there is no significant difference in the results 
37,A1.T10, Path Planner MSE Action Error Peaks MSE Model of the Zones Lake Lawn mower 0 06407 plus or minus pm 0 11914 0 56516 plus or minus pm 0 57607 0 04625 plus or minus pm 0 03788 Classic PSO 0 05912 plus or minus pm 0 11250 0 33411 plus or minus pm 0 75806 0 04272 plus or minus pm 0 05509 Enhanced GP based PSO 0 03577 plus or minus pm 0 06286 0 33622 plus or minus pm 0 76587 0 01631 plus or minus pm 0 02798 Exploration Enhanced GP based PSO 0 06164 plus or minus pm 0 11393 0 36485 plus or minus pm 0 80912 0 04378 plus or minus pm 0 05145 Exploitation Epsilon Greedy Method 0 04524 plus or minus pm 0 08610 0 39966 plus or minus pm 0 85490 0 03297 plus or minus pm 0 04059 AquaFel PSO 0 02361 plus or minus pm 0 05634 0 07770 plus or minus pm 0 44700 0 00526 plus or minus pm 0 011230 ,Table 10 Comparison of the MSE and the error of the path planners for two vehicles,
38,A1.T11, Path Planner MSE Action Error Peaks MSE Model of the Zones Lake Lawn mower 0 21101 plus or minus pm 1 61966 0 27808 plus or minus pm 1 20208 0 05738 plus or minus pm 0 05048 Classic PSO 0 03357 plus or minus pm 0 07676 0 06726 plus or minus pm 0 25293 0 00450 plus or minus pm 0 00609 Enhanced GP based PSO 0 03472 plus or minus pm 0 07295 0 04969 plus or minus pm 0 19958 0 00131 plus or minus pm 0 00313 Exploration Enhanced GP based PSO 0 03667 plus or minus pm 0 08129 0 06685 plus or minus pm 0 25677 0 00501 plus or minus pm 0 00770 Exploitation Epsilon Greedy Method 0 03122 plus or minus pm 0 06354 0 05911 plus or minus pm 0 25621 0 00200 plus or minus pm 0 00385 AquaFel PSO 0 03156 plus or minus pm 0 06709 0 00255 plus or minus pm 0 01590 0 00042 plus or minus pm 0 00097 ,Table 11 Comparison of the MSE and the error of the path planners for six vehicles,
39,S3.T1, Model Ok vqa A okvqa Vcr direct answer multiple choice direct answer multiple choice bertscore test val ppl val GloVe test val test val ppl val ofa 40 40 24 54 56 19 47 40 48 09 39 77 33 55 64 55 ofa q gt a 49 93 74 32 65 30 61 71 63 00 53 91 54 89 83 85 umae all 51 77 74 59 65 67 63 26 63 29 56 14 56 66 85 97 Prior best 54 41 60 30 53 70 48 60 40 70 77 10 ,Table 1 Performance of models for answer generation Better results are in bold ofa refers to the pretrained OFA Prior best results for the three datasets are from Gui et al 2022 Schwenk et al 2022 Wang et al 2022b respectively is from a discriminative model and thus not comparable see Ye and Kovashka 2021 ,We also compare the performance of our approach using perplexity as the metric with GloVe embedding similarity for A OKVQA see Table 1 Table 1 presents our observations for answer accuracy on Q A task over the three datasets We also evaluate VCR answers using BERTScore as the answers for VCR are usually sentences We observe that umaeall outperforms ofaq a on all datasets improves the prior SOTA on A OKVQA by 10 similar to sim15 and achieves competitive results on OK VQA For models that are finetuned on A OKVQA we also see a salient improvement 9 with the proposed mapping of options by perplexity in Multiple Choice instead of GloVe embeddings similarity666Preliminary experiments with NLG metrics BERTScore and BLEU for selecting the options given generation were sub optimal We conducted several ablation studies on the dependency of the modality for the answer accuracy in A OKVQA where we find the visual encoder is crucial for performance Details are included in Appendix C 
40,S3.T2, Dataset Model e V i l Scores N gram Scores Learnt Score S O S T S E bleu4 rouge l meteor cide r spice bertscore A okvqa ofa 4 44 56 19 7 90 0 30 4 45 3 26 4 82 4 62 68 64 ofa q gt a ofa qa gt e 35 82 74 32 48 29 22 18 48 51 23 56 86 76 22 46 85 96 umae a okvqa 37 10 73 97 50 15 27 61 52 23 24 06 104 39 22 88 87 86 umae all 37 91 74 59 50 82 27 35 52 56 24 83 101 09 23 33 88 21 Vcr e ug 19 30 69 80 27 60 4 30 22 50 11 80 32 70 12 60 79 00 umae vcr 22 57 56 68 39 82 12 25 28 87 16 67 48 14 27 36 81 77 umae all 22 82 56 66 40 27 13 44 29 53 17 54 47 33 26 45 81 91 Vqa x e ug 36 50 80 50 45 40 23 20 45 70 22 10 74 10 20 10 87 00 umae all 31 58 77 65 40 67 14 63 35 12 20 29 50 35 19 13 85 40 ,Table 2 Explanation Scores ofa is the pretrained OFA showing the transferability of OFA for generating explanations with natural language instructions Results with e UG are from Kayser et al 2021 We show the best results of A OKVQA and VCR in bold The last row in blue shade shows out of domain performance ,Table 2 shows e ViL sores section 4 for explanations using automatic NLG metrics777Nucleus sampling shows best results and is reported Detailed scores with different decoding methods are shown in Appendix D Following the same setup as in Kayser et al 2021 an explanation is evaluated only if the answer predicted by the system is correct888A limitation of evaluating all explanations is that explanations of wrong answers may get high scores with n gram metrics even though they are justifying wrong answers and should be penalised We observe that pretrained OFA with natural language prompts e g what is the explanation for the answer or this is because performs poorly as most generated explanations are words yes no or short phrases999BERTScore in not representative of the validity of outputs from OFA We refer the reader to an exposition of the problems associated with NLG metrics in Caglayan et al 2020 We compare UMAE models on all and individual datasets with prior best results from e UG see section 2 and standard separated trained baselines ofaq a ofaqa e umaeall achieves better results across all datasets showing the advantage of mixing tasks and datasets in different domains For out of domain evaluation on VQA X umaeall also shows mostly competitive results Examples of explanation generation are shown in Figure 2 and Appendix E 
41,S5.T3, Model S E bleu4 r l met cide r spice berts c ofa q gt a ofa qa gt e 42 4 20 0 44 2 19 3 66 7 19 1 85 1 umae a okvqa 45 8 23 6 47 9 21 7 78 0 20 5 86 9 umae all 46 8 24 9 49 5 22 3 84 1 20 8 87 3 ,Table 3 Explanation scores on the same subset of A OKVQA ,Since e ViL only evaluates an explanation if a model generates the correct answer the subset of explanations evaluated varies by model To fairly compare explanations on the same subset we propose only using the subset of samples where all models provide correct answers for explanation prediction Table 3 shows the results on A OKVQA with such a subset of 770 candidates where umaeall shows an even higher explanation score This highlights that umaeall generates explanations that overlap significantly better with gold explanations 
42,A2.T4, Question Objects Images Accuracy original 50 39 39 16 random 33 48 33 28 ,Table 4 Ablation on the modality dependency for answer accuracy of A OKVQA ,We conduct several ablation studies to investigate the dependency of object features and images on the performance of our model UMAEall for answer accuracy of A OKVQA where we removed images replaced them with random images and removed extracted attributes and features Results in Table 4 show that the visual encoder is crucial for performance and that visual objects alone are not sufficient for answer prediction Using a random image would introduce noise and therefore performs worse than not including the image at all We did not test removing the question because we believe the model needs the questions to be able to provide answers 
43,A3.T5, Dataset Decoding e V i l N gram Scores Learnt Sc S E bleu1 bleu2 bleu3 bleu4 rouge l meteor cide r spice bertscore A okvqa beamsearch 44 71 52 01 36 69 26 72 19 88 40 39 22 06 68 48 20 94 86 05 top k k 100 k 100 k 100 44 34 52 56 37 06 27 06 19 72 44 45 21 58 73 44 19 38 86 27 nucleus p 0 4 p 0 4 p 0 4 50 82 58 92 44 66 35 06 27 35 52 56 24 83 101 09 23 33 88 21 typical p 0 6 p 0 6 p 0 6 47 27 54 18 39 39 29 82 22 18 47 78 22 79 84 43 21 47 86 95 Vcr beamsearch 40 23 26 41 20 15 15 95 12 47 29 13 16 82 49 72 27 70 81 84 top k k 50 k 50 k 50 33 19 20 98 14 89 11 18 8 33 23 65 13 72 32 73 21 99 80 31 nucleus p 0 1 p 0 1 p 0 1 40 27 31 42 22 95 17 62 13 44 29 53 17 54 47 33 26 45 81 91 typical p 0 4 p 0 4 p 0 4 35 12 23 42 16 88 12 83 9 64 25 36 14 70 35 85 23 32 80 70 Vqa x beamsearch 35 88 37 84 24 91 16 67 10 97 31 32 17 90 38 23 16 23 84 39 top k k 50 k 50 k 50 33 28 38 35 23 11 14 21 8 45 29 15 17 05 32 89 15 26 83 41 nucleus p 0 1 p 0 1 p 0 1 40 67 47 56 31 44 21 47 14 63 35 12 20 29 50 35 19 13 85 40 typical p 0 5 p 0 5 p 0 5 36 31 40 85 25 57 16 82 11 14 31 08 18 15 39 71 16 62 83 93 ,Table 5 Explanation scores with automatic NLG for generated explanations QA rightarrowE from umaeall model with different decoding strategies The last two rows with blue shadow indicate out of domain performance ,For decoding we evaluate the performance of beam search with the size of 5 top k sampling with kkk from 50 100 200 1000 50100200 1000 50 100 200 1000 and Nucleus and Typical Meister et al 2022 sampling both with ppp from 0 1 0 2 0 9 0 10 2 0 9 0 1 0 2 0 9 We show the details of the NLG scores using different decoding strategies for explanations generated from QA rightarrowE in Table 5 and Q rightarrowAE in Table 6 
44,A3.T6, Dataset Decoding e V i l N gram Scores Learnt Sc S E bleu1 bleu2 bleu3 bleu4 rouge l meteor cide r spice bertscore A okvqa beamsearch 47 01 54 75 41 39 32 08 24 25 49 75 22 54 86 28 20 68 87 39 nucleus p 0 5 p 0 5 p 0 5 46 72 55 53 41 63 31 91 23 67 49 16 22 48 82 37 20 67 87 18 Vcr beamsearch 37 02 25 00 18 90 14 87 11 54 27 07 15 66 38 77 25 03 80 68 nucleus p 0 1 p 0 1 p 0 1 35 10 27 41 19 36 14 50 10 73 26 18 15 21 34 99 21 88 80 52 Vqa x beamsearch 38 13 39 91 26 30 17 99 12 46 31 69 19 11 42 10 18 15 84 95 nucleus p 0 1 p 0 1 p 0 1 39 67 44 92 28 88 19 04 12 55 33 08 20 07 44 28 19 19 85 21 ,Table 6 Explanation scores with automatic NLG for generated explanations from Q rightarrowAE with umaeall model The last two rows with blue shadow indicate out of domain performance ,For decoding we evaluate the performance of beam search with the size of 5 top k sampling with kkk from 50 100 200 1000 50100200 1000 50 100 200 1000 and Nucleus and Typical Meister et al 2022 sampling both with ppp from 0 1 0 2 0 9 0 10 2 0 9 0 1 0 2 0 9 We show the details of the NLG scores using different decoding strategies for explanations generated from QA rightarrowE in Table 5 and Q rightarrowAE in Table 6 
45,A6.T7, Task A okvqa Vcr Vqa x mc golve bertscore da q gt a 65 67 81 91 77 65 q gt ae 65 67 82 30 69 60 ,Table 7 Evaluation of answers generated given questions Q A and jointly generated with explanations Q AE MC stands for Multiple Choice DA for Direct Answer The last column with a blue shadow indicates out of domain performance ,We present the results of the proposed Q rightarrowAE task where answers and explanations are jointly generated We parse the generated sequence to the answer and the explanation and use the same sets of metrics as the separate generation for evaluation Results for answers in Table 7 and explanations in Table 8 For answers since the perplexity metric does not directly compare the generation we show the Multiple Choice accuracy using the Glove metric for A OKVQA and BERTScore for VCR answer sentences 
46,A6.T8, Dataset S E Ngramscore Bertscore qa gt e q gt ae qa gt e q gt ae qa gt e q gt ae A okvqa 50 82 47 01 35 69 32 15 88 21 87 39 Vcr 40 27 37 02 26 70 24 02 81 91 80 68 Vqa x 40 67 39 67 26 69 25 85 85 40 85 21 ,Table 8 Scores of explanations generated given answers QA E and jointly generated with answers Q AE The last row with a blue shadow indicates out of domain performance ,We present the results of the proposed Q rightarrowAE task where answers and explanations are jointly generated We parse the generated sequence to the answer and the explanation and use the same sets of metrics as the separate generation for evaluation Results for answers in Table 7 and explanations in Table 8 For answers since the perplexity metric does not directly compare the generation we show the Multiple Choice accuracy using the Glove metric for A OKVQA and BERTScore for VCR answer sentences 
47,A6.T9, Ok vqa A okvqa da mc glove da Best 80 94 80 74 66 20 Average 54 98 71 53 57 29 Worst 16 37 59 35 41 46 ,Table 9 Human performance on OK VQA and A OKVQA measured from the ground truth answers ,To understand the inter annotator agreement for the datasets we further measure the best average and worst human performance on OK VQA and A OKVQA by selecting the most common answer a random answer and the least common answer respectively from the 10 ground truth answers for each question We calculate the performance using the VQA metric for direct answers and the GloVe metric for Multiple Choice for simplicity Note that we also remove the answer selected from the ground truth answers when measuring human performance From the results in Table 9 we can see that the average performance on both datasets is relatively poor which indicates the noise in the datasets The quality of the datasets needs to be more carefully inspected so that the model performance evaluated on these datasets can be more meaningful 
48,S5.T1, Dataset Methods M 10 M 20 M 40 IoT devices Local learning 0 210 plus or minus pm 0 089 FedAvg 0 759 plus or minus pm 0 650 0 643 plus or minus pm 0 107 0 610 plus or minus pm 0 493 FedAvg 0 802 plus or minus pm 0 028 0 726 plus or minus pm 0 630 0 697 plus or minus pm 0 117 KD PDFL ours 0 816 plus or minus pm 0 032 0 739 plus or minus pm 0 040 0 716 plus or minus pm 0 101 EMNIST Local learning 0 697 plus or minus pm 0 209 FedAvg 0 764 plus or minus pm 0 119 0 784 plus or minus pm 0 108 0 824 plus or minus pm 0 142 FedAvg 0 771 plus or minus pm 0 104 0 806 plus or minus pm 0 123 0 841 plus or minus pm 0 136 KD PDFL ours 0 787 plus or minus pm 0 108 0 835 plus or minus pm 0 116 0 870 plus or minus pm 0 082 ,TABLE I Summary of per client test accuracy under IoT devices Te x 20subscriptTex20T ex 20 and EMNIST datasets Te x 5subscriptTex5T ex 5 ,Table I shows that KD PDFL enables the clients to extend the upper bound of their estimated test accuracy without collaboration Notably users with small local training sets benefit from increased test accuracy from 21 021 021 0 to 81 681 681 6 on average These weak users with small training sets also undergo a more challenging similarity decision Since the lack of training set incurs blurry distance divergence across intermediate outputs that they calculate estimating connectivity weights becomes more difficult 
49,S4.T1, Slake OVQA PathVQA Number of images 642 2 001 4 998 Number of questions 14 028 19 020 32 799 Mean length of questions 4 52 4 52 4 52 8 98 8 98 8 98 6 36 6 36 6 36 Mean length of answers 1 21 1 21 1 21 3 31 3 31 3 31 1 80 1 80 1 80 Number of unique answers 461 641 3 182 ,Table 1 Statistics of the medical VQA datasets used in this paper ,The three datasets used for the evaluation of our method are Slake 20 PathVQA 11 and OVQA 13 These three datasets are the current most suitable VQA datasets given their large variety in answers and the manual curation of answers by domain experts Each dataset is split 50 50 between yes no and open set answers See the datatset details in Table 1 We use the official train validation test splits across all three datasets 
50,S5.T2, LM fine tuning LM size Params Slake OVQA PathVQA BL1 BS F1 Acc BL1 BS F1 Acc BL1 BS F1 Acc MedFuseNet 28 60 5 38 1 Ours w BioGPT Frozen 1 5B 0 64 5 69 9 57 7 66 5 32 4 71 9 52 5 53 5 36 9 57 6 31 0 45 3 Prefix 16 0 487 58 1 74 1 54 1 67 4 37 9 65 0 46 1 53 2 53 6 61 8 34 8 46 7 Prompt 15 0 001 44 2 75 6 47 6 53 7 47 5 62 9 34 6 50 3 28 0 58 7 43 8 33 2 LoRA 12 0 311 59 2 72 2 63 1 71 9 41 0 68 5 57 7 57 3 57 8 62 9 40 4 47 9 Ours w BioMedLM Frozen 2 7B 0 70 2 77 8 47 8 66 0 55 2 72 9 54 2 61 1 61 2 66 1 52 4 53 0 Prefix 16 0 753 64 3 79 4 60 9 63 3 49 1 76 9 51 5 60 1 59 7 60 7 48 9 52 3 Prompt 15 0 009 44 6 73 5 38 8 41 6 48 9 72 8 44 3 59 5 51 9 59 8 38 9 49 3 LoRA 12 0 101 72 3 80 6 62 4 71 7 59 0 76 2 62 6 67 8 67 9 76 0 54 4 57 2 Ours w GPT2 Frozen 1 5B 0 65 1 83 3 57 7 71 2 60 2 79 8 59 4 66 1 64 2 74 6 47 5 58 1 Prefix 16 0 492 70 0 86 5 66 3 74 1 61 2 83 9 65 5 68 9 67 5 76 2 52 5 60 5 Prompt 15 0 003 57 8 80 3 49 9 60 0 57 8 78 3 55 2 63 1 54 4 72 0 38 1 46 6 LoRA 12 0 157 78 6 91 2 78 1 83 3 61 8 85 4 69 1 71 0 70 3 78 5 58 4 63 6 ,Table 2 Performance across different language models and fine tuning strategies measured in BLEU1 BL1 BERTScore BS F1 and accuracy Params is the amount of trainable parameters in the language model Our method using GPT2 in combination with LoRA yields the best performance across all datasets ,The evaluation of our method across various language models and fine tuning settings in Table 2 shows that language models can perform open ended medical VQA Specifically we outperform the only existing method MedFuseNet 28 that does open ended VQA due to the capability of pre trained language models to capture long term dependencies when generating free form answers Additionally prefix 16 and prompt tuning 15 do not improve the performance of the model as much as using LoRA 12 which directly adapts the QQQ and VVV weight matrices of the attention blocks Moreover larger datasets show the most consistent performance gain of parameter efficient fine tuning across all metrics Using a language model pre trained on a general text corpus such as GPT2 26 improves the overall performance compared to its medically trained models e g BioGPT or BioMedLM as can be observed in Table 2 BioGPT and BioMedLM could be overoptimized to their medical text corpora which leads to lack of generalization to different downstream domains 
51,S5.T3, Slake OVQA PathVQA Open set Yes no All Open set Yes no All Open set Yes no All MEVF SAN 24 75 3 78 4 76 5 36 9 72 8 58 5 6 0 81 0 43 6 MEVF BAN 24 77 8 79 8 78 6 36 3 76 3 60 4 8 1 81 4 44 8 MEVF SAN VQAMix 10 12 1 84 4 48 4 MEVF BAN VQAMix 10 13 4 83 5 48 6 MMQ SAN 5 56 9 76 2 68 5 9 6 83 7 46 8 MMQ BAN 5 48 2 76 2 65 0 11 8 82 1 47 1 QCR BAN 34 78 8 82 0 80 0 52 6 77 7 67 7 CRPD BAN 19 81 2 84 4 82 1 MMBERT 14 37 9 80 2 63 3 QCR CLIP 6 78 4 82 5 80 1 Ours w BioGPT LoRA 71 1 72 7 71 9 48 3 66 5 57 3 30 2 65 5 47 9 Ours w BioMedLM LoRA 72 1 71 4 71 7 55 3 80 3 67 8 34 1 80 4 57 2 Ours w GPT2 LoRA 84 3 82 1 83 3 62 6 84 7 71 0 40 0 87 0 63 6 ,Table 3 Comparison of the accuracy between open ended VQA against classification based VQA methods split between yes no and open set answers Our method performs particularly well on both types of answers compared to the state of the art methods ,Our method is performing significantly better on the open set answering in comparison to classification based methods as shown in Table 3 We also confirm that CLIP based image embeddings perform well in the medical domain 6 compared to the conventional use of CNNs Since our approach is generative it is not bounded by the class imbalance issue which is considered a bottleneck of classification based VQA models Our method performs especially well compared to other method on PathVQA which relatively has the largest class imbalance accentuating this effect Even on the simple yes no questions the performance is better showing that this simple yet effective method provides a more natural way of doing VQA 
52,S5.T4, Setting Slake OVQA PathVQA B1 BS F1 Acc B1 BS F1 Acc B1 BS F1 Acc w o Q Q mathbf Q 29 4 48 4 14 3 22 1 33 2 41 9 18 1 27 6 42 9 43 8 18 3 24 6 w o I I mathbf I 54 8 79 3 49 5 50 9 45 5 77 1 49 5 54 4 65 9 72 8 47 2 46 3 Swap Q Q mathbf Q and I I mathbf I 73 3 88 7 73 2 74 9 60 0 84 2 67 3 66 9 70 2 78 0 57 2 58 7 Regular 78 6 91 2 78 1 83 3 61 8 85 4 69 1 71 0 70 3 78 5 58 4 63 6 ,Table 4 Effect of using different prompt structures Note that QQ mathbf Q and II mathbf I denote the question and image respectively The regular setting with the question embeddings followed by the visual prefix Fig 1 leads to the best overall performance ,We also investigate the influence of the prompt structure on the overall performance demonstrated in Table 4 It can be observed that the performance largely decreases when the question is removed compared to when the visual information is removed This suggests that the question plays a more important role in answer generation Interestingly the model is sensitive the order of the elements in the prompt as the swapping of the question embeddings and the visual prefix yields decreases the performance The reason for this is that the language model conveys lower to no importance the visual information if it is located in front of the question In this situation the language model basically generates blind answers This highlights the importance of prompt structure 
53,S3.T1, Indices c c c Index for a class c 1 C c 1 C c in 1 dots C r r r Index for AL round r 1 R R r 1 R delimited R r in 1 dots R R k k k Index for a client k 1 K K k 1 K delimited K k in 1 dots K K Parameters B B B Labeling budget for each AL round r r r a alpha Local heterogeneity level r rho Global imbalance ratio Data U k r superscript subscript U k r U k r Pool of unlabeled instances for a client k k k at round r r r L k r superscript subscript L k r L k r A queried instance set from U k r superscript subscript U k r U k r at round r r r D k r superscript subscript D k r D k r An available labeled set at round r r r Weights Th r superscript Th r Theta r Aggregated weights via FL phases on D r superscript D r D r global model Th k r superscript subscript Th k r Theta k r Separately optimized weights on D k r superscript subscript D k r D k r local only model ,Table 1 Summary of notations throughout the paper ,AL Procedure For the ease of understanding we summarize notations in Table 1 At the first AL round i e r 1r1r 1 each client kkk randomly selects BBB instances Lk1 x1 xB superscriptsubscriptLk1subscriptx1 subscriptxB L k 1 x 1 dots x B from Uk1superscriptsubscriptUk1 U k 1 and oracles annotate them to obtain the initial labeled set Dk1 x1 y1 xB yB superscriptsubscriptDk1subscriptx1subscripty1 subscriptxBsubscriptyB D k 1 x 1 y 1 dots x B y B For the next round r 2r2r geq 2 based on the given querying strategy A A mathcal A cdot and the model parameters ThTh Theta the query set of the kkk th client at round rrr is sampled by
54,S4.T2, Obs 2 Local EMD downarrow Obs 3 Global EMD downarrow Case Model 10 20 30 40 10 20 30 40 a G 0 632 0 638 0 641 0 643 0 019 0 064 0 086 0 095 L 0 632 0 597 0 592 0 595 0 019 0 050 0 050 0 046 b G 0 049 0 077 0 070 0 084 0 014 0 070 0 066 0 063 L 0 049 0 042 0 054 0 059 0 014 0 025 0 044 0 053 c G 0 692 0 680 0 676 0 674 0 377 0 300 0 294 0 294 L 0 692 0 641 0 633 0 636 0 377 0 334 0 326 0 321 d G 0 371 0 298 0 284 0 274 0 368 0 294 0 282 0 272 L 0 371 0 313 0 293 0 290 0 368 0 309 0 287 0 288 ,Table 2 Local EMD and global EMD on CIFAR 10 We summarize the results of four AL rounds with the labeling budget of 10 per round a d setups correspond to those of Figure 2 See Appendix B 2 for EMDs of more cases ,More precisely we verify that the local only model indeed queries the locally balanced set using earth mover s distance EMD 46 In Table 2 local EMD measures the mean of distance between class distribution of local query sets and a uniform distribution The lower the value the more balanced the locally queried instances As shown in Table 2 a with high local heterogeneity the local EMD of the local only model L is lower than that of the global model G That is the local only model queries more diverse instances than the global model with respect to the local inter class diversity Meanwhile in the case of b the global model trained with more samples has higher accuracy over the classes due to little distribution discrepancy Although the more accurate model is likely to have the higher prediction confidence it does not mean that it is better at identifying the required instances based on the current local dataset which the global model had not directly learned In practice the local only model still chose the more locally balanced query set Table 2 b and we supposed this contradiction makes no sizeable winning gap of the case b in Figure 2 We introduce an additional global EMD the indicator of measuring the inter class diversity of the aggregated queried set over all clients As can be seen in Table 2 c where the global imbalance ratio is high we confirm that the global query selector G favors to query global minority classes The global EMD of the global model is lower than that of the local only model i e the more globally balanced query set but the local EMD is the opposite 
55,S6.T3, CIFAR 10 SVHN PathMNIST DermaMNIST Method Model 20 40 60 80 20 30 40 50 20 30 40 50 20 30 40 50 Random 64 19 69 07 71 63 72 81 80 90 83 07 84 22 84 77 68 41 72 70 73 76 75 49 71 70 72 57 72 66 72 86 G 64 02 69 12 71 87 73 33 82 08 84 61 85 88 86 31 71 54 74 39 75 91 76 65 72 49 72 63 73 02 73 20 Entropy 41 L 66 29 71 45 73 51 74 02 82 09 84 58 85 69 86 18 76 52 78 29 78 71 79 10 71 38 72 04 72 22 72 65 G 64 66 69 43 71 75 73 1 80 94 82 74 83 81 84 46 74 84 76 24 76 85 76 80 72 02 72 16 72 34 72 74 Coreset 37 L 64 06 68 79 71 49 73 28 80 94 82 92 83 78 84 48 72 53 76 06 76 28 76 86 71 13 71 48 72 15 72 38 G 65 12 69 57 72 11 73 53 82 81 84 82 85 89 86 2 72 21 74 38 75 53 76 97 72 59 73 09 73 23 73 45 BADGE 6 L 66 32 71 28 73 41 74 28 82 69 84 67 85 61 86 1 76 48 78 51 78 42 78 68 71 35 72 13 72 25 72 99 G 65 40 70 05 72 41 73 42 82 05 84 07 85 09 85 61 75 51 77 79 78 13 78 81 72 01 72 60 73 07 73 17 GCNAL 8 L 65 62 70 18 72 36 73 42 81 92 83 58 84 55 85 10 74 85 76 46 77 18 77 45 71 95 72 91 72 91 73 29 G 65 45 69 87 72 24 73 29 83 02 84 99 86 05 86 33 73 34 74 83 76 31 77 43 72 39 73 14 73 27 73 10 ALFA Mix 34 L 64 14 68 79 71 03 72 6 81 08 82 55 83 62 84 33 71 10 75 01 75 81 76 70 71 51 72 18 72 94 73 28 LoGo ours G L 66 50 71 70 73 80 74 49 83 46 85 31 86 02 86 38 76 32 78 72 79 51 79 58 72 61 73 18 73 33 73 77 ,Table 3 Comparison of test accuracy on four benchmarks with a alpha 0 1 We reported the results with four random seeds The baselines except for Random sampling are combined with two query selector models GGG and LLL that stands for a global or local only model respectively Bold and underline mean Top 1 and Top 2 respectively ,Table 3 shows the test accuracy according to the increased labeling budgets over rounds on four datasets Even with the same active learning strategy a gap in test accuracy occurs depending on which query selector is used because the global imbalance varies by 1 01 01 0 58 758 758 7 across datasets For example in general global models outperform local only models for SVHN and DermaMNIST while the opposite trend is observed for CIFAR 10 and PathMNIST However irrespective of the benchmarks and querying model types our LoGo shows the best performance in most cases Two step selection strategy enables LoGo to be robust by utilizing both the benefits of global and local only models Because we cannot know the degree of local and global imbalance in advance our proposed method has a strong advantage in providing data agnostic performance improvements over all baselines Appendix G 2 provides a detailed performance comparison between LoGo and baselines under various experimental settings We have further experimented with two federated learning algorithms FedProx 28 and SCAFFOLD 23 in conjunction with AL strategies Specifically we compared our LoGo with baselines that demonstrated Top 1 or Top 2 performance more than once in Table 3 The experimental configurations are same to those used in Table 3 As summarized in Table 9 LoGo consistently outperforms the baselines for both federated learning algorithms This observation suggests that LoGo is an orthogonal selection algorithm that can be integrated with any federated learning algorithm having potential to improve the performance in various applications 
56,S6.T4, CIFAR 10 SVHN Method Strategy 20 40 60 80 20 30 40 Entropy 64 53 70 36 73 02 74 28 81 81 84 64 85 87 Ens Logit BADGE 65 55 70 31 72 83 73 97 82 77 84 76 85 90 Entropy 65 90 70 92 73 34 74 20 82 15 84 38 85 64 Ens Rank BADGE 66 21 70 98 73 15 74 01 83 02 85 05 85 86 Entropy 65 10 70 75 73 21 74 23 82 53 85 05 86 01 Fine tuning BADGE 65 82 70 95 72 94 74 12 82 59 84 89 85 82 LoGo ours 66 50 71 70 73 80 74 49 83 46 85 31 86 02 ,Table 4 Comparison of test accuracy on two benchmarks a alpha 0 1 with baselines using both global and local information ,In Table 4 LoGo consistently shows better classification accuracy over increasing labeling budgets than three counterparts Compared with the results in both Tables 3 and 4 all three ensemble methods show lower performance than using a single superior query selector That is the naive ensemble suffers from a performance trade off between two query selector models and therefore their results fall somewhere in the middle of using global and local models 
57,A2.T5, r rho a alpha model Local EMD downarrow Global EMD downarrow 10 20 30 40 50 10 20 30 40 50 1 0 1 G 0 632 0 638 0 641 0 643 0 646 0 019 0 064 0 086 0 095 0 091 L 0 632 0 597 0 592 0 595 0 601 0 019 0 050 0 050 0 046 0 055 1 1 0 G 0 297 0 297 0 300 0 300 0 300 0 017 0 066 0 079 0 084 0 083 L 0 297 0 248 0 232 0 235 0 241 0 017 0 053 0 065 0 068 0 074 1 infty G 0 049 0 077 0 070 0 065 0 061 0 014 0 070 0 066 0 063 0 060 L 0 049 0 042 0 054 0 059 0 066 0 014 0 025 0 044 0 053 0 062 5 0 1 G 0 662 0 663 0 666 0 666 0 669 0 211 0 201 0 196 0 194 0 195 L 0 662 0 628 0 627 0 628 0 634 0 211 0 232 0 232 0 236 0 228 5 1 0 G 0 402 0 391 0 387 0 388 0 389 0 206 0 188 0 180 0 173 0 169 L 0 402 0 309 0 306 0 306 0 341 0 206 0 200 0 201 0 196 0 196 5 infty G 0 213 0 190 0 178 0 168 0 165 0 206 0 185 0 174 0 162 0 163 L 0 213 0 179 0 176 0 180 0 180 0 206 0 176 0 173 0 178 0 180 10 0 1 G 0 692 0 685 0 687 0 685 0 685 0 280 0 268 0 267 0 265 0 267 L 0 692 0 652 0 650 0 654 0 660 0 280 0 270 0 277 0 282 0 281 10 1 0 G 0 491 0 463 0 459 0 456 0 455 0 297 0 263 0 247 0 244 0 242 L 0 491 0 408 0 402 0 405 0 415 0 297 0 256 0 257 0 255 0 255 10 infty G 0 315 0 240 0 229 0 223 0 222 0 303 0 237 0 226 0 222 0 221 L 0 315 0 238 0 237 0 239 0 240 0 303 0 237 0 234 0 237 0 239 20 0 1 G 0 692 0 680 0 676 0 674 0 677 0 377 0 300 0 294 0 294 0 298 L 0 692 0 641 0 633 0 636 0 644 0 377 0 304 0 326 0 321 0 323 20 1 0 G 0 481 0 455 0 450 0 448 0 448 0 374 0 311 0 300 0 295 0 292 L 0 481 0 448 0 437 0 431 0 437 0 374 0 354 0 342 0 303 0 304 20 infty G 0 371 0 298 0 284 0 274 0 276 0 368 0 294 0 282 0 271 0 272 L 0 371 0 313 0 293 0 290 0 289 0 368 0 309 0 287 0 288 0 289 ,Table 5 Local and global EMD on CIFAR 10 for 12 combinations of r rho 1 5 10 20 and a alpha 0 1 1 0 infty ,Table 5 summarizes the detailed local and global EMD for the combinations of r rho 1 5 10 20 and a alpha 0 1 1 0 infty 
58,A4.T6, Dataset of Train of Test of Classes r rho Natural CIFAR 10 50 000 10 000 10 1 0 SVHN 73 257 26 032 10 2 97 Medical PathMNIST 89 996 7 180 9 1 63 DermaMNIST 7 007 2 005 7 58 66 OrganAMNIST 34 581 17 778 11 4 54 ,Table 6 Summary of benchmark datasets ,We mainly experimented on two natural image datasets CIFAR 10222https www cs toronto edu kriz cifar html SVHN333http ufldl stanford edu housenumbers and three medical image datasets444https medmnist com PathMNIST DermaMNIST OrganAMNIST Table 6 provides a summary of the five datasets For the details of partitioning data to each client please refer to Appendix A 
59,A4.T7, Query Selelctor Dir a alpha Data Type Model Arch Budget Size Model Init Global 0 1 CIFAR 10 4CNN 5 Random Global 0 1 SVHN 4CNN 5 Random Global 0 1 PathMNIST 4CNN 5 Random Global 0 1 OrganAMNIST 4CNN 5 Random Global 0 1 DermaMNIST 4CNN 5 Random Global 1 CIFAR 10 4CNN 5 Random Global 1 SVHN 4CNN 5 Random Global infty CIFAR 10 4CNN 5 Random Global infty SVHN 4CNN 5 Random Global 0 1 CIFAR 10 4CNN 5 Continue Global 0 1 SVHN 4CNN 5 Continue Global 0 1 CIFAR 10 ResNet 18 5 Random Global 0 1 SVHN ResNet 18 5 Random Global 0 1 CIFAR 10 MobileNet 5 Random Global 0 1 SVHN MobileNet 5 Random Global 0 1 CIFAR 10 4CNN 1 Random Global 0 1 SVHN 4CNN 1 Random Global 0 1 CIFAR 10 4CNN 20 Random Global 0 1 SVHN 4CNN 20 Random Local only 0 1 CIFAR 10 4CNN 5 Random Local only 0 1 SVHN 4CNN 5 Random Local only 0 1 PathMNIST 4CNN 5 Random Local only 0 1 OrganAMNIST 4CNN 5 Random Local only 0 1 DermaMNIST 4CNN 5 Random Local only 1 CIFAR 10 4CNN 5 Random Local only 1 SVHN 4CNN 5 Random Local only infty CIFAR 10 4CNN 5 Random Local only infty SVHN 4CNN 5 Random Local only 0 1 CIFAR 10 4CNN 5 Continue Local only 0 1 SVHN 4CNN 5 Continue Local only 0 1 CIFAR 10 ResNet 18 5 Random Local only 0 1 SVHN ResNet 18 5 Random Local only 0 1 CIFAR 10 MobileNet 5 Random Local only 0 1 SVHN MobileNet 5 Random Local only 0 1 CIFAR 10 4CNN 1 Random Local only 0 1 SVHN 4CNN 1 Random Local only 0 1 CIFAR 10 4CNN 20 Random Local only 0 1 SVHN 4CNN 20 Random ,Table 7 Summary of the entire experimental combinations ,We compared our algorithms and baselines in 38 comprehensive experimental settings which are the combinations of the aforementioned six categories All the experimental combinations we performed are summarized in Table 7 A maximum value of each matrix corresponds to Table 7 and the bar plots in Figure 4 are calculated from these matrices 
60,A5.T8, Entropy Coreset BADGE GCNAL ALFA Mix LoGo Query ratio G L G L G L G L G L G L 5 rightarrow 10 5 99 8 85 7 32 10 24 14 43 17 36 8 20 11 13 13 88 20 87 17 10 40 rightarrow 45 4 17 33 59 7 02 33 99 10 01 39 11 8 11 35 46 11 94 41 99 37 42 75 rightarrow 80 3 95 59 57 6 72 58 98 3 95 62 62 7 71 60 26 10 46 65 16 56 81 ,Table 8 Computational cost on CIFAR 10 with 4 layers of CNN We averaged the query selection time sec of all 10 clients measured on a RTX 3090 GPU ,In Table 8 we measured the wallclock time for various combinations of the algorithm query selector and labeling ratio We confirmed that as the percentage of labeled data increases the time required to measure the importance score with the global model decreases due to the reduced amount of unlabeled data Conversly the local only model takes more time as it requires training on a larger number of labeled samples Our LoGo algorithm shows a comparable computational cost to the baselines that use the local only model L for query selection Note that we used a simple Entropy sampling within LoGo algorithm to measure the uncertainty and the only possible bottleneck is k means clustering in the Macro step 
61,A6.T9, CIFAR 10 SVHN FL algo Method Model 20 40 60 20 30 40 FedProx G 62 89 67 52 70 38 82 22 84 34 85 42 Entropy L 65 72 70 57 72 42 82 08 83 73 85 30 G 64 16 68 62 70 82 83 09 84 65 85 84 BADGE L 65 54 70 56 72 30 81 99 84 17 85 17 G 63 77 68 34 70 78 82 63 84 48 85 94 ALFA Mix L 63 44 67 83 70 31 80 71 82 81 84 22 LoGo G L 65 79 70 61 72 61 83 12 84 61 86 09 SCAFFOLD G 65 58 70 37 72 52 82 75 85 69 86 48 Entropy L 67 96 72 67 74 06 83 24 84 30 85 82 G 66 33 70 68 72 79 83 80 84 72 86 93 BADGE L 68 27 72 52 73 79 83 40 84 61 86 16 G 66 11 70 50 72 55 84 11 85 72 86 14 ALFA Mix L 66 11 70 00 71 91 82 15 82 89 84 74 LoGo G L 68 33 72 77 74 48 84 29 85 70 86 73 ,Table 9 Classification accuracy on two benchmarks with FedProx m mu 0 01 and SCAFFOLD We compared to three overwhelming baselines and averaged three random seeds Bold and underline mean Top 1 and Top 2 respectively ,We have further experimented with two federated learning algorithms FedProx 28 and SCAFFOLD 23 in conjunction with AL strategies Specifically we compared our LoGo with baselines that demonstrated Top 1 or Top 2 performance more than once in Table 3 The experimental configurations are same to those used in Table 3 As summarized in Table 9 LoGo consistently outperforms the baselines for both federated learning algorithms This observation suggests that LoGo is an orthogonal selection algorithm that can be integrated with any federated learning algorithm having potential to improve the performance in various applications 
62,S3.T1, Evaluation Institution 1 Institution 2 Institution 3 PSNR NMSE SSIM 5 10 20 2 5 10 2 5 10 Original 20 46 129 931 20 46 129 931 20 46 129 931 23 80 059 954 23 80 059 954 23 80 059 954 27 40 026 972 27 40 026 972 27 40 026 972 19 90 076 923 19 90 076 923 19 90 076 923 23 66 031 951 23 66 031 951 23 66 031 951 26 26 017 968 26 26 017 968 26 26 017 968 20 61 121 933 20 61 121 933 20 61 121 933 24 89 050 961 24 89 050 961 24 89 050 961 27 55 030 975 27 55 030 975 27 55 030 975 Local Single Models 25 93 034 966 25 93 034 966 25 93 034 966 27 91 021 975 27 91 021 975 27 91 021 975 30 22 012 983 30 22 012 983 30 22 012 983 25 37 021 971 25 37 021 971 25 37 021 971 27 16 014 980 27 16 014 980 27 16 014 980 28 64 010 985 28 64 010 985 28 64 010 985 26 00 035 974 26 00 035 974 26 00 035 974 28 17 023 982 28 17 023 982 28 17 023 982 29 90 018 986 29 90 018 986 29 90 018 986 Local Unified Model 26 02 032 968 26 02 032 968 26 02 032 968 28 01 020 975 28 01 020 975 28 01 020 975 30 21 012 983 30 21 012 983 30 21 012 983 25 42 020 970 25 42 020 970 25 42 020 970 27 21 014 980 27 21 014 980 27 21 014 980 28 64 010 985 28 64 010 985 28 64 010 985 26 02 035 975 26 02 035 975 26 02 035 975 28 20 023 982 28 20 023 982 28 20 023 982 29 91 018 986 29 91 018 986 29 91 018 986 FedFTN 27 24 025 999 27 24 025 999 mathbf 27 24 025 999 28 96 017 999 28 96 017 999 mathbf 28 96 017 999 30 82 011 999 30 82 011 999 mathbf 30 82 011 999 26 12 018 999 26 12 018 999 mathbf 26 12 018 999 27 80 013 999 27 80 013 999 mathbf 27 80 013 999 29 03 009 999 29 03 009 999 mathbf 29 03 009 999 26 83 031 999 26 83 031 999 mathbf 26 83 031 999 28 91 021 999 28 91 021 999 mathbf 28 91 021 999 30 23 017 999 30 23 017 999 mathbf 30 23 017 999 ,Table 1 Quantitative comparisons of the low count PET denoised images between our FedFTN and locally trained models The local single model means FTN modulated denoising networks trained at one specific low count level at the specific institution The local unified model means FTN modulated denoising networks trained with all three low count levels within each institution The performance of FedFTN with Site Adaptation SA via further local data fine tuning is reported in the last row The best results are marked in bold dagger indicates that the difference between FedFTN and all compared methods is significant at p 0 005p0 005p 0 005 based on the non parametric Wilcoxon signed rank test ,Figure 3 shows qualitative comparisons of low count PET denoised images using locally trained denoising models and our FedFTN method The lowest low count level denoised results from each institution were visualized for comparison As we can see since only less than 5 count was used the original images 1st row suffer from high noise and image artifacts While the locally trained models can reduce the noise and recover the general structure the detail recovery was still not ideal For example the 5 low count denoised image from the locally trained model at Institution 1 created additional artifacts at the intersection between the liver and kidney The 2 low count denoised images from the locally trained models at Institution 2 and Institution 3 suffered from heavy blurring on important regions such as the hypermetabolic lesions in the zoomed boxes The quantitative comparisons were summarized in Table 1 We reported quantitative results for all available low count levels for the three institutions Similar to the observation from the visualizations all the original PET images suffered from low SNR resulting in low PSNR and NMSE values Taking Institution 1 as an example we can see the locally trained unified model i e the FTN modulated Denoising Network was able to improve the PSNR from 20 46 to 26 02 for the 5 low count PET Please note that the local unified model is the network shown in Figure 1 that is trained locally with the local site s low count level as an additional input Using the FedFTN we can further improve the PSNR from 26 02 to 27 24 with statistical significance Similar observations on improvement over the locally trained models can be found for other low count levels at Institution 1 and other institutions The average inference time for testing data of institution 1 institution 2 and institution 3 were 8 08 2 83plus or minus8 082 838 08 pm 2 83 s 11 32 1 05plus or minus11 321 0511 32 pm 1 05 s and 10 04 1 09plus or minus10 041 0910 04 pm 1 09 s respectively Figure 4 presented qualitative comparisons of low count PET denoised images using different federated methods For each institution we showed one patient example for each low count level and all low count levels are visualized At Institution 1 all three low count levels denoised images suffered from low SNR with the noise level increasing as the dose level decreases While previous FL methods can improve the image quality by suppressing the noise the detail recovery was still suboptimal For instance for the 5 low count level the bone marrows in the spine were heavily blurred by the previous methods whereas FedFTN showed much sharper spine demonstrating the best consistency with the ground truth full count reconstruction Additionally for the 10 low count level a strong false positive signal was visible in the spine from the original low count reconstruction and previous methods failed to suppress it which may lead to misdiagnosis In contrast using FedFTN we can suppress this false positive signal and provided a high quality image that best matches the ground truth At Institution 2 for the 2 low count level a false positive of cardiac defect was visible from the original low count reconstruction While previous methods can suppress the noise this false positive defect signal cannot be fully removed particularly for FedSP In contrast FedFTN demonstrated significantly better performance in false positive defect removal and the most consistent cardiac shape as compared to the ground truth Similarly for the 5 low count level FedFTN better recovered the signal of the aorta wall than previous methods We observe similar results in the patient examples from Institution 3 where FedFTN can provide better image quality as compared to previous methods The corresponding quantitative comparisons were summarized in Table 2 As mentioned previously the original low count reconstructions from all three institutions suffered from poor image quality where the PSNR values were all lower than 21 0021 0021 00 at the lowest low count levels for all three institutions By deploying previous federate learning methods as compared to the locally trained models Figure 3 and Table 1 we can see that these previous FL methods can already improve the low count image quality across all institutions at all low count levels For example FedSP was able to increase the PSNR from 26 0226 0226 02 to 26 6926 6926 69 for the 5 low count reconstruction at Institution 1 from 25 4225 4225 42 to 25 8025 8025 80 for the 2 low count reconstruction at Institution 2 and from 26 0226 0226 02 to 26 4426 4426 44 for the 2 low count reconstruction at Institution 3 In the second last row we showed that our FedFTN can significantly outperform these previous FL baselines and achieved state of the art performance across all the low count levels at all three institutions For example as compared to the FedHyper with the best performance among previous FL reconstruction algorithms our FedFTN can further improve the PSNR from 26 8826 8826 88 to 27 2427 2427 24 for the 5 low count reconstruction at Institution 1 from 25 8525 8525 85 to 26 1226 1226 12 for the 2 low count reconstruction at Institution 2 and from 26 4426 4426 44 to 26 8926 8926 89 for the 2 low count reconstruction at Institution 3 We collected large scale real world low count PET data from three different institutions in the U S A Europe and China to validate our method From our experimental results we demonstrated the feasibility of using our FedFTN for collaborative training without sharing data while enabling personalized low count PET denoising at different institutions First as we can observe from Table 1 and Figure 3 even though training a denoising model from scratch using local data with limited diversity can generate reasonable denoising performance and potentially avoid domain shift issues our FedFTN can provide significantly better denoising results For example as shown in the last row of Table 1 our FedFTN demonstrated superior PSNR and NMSE values as compared to the locally trained models across all the low count levels at all the institutions This is mainly due to the fact that the FedFTN utilizes all the institutional data with a wider spectrum of data diversity for collaborative learning while using the FTN modulation to mitigate the domain shift issues Second as we can see from Table 2 and Figure 4 our method generating personalized FTN modulated denoising networks for individual institutions can consistently outperform previous FL reconstruction methods that either only produce one global model FedAvg or deploy personalize FL strategies FedSP and FedHyper Using an identical backbone reconstruction denoising network our FedFTN achieved the best image quality over all the previous FL baselines in terms of both PSNR and NMSE with statistical significance as reported in the second last row of Table 2 Further fine tuning the personalized denoising models from FedFTN with local data slightly boosted the performance In addition we found that adding the Global Weight Constraint GWC loss helped stabilize the FedFTN and improved the image quality at all low count levels at all institutions as demonstrated in Figure 7 and Table 3 The presented work also has limitations with several potential improvements that are the subjects of our future studies First our study only considered three institutions and all with 18F FDG tracer While there are other PET tracers that could be used for specific applications 18F FDG is still the most commonly used PET tracer in clinical practice and thus is the primary focus of our study Furthermore our FedFTN framework can be flexibly adjusted to different numbers of institutions and potentially adapted to multi tracer PET scenarios Specifically we could incorporate the tracer type as well as other dose and patient information as additional inputs to the FTN which would also allow the FTN to transform the features in the denoising network depending on the input tracer type In fact we believe that including more diverse tracer types with expanded training data with more diverse data representation would potentially further improve our performance In this work we have already shown that we can use FTN to adapt and unify those different low count distributions However we believe expanding to include multi tracer multi institutional data is an important future direction to validate any conclusion Second we only evaluated the overall image quality based on image quality metrics i e PSNR NMSE SSIM which use full count PET as ground truth The difference between our FL method and locally trained baselines is in the order of 1 2 dB PSNR which implies a significant image quality improvement with our FL strategy as compared to the local training strategy which can be observed from Table 1 and Figure 3 While the image quality metrics improvements from our FedFTN as compared to the prior SOTA FL methods is in smaller magnitude the image quality improvements are reflected on more detailed regions as shown in Figure 4 We believe this kind of improvement could potentially lead to more accurate disease quantification e g lesion radiomic cardiac function etc However how will such improvement over the prior SOTA FL method be reflected in clinically relevant tasks is an important direction for our following clinical investigation Given PET has extensive clinical applications in oncology cardiology and neurology our future work also includes evaluations of how the denoised image impacts the downstream tasks such as the impacts on staging and therapy response and human experts evaluations on these clinical tasks Lastly for a fair comparison and to demonstrate the idea we performed all our experiments using a simple UNet as the backbone denoising network However our method could be adapted with more advanced restoration network structures For instance we could use more advanced network designs such as cascade based transformer based and multistage based reconstruction networks 48 41 47 40 29 in our FedFTN Specifically we can use FTN to transform and modulate the intermediate features in these networks thus enabling personalized FL with these networks Deploying these networks in our FedFTN could potentially further improve our performance and will also be an important direction for our future studies 
63,S4.T2, Evaluation Institution 1 Institution 2 Institution 3 PSNR NMSE SSIM 5 10 20 2 5 10 2 5 10 Original 20 46 129 931 20 46 129 931 20 46 129 931 23 80 059 954 23 80 059 954 23 80 059 954 27 40 026 972 27 40 026 972 27 40 026 972 19 90 076 923 19 90 076 923 19 90 076 923 23 66 031 951 23 66 031 951 23 66 031 951 26 26 017 968 26 26 017 968 26 26 017 968 20 61 121 933 20 61 121 933 20 61 121 933 24 89 050 961 24 89 050 961 24 89 050 961 27 55 030 975 27 55 030 975 27 55 030 975 FedAvg 26 62 029 969 26 62 029 969 26 62 029 969 28 30 019 976 28 30 019 976 28 30 019 976 29 95 013 981 29 95 013 981 29 95 013 981 25 70 020 971 25 70 020 971 25 70 020 971 27 35 014 980 27 35 014 980 27 35 014 980 28 43 010 985 28 43 010 985 28 43 010 985 26 07 035 973 26 07 035 973 26 07 035 973 28 07 023 982 28 07 023 982 28 07 023 982 29 16 019 986 29 16 019 986 29 16 019 986 FedBN 26 68 028 970 26 68 028 970 26 68 028 970 28 35 019 978 28 35 019 978 28 35 019 978 30 05 013 982 30 05 013 982 30 05 013 982 25 79 019 973 25 79 019 973 25 79 019 973 27 47 013 981 27 47 013 981 27 47 013 981 28 60 010 986 28 60 010 986 28 60 010 986 26 31 033 975 26 31 033 975 26 31 033 975 28 34 022 983 28 34 022 983 28 34 022 983 29 38 018 987 29 38 018 987 29 38 018 987 FedProx 26 64 028 969 26 64 028 969 26 64 028 969 28 32 018 977 28 32 018 977 28 32 018 977 29 99 013 981 29 99 013 981 29 99 013 981 25 75 020 972 25 75 020 972 25 75 020 972 27 39 014 981 27 39 014 981 27 39 014 981 28 50 010 985 28 50 010 985 28 50 010 985 26 13 034 974 26 13 034 974 26 13 034 974 28 17 023 983 28 17 023 983 28 17 023 983 29 23 019 986 29 23 019 986 29 23 019 986 FedSP 26 69 028 969 26 69 028 969 26 69 028 969 28 36 019 976 28 36 019 976 28 36 019 976 30 16 013 982 30 16 013 982 30 16 013 982 25 80 019 973 25 80 019 973 25 80 019 973 27 51 013 981 27 51 013 981 27 51 013 981 28 63 010 985 28 63 010 985 28 63 010 985 26 23 034 974 26 23 034 974 26 23 034 974 28 33 022 983 28 33 022 983 28 33 022 983 29 48 018 987 29 48 018 987 29 48 018 987 FedHyper 26 88 027 971 26 88 027 971 26 88 027 971 28 56 018 978 28 56 018 978 28 56 018 978 30 33 012 983 30 33 012 983 30 33 012 983 25 85 019 974 25 85 019 974 25 85 019 974 27 49 013 982 27 49 013 982 27 49 013 982 28 65 010 986 28 65 010 986 28 65 010 986 26 44 033 976 26 44 033 976 26 44 033 976 28 52 022 983 28 52 022 983 28 52 022 983 29 73 018 987 29 73 018 987 29 73 018 987 FedFTN 27 24 025 979 27 24 025 979 mathbf 27 24 025 979 28 96 017 983 28 96 017 983 mathbf 28 96 017 983 30 82 011 990 30 82 011 990 mathbf 30 82 011 990 26 12 018 980 26 12 018 980 mathbf 26 12 018 980 27 80 013 989 27 80 013 989 mathbf 27 80 013 989 29 03 009 991 29 03 009 991 mathbf 29 03 009 991 26 83 031 980 26 83 031 980 mathbf 26 83 031 980 28 91 021 990 28 91 021 990 mathbf 28 91 021 990 30 23 017 992 30 23 017 992 mathbf 30 23 017 992 FedFTN SA 27 32 024 980 27 32 024 980 mathbf 27 32 024 980 28 99 016 985 28 99 016 985 mathbf 28 99 016 985 30 83 011 991 30 83 011 991 mathbf 30 83 011 991 26 21 017 981 26 21 017 981 mathbf 26 21 017 981 27 84 012 990 27 84 012 990 mathbf 27 84 012 990 29 05 009 992 29 05 009 992 mathbf 29 05 009 992 26 89 031 980 26 89 031 980 mathbf 26 89 031 980 28 94 021 991 28 94 021 991 mathbf 28 94 021 991 30 25 017 992 30 25 017 992 mathbf 30 25 017 992 ,Table 2 Quantitative comparisons of low count PET denoised images using different federated learning methods Each institution contains three different low count levels The best results are marked in bold dagger indicates that the difference between FedFTN and all compared methods is significant at p 0 005p0 005p 0 005 based on the non parametric Wilcoxon signed rank test ,Figure 4 presented qualitative comparisons of low count PET denoised images using different federated methods For each institution we showed one patient example for each low count level and all low count levels are visualized At Institution 1 all three low count levels denoised images suffered from low SNR with the noise level increasing as the dose level decreases While previous FL methods can improve the image quality by suppressing the noise the detail recovery was still suboptimal For instance for the 5 low count level the bone marrows in the spine were heavily blurred by the previous methods whereas FedFTN showed much sharper spine demonstrating the best consistency with the ground truth full count reconstruction Additionally for the 10 low count level a strong false positive signal was visible in the spine from the original low count reconstruction and previous methods failed to suppress it which may lead to misdiagnosis In contrast using FedFTN we can suppress this false positive signal and provided a high quality image that best matches the ground truth At Institution 2 for the 2 low count level a false positive of cardiac defect was visible from the original low count reconstruction While previous methods can suppress the noise this false positive defect signal cannot be fully removed particularly for FedSP In contrast FedFTN demonstrated significantly better performance in false positive defect removal and the most consistent cardiac shape as compared to the ground truth Similarly for the 5 low count level FedFTN better recovered the signal of the aorta wall than previous methods We observe similar results in the patient examples from Institution 3 where FedFTN can provide better image quality as compared to previous methods The corresponding quantitative comparisons were summarized in Table 2 As mentioned previously the original low count reconstructions from all three institutions suffered from poor image quality where the PSNR values were all lower than 21 0021 0021 00 at the lowest low count levels for all three institutions By deploying previous federate learning methods as compared to the locally trained models Figure 3 and Table 1 we can see that these previous FL methods can already improve the low count image quality across all institutions at all low count levels For example FedSP was able to increase the PSNR from 26 0226 0226 02 to 26 6926 6926 69 for the 5 low count reconstruction at Institution 1 from 25 4225 4225 42 to 25 8025 8025 80 for the 2 low count reconstruction at Institution 2 and from 26 0226 0226 02 to 26 4426 4426 44 for the 2 low count reconstruction at Institution 3 In the second last row we showed that our FedFTN can significantly outperform these previous FL baselines and achieved state of the art performance across all the low count levels at all three institutions For example as compared to the FedHyper with the best performance among previous FL reconstruction algorithms our FedFTN can further improve the PSNR from 26 8826 8826 88 to 27 2427 2427 24 for the 5 low count reconstruction at Institution 1 from 25 8525 8525 85 to 26 1226 1226 12 for the 2 low count reconstruction at Institution 2 and from 26 4426 4426 44 to 26 8926 8926 89 for the 2 low count reconstruction at Institution 3 Similar to the process of FTL 43 we further performed Site Adaptation SA through local fine tuning for our FedFTN The quantitative results were reported in the last row in Table 2 We can observe further improved image quality metrics of FedFTN through SA for all low count levels at all three institutions In addition we also found the denoising results from FedFTN with SA still significantly outperformed all previous FL baseline methods in terms of both PSNR and NMSE Visual comparisons of low count denoising before and after SA of FedFTN are shown in Figure 5 We can see SA improves the image quality by further suppressing the false positive signal and improving the image resolution across different low count levels at Institution 1 In this work we developed a novel personalized federated learning approach called FedFTN for multi institutional low count PET denoising Specifically we proposed to use a deep Feature Transformation Network FTN that is kept at the local institutions and takes the low count level as input to transform the intermediate feature outputs from the globally shared denoising network There are several key advantages of this design First of all with different FTNs modulating the denoising network features at different local sites we can personalize the denoising network adapting to different low count PET data with different distributions caused by differences in scanners pre post processing protocols etc Second unlike previous personalized FL modules for image classification 16 MRI synthesis 6 and 2D SVCT reconstruction 37 that use mapper sub network directly generate scalars for feature map channel wise multiplication and addition our FTN first squeezes the feature map into a latent representation and fuse with the site and subject specific latent vector before re excitation In Table 2 we found our FedFTN can provide better reconstruction performance than FedBN 16 which also uses a locally kept module but generates normalization parameters for direct feature map affine transformation Third each institution often has multiple low count protocols thus requiring the denoising network to be able to adapt to inputs with different low count levels Instead of blindly inputting the original low count reconstruction into the network without the knowledge of the low count level the FTN modulated denoising network takes the information of the low count level as additional input thus enabling dose level aware denoising Please also note that because individual FTN network is kept locally at each institution the institution can define its own low count levels The sites do not need to share the exact same set of levels and no information about the low count level is shared between institutions Even though it is possible that better performance could be achieved if different sites share identical and matched low counts having identical low counts across different sites based on different scanners different injection protocols and different processing reconstruction protocols may not be easily realizable in real world FL scenarios Lastly we also proposed a Global Weight Constraint GWC loss that regularizes the denoising network parameters not to have strong deviation over the aggregated parameter during the training at local sites helping stabilize the federated learning process and thus improving the final personalized denoising performance at each site We collected large scale real world low count PET data from three different institutions in the U S A Europe and China to validate our method From our experimental results we demonstrated the feasibility of using our FedFTN for collaborative training without sharing data while enabling personalized low count PET denoising at different institutions First as we can observe from Table 1 and Figure 3 even though training a denoising model from scratch using local data with limited diversity can generate reasonable denoising performance and potentially avoid domain shift issues our FedFTN can provide significantly better denoising results For example as shown in the last row of Table 1 our FedFTN demonstrated superior PSNR and NMSE values as compared to the locally trained models across all the low count levels at all the institutions This is mainly due to the fact that the FedFTN utilizes all the institutional data with a wider spectrum of data diversity for collaborative learning while using the FTN modulation to mitigate the domain shift issues Second as we can see from Table 2 and Figure 4 our method generating personalized FTN modulated denoising networks for individual institutions can consistently outperform previous FL reconstruction methods that either only produce one global model FedAvg or deploy personalize FL strategies FedSP and FedHyper Using an identical backbone reconstruction denoising network our FedFTN achieved the best image quality over all the previous FL baselines in terms of both PSNR and NMSE with statistical significance as reported in the second last row of Table 2 Further fine tuning the personalized denoising models from FedFTN with local data slightly boosted the performance In addition we found that adding the Global Weight Constraint GWC loss helped stabilize the FedFTN and improved the image quality at all low count levels at all institutions as demonstrated in Figure 7 and Table 3 
64,S5.T3, Institution 1 5 10 20 FedFTN w o GWC 27 11 0262 978 27 11 0262 978 27 11 0262 978 28 82 0178 982 28 82 0178 982 28 82 0178 982 30 71 0117 990 30 71 0117 990 30 71 0117 990 FedFTN w GWC 27 24 0254 979 27 24 0254 979 27 24 0254 979 28 96 0172 983 28 96 0172 983 28 96 0172 983 30 82 0113 990 30 82 0113 990 30 82 0113 990 Institution 2 2 5 10 FedFTN w o GWC 25 91 0194 979 25 91 0194 979 25 91 0194 979 27 59 0137 988 27 59 0137 988 27 59 0137 988 28 82 0103 989 28 82 0103 989 28 82 0103 989 FedFTN w GWC 26 12 0185 980 26 12 0185 980 26 12 0185 980 27 80 0130 989 27 80 0130 989 27 80 0130 989 29 03 0098 991 29 03 0098 991 29 03 0098 991 Institution 3 2 5 10 FedFTN w o GWC 26 83 0323 980 26 83 0323 980 26 83 0323 980 28 87 0223 988 28 87 0223 988 28 87 0223 988 30 14 0179 991 30 14 0179 991 30 14 0179 991 FedFTN w GWC 26 83 0319 980 26 83 0319 980 26 83 0319 980 28 91 0219 990 28 91 0219 990 28 91 0219 990 30 23 0176 992 30 23 0176 992 30 23 0176 992 ,Table 3 Ablation study on Global Weight Constraint GWC loss Quantitative comparisons of FedFTN with and without GWC loss during training are shown here ,We conducted ablative studies on the Global Weight Constraint GWC loss that was used in our FedFTN framework to stabilize the training process The results were summarized in Table 3 which included the image quality analysis for all low count levels across all three institutions Adding the GWC component consistently improved image quality compared to using FedFTN without GWC For instance at Institution 1 the PSNR for the 5 PET imaging increased from 27 1127 1127 11 to 27 2427 2427 24 from 28 8228 8228 82 to 28 9628 9628 96 for the 10 PET imaging and from 30 7130 7130 71 to 30 8230 8230 82 for the 20 PET imaging Figure 7 provided visual comparisons from multiple institutions We observed that adding GWC helps stabilize the FedFTN training process and helps maintain the subtle true positive signals within the kidneys as seen in the denoised images from Institutions 1 and 2 Similarly adding GWC results in better resolution recovery for clustered thin anatomic structures such as ribs as demonstrated in the Institution 3 comparison While the quantitative and qualitative improvements are incremental as shown in Figure 6 we can observe that FedFTN with GWC can provide faster and more stable training loss convergence as compared to the one without GWC demonstrating the GWC s benefits during FL training We collected large scale real world low count PET data from three different institutions in the U S A Europe and China to validate our method From our experimental results we demonstrated the feasibility of using our FedFTN for collaborative training without sharing data while enabling personalized low count PET denoising at different institutions First as we can observe from Table 1 and Figure 3 even though training a denoising model from scratch using local data with limited diversity can generate reasonable denoising performance and potentially avoid domain shift issues our FedFTN can provide significantly better denoising results For example as shown in the last row of Table 1 our FedFTN demonstrated superior PSNR and NMSE values as compared to the locally trained models across all the low count levels at all the institutions This is mainly due to the fact that the FedFTN utilizes all the institutional data with a wider spectrum of data diversity for collaborative learning while using the FTN modulation to mitigate the domain shift issues Second as we can see from Table 2 and Figure 4 our method generating personalized FTN modulated denoising networks for individual institutions can consistently outperform previous FL reconstruction methods that either only produce one global model FedAvg or deploy personalize FL strategies FedSP and FedHyper Using an identical backbone reconstruction denoising network our FedFTN achieved the best image quality over all the previous FL baselines in terms of both PSNR and NMSE with statistical significance as reported in the second last row of Table 2 Further fine tuning the personalized denoising models from FedFTN with local data slightly boosted the performance In addition we found that adding the Global Weight Constraint GWC loss helped stabilize the FedFTN and improved the image quality at all low count levels at all institutions as demonstrated in Figure 7 and Table 3 
65,S5.T1, Task Algorithm Dirichlet 0 3 Dirichlet 0 6 IID Train Validation Train Validation Train Validation DP FedAvg 99 28 plus or minus pm 0 02 73 10 plus or minus pm 0 16 99 55 plus or minus pm 0 02 82 20 plus or minus pm 0 35 99 66 plus or minus pm 0 40 81 90 plus or minus pm 0 86 Fed SMP rand k subscript rand k operatorname rand k 99 24 plus or minus pm 0 02 73 72 plus or minus pm 0 53 99 71 plus or minus pm 0 01 82 18 plus or minus pm 0 73 99 71 plus or minus pm 0 61 84 16 plus or minus pm 0 83 Fed SMP top k subscript top k operatorname top k 99 31 plus or minus pm 0 04 75 75 plus or minus pm 0 35 99 72 plus or minus pm 0 02 83 41 plus or minus pm 0 91 99 73 plus or minus pm 0 40 83 32 plus or minus pm 0 52 EMNIST DP FedAvg blur blur operatorname blur 99 12 plus or minus pm 0 02 73 71 plus or minus pm 0 02 99 66 plus or minus pm 0 00 83 20 plus or minus pm 0 01 99 67 plus or minus pm 0 03 82 92 plus or minus pm 0 49 DP FedAvg blurs blurs operatorname blurs 99 63 plus or minus pm 0 08 76 25 plus or minus pm 0 35 99 72 plus or minus pm 0 02 83 41 plus or minus pm 0 91 99 74 plus or minus pm 0 45 82 92 plus or minus pm 0 49 DP FedSAM 96 28 plus or minus pm 0 64 76 81 plus or minus pm 0 81 95 07 plus or minus pm 0 45 84 32 plus or minus pm 0 19 95 61 plus or minus pm 0 94 85 90 plus or minus pm 0 72 DP FedSAM top k subscript top k operatorname top k 94 77 plus or minus pm 0 11 77 27 plus or minus pm 0 67 95 87 plus or minus pm 1 52 84 80 plus or minus pm 0 60 96 12 plus or minus pm 0 85 87 70 plus or minus pm 0 83 DP FedAvg 93 65 plus or minus pm 0 47 47 98 plus or minus pm 0 24 93 65 plus or minus pm 0 42 50 05 plus or minus pm 0 47 93 65 plus or minus pm 0 15 50 90 plus or minus pm 0 86 Fed SMP rand k subscript rand k operatorname rand k 95 46 plus or minus pm 0 43 48 14 plus or minus pm 0 12 95 36 plus or minus pm 0 06 51 33 plus or minus pm 0 36 95 36 plus or minus pm 0 06 50 61 plus or minus pm 0 20 Fed SMP top k subscript top k operatorname top k 95 49 plus or minus pm 0 14 49 93 plus or minus pm 2 29 95 49 plus or minus pm 0 09 54 11 plus or minus pm 0 83 95 49 plus or minus pm 0 10 53 30 plus or minus pm 0 45 CIFAR 10 DP FedAvg blur blur operatorname blur 95 47 plus or minus pm 0 12 47 66 plus or minus pm 0 01 99 66 plus or minus pm 0 42 51 05 plus or minus pm 0 01 94 50 plus or minus pm 0 05 52 56 plus or minus pm 0 47 DP FedAvg blurs blurs operatorname blurs 96 79 plus or minus pm 0 51 51 23 plus or minus pm 0 66 99 72 plus or minus pm 0 09 54 11 plus or minus pm 0 83 96 45 plus or minus pm 0 30 53 48 plus or minus pm 0 76 DP FedSAM 90 38 plus or minus pm 0 90 53 92 plus or minus pm 0 55 90 83 plus or minus pm 0 15 54 14 plus or minus pm 0 60 90 83 plus or minus pm 0 16 55 58 plus or minus pm 0 50 DP FedSAM top k subscript top k operatorname top k 93 25 plus or minus pm 0 60 54 85 plus or minus pm 0 86 92 60 plus or minus pm 0 65 57 00 plus or minus pm 0 69 91 52 plus or minus pm 0 11 58 82 plus or minus pm 0 51 DP FedAvg 91 14 plus or minus pm 0 16 16 10 plus or minus pm 0 71 92 33 plus or minus pm 0 08 15 92 plus or minus pm 0 39 94 01 plus or minus pm 0 10 17 47 plus or minus pm 0 47 Fed SMP rand k subscript rand k operatorname rand k 90 70 plus or minus pm 0 01 17 25 plus or minus pm 0 16 92 28 plus or minus pm 0 32 17 50 plus or minus pm 0 19 94 31 plus or minus pm 0 02 17 68 plus or minus pm 0 44 Fed SMP top k subscript top k operatorname top k 92 58 plus or minus pm 0 24 18 58 plus or minus pm 0 25 93 51 plus or minus pm 0 11 18 07 plus or minus pm 0 09 95 06 plus or minus pm 0 05 19 09 plus or minus pm 0 56 CIFAR 100 DP FedAvg blur blur operatorname blur 91 27 plus or minus pm 0 01 17 03 plus or minus pm 0 09 92 33 plus or minus pm 0 03 17 92 plus or minus pm 0 01 94 01 plus or minus pm 0 04 18 47 plus or minus pm 0 02 DP FedAvg blurs blurs operatorname blurs 92 98 plus or minus pm 0 24 18 98 plus or minus pm 0 25 94 01 plus or minus pm 0 11 18 27 plus or minus pm 0 19 95 46 plus or minus pm 0 05 19 59 plus or minus pm 0 06 DP FedSAM 82 19 plus or minus pm 0 01 18 88 plus or minus pm 0 31 85 47 plus or minus pm 0 13 19 09 plus or minus pm 0 15 87 12 plus or minus pm 0 37 20 64 plus or minus pm 0 48 DP FedSAM top k subscript top k operatorname top k 84 49 plus or minus pm 0 24 20 85 plus or minus pm 0 63 88 23 plus or minus pm 0 23 21 24 plus or minus pm 0 69 89 86 plus or minus pm 0 21 22 30 plus or minus pm 0 05 ,TABLE I Averaged training accuracy and testing accuracy on two data in both IID and Non IID settings for all compared methods ,Overall performance comparison In Table I and Figure 2 we evaluate DP FedSAM and DP FedSAM topksubscripttopk operatorname top k on EMNIST CIFAR 10 and CIFAR 100 in both settings compared with all baselines It is clear that our proposed algorithms consistently outperform other baselines under symmetric noise in terms of accuracy and generalization This fact indicates that we significantly improve the performance and generate better trade off between performance and privacy in DPFL For instance the averaged testing accuracies are 85 90 percent85 9085 90 in DP FedSAM and 87 70 percent87 7087 70 in DP FedSAM topksubscripttopk operatorname top k on EMNIST in the IID setting which are better than other baselines Meanwhile the differences between training accuracy and test accuracy are 9 71 percent9 719 71 in DP FedSAM and 8 40 percent8 408 40 in DP FedSAM topksubscripttopk operatorname top k in comparison to 17 74 percent17 7417 74 in DP FedAvg and 16 41 percent16 4116 41 in Fed SMP topksubscripttopk operatorname top k respectively Consequently it shows that our algorithms significantly mitigate the performance degradation issue caused by DP Impact of Non IID levels In the experiments under different participation cases as shown in Table I we further demonstrate the robustness of the proposed algorithms in generalization The heterogeneous data distribution of local clients is set to various participation levels including IID Dirichlet 0 6 and Dirichlet 0 3 making the training of the global model more challenging On EMNIST as the Non IID level decreases DP FedSAM achieves better generalization than DP FedAvg and the differences between training and test accuracies in DP FedSAM 19 47 10 75 9 71 percent19 47percent10 75percent9 71 19 47 10 75 9 71 are lower than those in DP FedAvg 26 18 17 35 17 74 percent26 18percent17 35percent17 74 26 18 17 35 17 74 Similarly the differences in DP FedSAM topksubscripttopk operatorname top k 17 50 11 07 8 40 percent17 50percent11 07percent8 40 17 50 11 07 8 40 are also lower than those in Fed SMP topksubscripttopk operatorname top k 23 56 16 31 16 41 percent23 56percent16 31percent16 41 23 56 16 31 16 41 These observations confirm that our algorithms are more robust than baselines in various degrees of heterogeneous data 
66,S6.T2, Task Algorithm Averaged test accuracy under different privacy budgets italic epsilon italic epsilon 4 italic epsilon 6 italic epsilon 8 italic epsilon 10 CIFAR 10 DP FedAvg 38 23 plus or minus pm 0 15 43 87 plus or minus pm 0 62 46 74 plus or minus pm 0 03 49 06 plus or minus pm 0 49 Fed SMP rand k subscript rand k operatorname rand k 33 78 plus or minus pm 0 92 42 21 plus or minus pm 0 21 48 20 plus or minus pm 0 05 50 62 plus or minus pm 0 14 Fed SMP top k subscript top k operatorname top k 38 99 plus or minus pm 0 50 46 24 plus or minus pm 0 80 49 78 plus or minus pm 0 78 52 51 plus or minus pm 0 83 DP FedAvg blur blur operatorname blur 38 23 plus or minus pm 0 70 43 93 plus or minus pm 0 48 46 74 plus or minus pm 0 92 49 06 plus or minus pm 0 13 DP FedAvg blurs blurs operatorname blurs 39 39 plus or minus pm 0 43 46 64 plus or minus pm 0 36 50 18 plus or minus pm 0 27 52 91 plus or minus pm 0 57 DP FedSAM 39 89 plus or minus pm 0 17 47 92 plus or minus pm 0 23 51 30 plus or minus pm 0 95 53 18 plus or minus pm 0 40 DP FedSAM top k subscript top k operatorname top k 38 96 plus or minus pm 0 61 49 17 plus or minus pm 0 15 53 64 plus or minus pm 0 12 56 36 plus or minus pm 0 36 CIFAR 100 DP FedAvg 9 65 plus or minus pm 0 34 12 81 plus or minus pm 0 29 14 30 plus or minus pm 0 05 15 23 plus or minus pm 0 24 Fed SMP rand k subscript rand k operatorname rand k 7 95 plus or minus pm 0 91 11 51 plus or minus pm 0 18 14 20 plus or minus pm 0 90 15 92 plus or minus pm 1 03 Fed SMP top k subscript top k operatorname top k 9 90 plus or minus pm 0 89 14 22 plus or minus pm 0 82 16 57 plus or minus pm 0 75 17 71 plus or minus pm 0 36 DP FedAvg blur blur operatorname blur 9 90 plus or minus pm 0 89 14 22 plus or minus pm 0 82 16 57 plus or minus pm 0 75 17 71 plus or minus pm 0 36 DP FedAvg blurs blurs operatorname blurs 9 65 plus or minus pm 0 34 12 81 plus or minus pm 0 29 14 30 plus or minus pm 0 05 15 23 plus or minus pm 0 24 DP FedSAM 10 03 plus or minus pm 0 63 14 46 plus or minus pm 1 21 18 20 plus or minus pm 1 34 19 65 plus or minus pm 0 80 DP FedSAM top k subscript top k operatorname top k 10 08 plus or minus pm 0 68 15 26 plus or minus pm 0 78 18 99 plus or minus pm 1 38 20 79 plus or minus pm 1 07 ,TABLE II Performance comparison under different privacy budgets italic epsilon on CIFAR 10 and CIFAR 100 ,Table II shows the test accuracies under various privacy budgets italic epsilon on both CIFAR 10 and CIFAR 100 datasets Specifically on CIFAR 10 DP FedSAM and DP FedSAM topksubscripttopk operatorname top k significantly outperform DP FedAvg and Fed SMP topksubscripttopk operatorname top k by 1 4 similar topercent1percent41 sim 4 and 3 4 similar topercent3percent43 sim 4 under the same italic epsilon respectively On the more complex CIFAR 100 dataset our algorithms also have significant performance improvement That is DP FedSAM and DP FedSAM topksubscripttopk operatorname top k significantly improve the accuracy of DP FedAvg and Fed SMP topksubscripttopk operatorname top k by 1 4 similar topercent1percent41 sim 4 and 1 3 similar topercent1percent31 sim 3 under the same italic epsilon respectively Furthermore the test accuracy tends to improve as the privacy budget italic epsilon increases which suggests that a proper balance is to be maintained between training performance and privacy 
67,S6.T3, Task Performance Different sparsity ratio p p p p 1 0 p 1 0 p 1 0 p 0 1 p 0 1 p 0 1 p 0 2 p 0 2 p 0 2 p 0 4 p 0 4 p 0 4 p 0 6 p 0 6 p 0 6 p 0 8 p 0 8 p 0 8 CIFAR 10 Train 90 83 plus or minus pm 0 15 91 35 plus or minus pm 0 66 92 83 plus or minus pm 0 09 92 60 plus or minus pm 0 65 92 43 plus or minus pm 0 37 91 89 plus or minus pm 0 17 Validation 54 14 plus or minus pm 0 60 55 05 plus or minus pm 0 74 56 12 plus or minus pm 0 41 57 00 plus or minus pm 0 69 56 42 plus or minus pm 0 13 56 14 plus or minus pm 0 32 Gain compared with p 1 0 p 1 0 p 1 0 0 00 0 91 plus or minus pm 0 14 1 98 plus or minus pm 0 19 2 86 plus or minus pm 0 09 2 28 plus or minus pm 0 47 2 00 plus or minus pm 0 28 CIFAR 100 Train 85 47 plus or minus pm 0 13 86 29 plus or minus pm 0 26 87 49 plus or minus pm 0 30 88 23 plus or minus pm 0 23 86 41 plus or minus pm 0 37 85 66 plus or minus pm 0 14 Validation 19 09 plus or minus pm 0 15 20 38 plus or minus pm 0 24 20 62 plus or minus pm 0 85 21 24 plus or minus pm 0 69 20 55 plus or minus pm 0 84 19 79 plus or minus pm 0 28 Gain compared with p 1 0 p 1 0 p 1 0 0 00 1 29 plus or minus pm 0 09 1 53 plus or minus pm 0 80 2 15 plus or minus pm 0 54 1 46 plus or minus pm 0 69 0 70 plus or minus pm 0 13 ,TABLE III Performance comparison under different sparsity ratio ppp ,In Table III we investigate the impact of various sparsity ratio ppp on the model performance improvement on both CIFAR 10 and CIFAR 100 datasets Specifically we can see that the validation accuracy test accuracy is improved as the value of ppp increases from 0 10 10 1 to 0 40 40 4 and then accuracy is degraded as the value of ppp increases from 0 40 40 4 to 1 01 01 0 It indicates where having an optimal value of sparsity ratio ppp The reason lies in the balance between the information error introduced by the sparsification operation and the magnitude of random noise caused by the DP operation When the value of ppp is small enough the information error is large so that this error is a more significant factor in the ill impact on performance than the random noise in DP due to lots of parameters with added noise being discarded Meanwhile when the value of ppp is large enough the random noise is a more important factor in the ill impact on performance than the information error Therefore a proper trade off between the information error and the magnitude of random noise is achieved when the value of ppp is 0 40 40 4 Note that DP FedSAM is the same as DP FedSAM topksubscripttopk operatorname top k when p 1 0p1 0p 1 0 
68,S6.T4, Algorithm Train Validation Differential value DP FedAvg 99 55 plus or minus pm 0 02 82 20 plus or minus pm 0 35 17 35 plus or minus pm 0 32 DP FedSAM 95 07 plus or minus pm 0 45 84 32 plus or minus pm 0 19 uparrow 10 75 plus or minus pm 0 26 downarrow Fed SMP top k subscript top k operatorname top k 99 72 plus or minus pm 0 02 83 41 plus or minus pm 0 91 16 31 plus or minus pm 0 89 DP FedSAM top k subscript top k operatorname top k 95 87 plus or minus pm 0 52 84 80 plus or minus pm 0 60 uparrow 11 07 plus or minus pm 0 08 downarrow ,TABLE IV The averaged training accuracy and testing accuracy ,Effect of SAM As shown in Table IV it is clear that DP FedSAM and DP FedSAM topksubscripttopk operatorname top k can achieve noticeable performance improvement and better generalization compared with DP FedAvg and Fed SMP topksubscripttopk operatorname top k when the SAM optimizer is adopted 
69,S4.T1, N N N s s s r rho T 50 T 50 T 50 T 100 T 100 T 100 T 200 T 200 T 200 T 300 T 300 T 300 T 400 T 400 T 400 T 500 T 500 T 500 superscript italic epsilon prime d superscript delta prime superscript italic epsilon prime d superscript delta prime superscript italic epsilon prime d superscript delta prime superscript italic epsilon prime d superscript delta prime superscript italic epsilon prime d superscript delta prime superscript italic epsilon prime d superscript delta prime 100 100 100 1 1 1 1 00 2 subscript 1 00 2 1 00 2 4 26 2 subscript 4 26 2 4 26 2 1 05 3 subscript 1 05 3 1 05 3 6 04 2 subscript 6 04 2 6 04 2 1 10 3 subscript 1 10 3 1 10 3 8 55 2 subscript 8 55 2 8 55 2 1 20 3 subscript 1 20 3 1 20 3 1 05 1 subscript 1 05 1 1 05 1 1 30 3 subscript 1 30 3 1 30 3 1 21 1 subscript 1 21 1 1 21 1 1 40 3 subscript 1 40 3 1 40 3 1 36 1 subscript 1 36 1 1 36 1 1 50 3 subscript 1 50 3 1 50 3 100 100 100 5 5 5 5 00 2 subscript 5 00 2 5 00 2 1 58 1 58 1 58 2 25 3 subscript 2 25 3 2 25 3 2 32 2 32 2 32 3 50 3 subscript 3 50 3 3 50 3 3 46 3 46 3 46 6 00 3 subscript 6 00 3 6 00 3 4 41 4 41 4 41 8 50 3 subscript 8 50 3 8 50 3 5 25 5 25 5 25 1 10 2 subscript 1 10 2 1 10 2 6 03 6 03 6 03 1 35 2 subscript 1 35 2 1 35 2 200 200 200 1 1 1 5 00 3 subscript 5 00 3 5 00 3 2 13 2 subscript 2 13 2 2 13 2 1 03 3 subscript 1 03 3 1 03 3 3 01 2 subscript 3 01 2 3 01 2 1 05 3 subscript 1 05 3 1 05 3 4 26 2 subscript 4 26 2 4 26 2 1 10 3 subscript 1 10 3 1 10 3 5 23 2 subscript 5 23 2 5 23 2 1 15 3 subscript 1 15 3 1 15 3 6 04 2 subscript 6 04 2 6 04 2 1 20 3 subscript 1 20 3 1 20 3 6 76 2 subscript 6 76 2 6 76 2 1 25 3 subscript 1 25 3 1 25 3 500 500 500 5 5 5 1 00 2 subscript 1 00 2 1 00 2 2 98 1 subscript 2 98 1 2 98 1 1 25 3 subscript 1 25 3 1 25 3 4 25 1 subscript 4 25 1 4 25 1 1 50 3 subscript 1 50 3 1 50 3 6 09 1 subscript 6 09 1 6 09 1 2 00 3 subscript 2 00 3 2 00 3 7 52 1 subscript 7 52 1 7 52 1 2 50 3 subscript 2 50 3 2 50 3 8 75 1 subscript 8 75 1 8 75 1 3 00 3 subscript 3 00 3 3 00 3 9 85 1 subscript 9 85 1 9 85 1 3 50 3 subscript 3 50 3 3 50 3 300 300 300 5 5 5 1 67 2 subscript 1 67 2 1 67 2 5 02 1 subscript 5 02 1 5 02 1 1 42 3 subscript 1 42 3 1 42 3 7 20 1 subscript 7 20 1 7 20 1 1 83 3 subscript 1 83 3 1 83 3 1 04 1 04 1 04 2 67 3 subscript 2 67 3 2 67 3 1 29 1 29 1 29 3 50 3 subscript 3 50 3 3 50 3 1 51 1 51 1 51 4 33 3 subscript 4 33 3 4 33 3 1 70 1 70 1 70 5 17 3 subscript 5 17 3 5 17 3 300 300 300 10 10 10 3 33 2 subscript 3 33 2 3 33 2 3 52 3 52 3 52 2 67 3 subscript 2 67 3 2 67 3 5 36 5 36 5 36 4 33 3 subscript 4 33 3 4 33 3 8 32 8 32 8 32 7 67 3 subscript 7 67 3 7 67 3 1 09 1 subscript 1 09 1 1 09 1 1 10 2 subscript 1 10 2 1 10 2 1 33 1 subscript 1 33 1 1 33 1 1 43 2 subscript 1 43 2 1 43 2 1 55 1 subscript 1 55 1 1 55 1 1 77 2 subscript 1 77 2 1 77 2 400 400 400 5 5 5 1 25 2 subscript 1 25 2 1 25 2 3 74 1 subscript 3 74 1 3 74 1 1 31 3 subscript 1 31 3 1 31 3 5 35 1 subscript 5 35 1 5 35 1 1 63 3 subscript 1 63 3 1 63 3 7 68 1 subscript 7 68 1 7 68 1 2 25 3 subscript 2 25 3 2 25 3 9 51 1 subscript 9 51 1 9 51 1 2 88 3 subscript 2 88 3 2 88 3 1 11 1 11 1 11 3 50 3 subscript 3 50 3 3 50 3 1 25 1 25 1 25 4 13 3 subscript 4 13 3 4 13 3 400 400 400 10 10 10 2 50 2 subscript 2 50 2 2 50 2 2 56 2 56 2 56 2 25 3 subscript 2 25 3 2 25 3 3 83 3 83 3 83 3 50 3 subscript 3 50 3 3 50 3 5 84 5 84 5 84 6 00 3 subscript 6 00 3 6 00 3 7 55 7 55 7 55 8 50 3 subscript 8 50 3 8 50 3 9 11 9 11 9 11 1 10 2 subscript 1 10 2 1 10 2 1 06 1 subscript 1 06 1 1 06 1 1 35 2 subscript 1 35 2 1 35 2 ,Table 1 The privacy parameters d superscriptitalic superscript epsilon prime delta prime of Algorithm 1 against the number of outer iterations with d 0 15 10 4 italic 0 15superscript104 epsilon delta 0 15 10 4 and d 10 3 superscript103 hat delta 10 3 The number a b ckformulae sequenceabsubscriptcka bc k means a b cx10kformulae sequenceabcsuperscript10ka bc times 10 k ,Table 1 reports the privacy parameter pairs d superscriptitalic superscript epsilon prime delta prime in Theorem 4 2 using multiple parameters i e the number of agents NNN the number of sampled agents sss the sampling rate r rho the number of iterations TTT From Table 1 we observe that for a suitable size of sampling e g s 5s5s 5 the privacy budget from Theorem 4 2 for Algorithm 1 is controlled in a reasonable range In addition if the sampling rate is sufficiently small then the number of outer iterations TTT can be large while keeping superscriptitalic epsilon prime and d superscript delta prime reasonably small The numerical results in Section 6 show that using a small sampling rate e g sampling only one agent the resulting trained model has satisfactory accuracy with a reasonable size of the number of outer iterations TTT 
70,S1.T1, Abbreviation Expanded Form Abbreviation Expanded Form AD A lzheimer 8217 s D isease AGM A ccelerated G radient M ethod APM A ccelerated P roximal M ethod CE C ross E ntropy CNN C onvolutional N eural N etwork CT C omputed T omography CV C omputer V ision DA D omain A daptation DL D eep L earning DNN D eep N eural N etwork FCN F ully C onvolutional N etwork FNN F eedforward N eural N etwork FSL F ew S hot L earning GAN G enerative A dversarial N etwork GCN G raph C onvolutional N etwork GNN G raph N eural N etwork GP G aussian P rocess GPT G enerative P retrained T ransformer GPU G raphics P rocessing U nit GRL G radient R eversal L ayer I O I nput O utput KD K nowledge D istillation LLM L arge L anguage M odel LSTM L ong S hort T erm M emory MAP M aximum A P osteriori MCI M ild C ognitive I mpairment MDP M arkov D ecision P rocess MIM M asked I mage M odeling MIML M ulti I nstance M ulti L abel learning MIMO M ulti I nput M ulti O utput MISO M ulti I nput S ingle O utput ML M achine L earning MLM M asked L anguage M odeling MLP M ulti L ayer P erceptron MoE M ixture o f E xperts MOO M ulti O bjective O ptimization MRI M agnetic R esonance I maging MSE M ean S quared E rror MTL M ulti T ask L earning MTRL M ulti T ask R einforcement L earning MVL M ulti V iew L earning NAS N eural A rchitecture S earch NLI N atural L anguage I nference NLP N atural L anguage P rocessing OCR O ptical C haracter R ecognition OOD O ut O f D istribution PET P ositron E mission T omography PFM P retrained F oundation M odel PSD P ositive S emi D efinite RL R einforcement L earning RNN R ecurrent N eural N etwork seq2seq seq uence to seq uence SIMO S ingle I nput M ulti O utput SNP S ingle N ucleotide P olymorphism SGD S tochastic G radient D escent SSL S elf S upervised L earning SOTA S tate O f T he A rt STL S ingle T ask L earning SVD S ingular V alue D ecomposition SVM S upport V ector M achine TL T ransfer L earning TPU T ensor P rocessing U nit VLM V ision L anguage M odel VQA V isual Q uestion A nswering ZSL Z ero S hot L earning ,Table 1 Alphabetically sorted index table of acronyms ,In SS 1 1 we progressively introduce Multi Task Learning MTL starting with a broad sense and culminating in a formal definition Subsequently SS 1 2 explores the position of MTL within the Machine Learning ML landscape drawing comparisons with related paradigms such as Transfer Learning TL Few Shot Learning FSL lifelong learning Multi View Learning MVL to name a few SS 1 3 delves into the motivations for employing MTL offering insights from both explicit and subtle angles while also addressing how MTL benefits the involved tasks In SS 1 4 we delve deeper into the fundamental mechanisms and theories underpinning MTL specifically 1 regularization 2 inductive bias and 3 feature sharing providing an understanding of its underlying principles Finally SS 1 5 reviews existing surveys on MTL underscoring the unique contributions of our survey and laying out a structured roadmap for the remainder of this work The structure of our survey is depicted in Fig 2 Before delving into this survey readers can quickly refer to Table 1 for a list of acronyms not related to datasets institutions and newly proposed methods while an overview of mathematical notations is provided in Table 3 and Table 6 
71,S2.T2, I O Data Modality Task Type MTL Strategy Assumption SIMO MISO MIMO Table Image Text Graph Regression Classification Dense Prediction Feature Selection 1 10003 10003 10003 10007 Decomposition 1 10003 10003 10003 10003 Regularization Low Rank Factorization 1 10003 10003 10003 10003 10003 Priori Sharing 1 10003 10003 10003 10003 10003 10003 Task Clustering Grouping 1 10003 10003 10003 Group Based Learning 1 10003 10007 10007 10003 10003 10003 Relationship Learning Mixture of Experts 1 10003 10003 10003 10003 10003 Feature Fusion 2 10003 10007 10007 10003 10003 10003 10003 10003 Cascading 2 10003 10007 10007 10003 10003 10003 10003 10003 Knowledge Distillation 2 10003 10007 10007 10003 10003 10003 10003 10003 Feature Propagation Cross Task Attention 2 10003 10007 10007 10007 10003 10003 10003 10003 Scalarization 3 10003 10003 10003 10003 10003 10003 10003 10003 Multi Objective Optimization 3 10003 10003 10003 10003 10003 10003 10003 10003 Adversarial Training 3 10003 10003 10003 10003 10003 10003 10003 Optimization Neural Architecture Search 1 10003 10003 10003 10003 10003 Downstream Fine tuning 1 10003 10007 10007 10003 10003 10003 10003 10003 10003 10003 Task Prompting 1 10003 10003 10003 10003 10003 10003 10003 10003 10003 Pre training Multi Modal Unification 1 10003 10003 10003 10003 10003 10003 10003 10003 10003 ,Table 2 Summary of MTL methods discussed in SS 2 2 ,To accommodate data in Eq 2 it is necessary to consider various input output I O configurations that may impose constraints on the MTL modeling process For instance tasks such as semantic segmentation and depth estimation can utilize the same input images and the applications are always developed using datasets where each image is attached with dense prediction labels for both segmentation and depth On the other hand when dealing with a digital recognition problem involving multiple domains e g handwritten digits and license plate digits different inputs are mapped to the same output space We refer the former as a single input multi output SIMO configuration and the latter as a multi input single output MISO configuration In MTL the most prevalent scenarios reside in multi input multi output MIMO configuration where each task maintains its own set of samples and the labels are omnivorous e g autonomous driving that involves pedestrian detection and traffic sign recognition Let us denote the data input space and its corresponding label space for the t 119905 t italic t t 119905 t italic t t t 119905 119905 t t t italic t italic t th task t 1 8943 T 119905 1 8943 119879 t 1 cdots T italic t 1 8943 italic T t 1 8943 T 119905 1 8943 119879 t 1 cdots T italic t 1 8943 italic T t 1 8943 T t 1 8943 T t t 1 8943 T 1 1 8943 T T 119905 1 8943 119879 119905 1 8943 119879 119905 t 1 8943 119879 1 1 8943 119879 T t 1 cdots T t 1 cdots T italic t 1 8943 italic T italic t 1 italic T by 119987 t superscript 119987 119905 mathcal X t caligraphic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119987 t superscript 119987 119905 mathcal X t caligraphic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119987 t 119987 X t t t superscript 119987 119905 superscript 119987 119905 superscript superscript 119987 X 119905 t mathcal X t mathcal X t caligraphic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT and 119988 t superscript 119988 119905 mathcal Y t caligraphic Y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119988 t superscript 119988 119905 mathcal Y t caligraphic Y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119988 t 119988 Y t t t superscript 119988 119905 superscript 119988 119905 superscript superscript 119988 Y 119905 t mathcal Y t mathcal Y t caligraphic Y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic Y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT respectively We classify the MTL problems into three cases SIMO MISO and MIMO Fig 5 shows the illustration of these three configurations It is worth noting that the I O configurations do not significantly impact the taxonomy of methods in MTL As indicated in Table 2 there are numerous shared practices of applying different methods to these I O configurations as well as various data modalities and task types We acknowledge that the presented taxonomy is not exhaustive and certain methods may be classified differently when viewed from a different perspective For example Task Tree TAT han2015learning han2015learning a clustering MTL method establishes task hierarchy by decomposing the parameter matrix into different component matrices for each tree layer we discuss it within the context of clustering MTL see SS 2 1 6 We also acknowledge that some methods that may be of interest to readers may not be included in this survey due to similarities or oversight We welcome paper recommendations and will update the survey on our project page accordingly 2 2 2 22https github com junfish Awesome Multitask Learning In Table 2 we summarize their assumptions common practice and technical constraints of these topics in terms of I O configuration data modality and task type 
72,S2.T3, Notation Description n N 8712 8477 119899 119873 8477 n N in mathbb R italic n italic N 8712 blackboard R Scalars are denoted by plain lowercase or uppercase letters object The number of object e g task denoting the number of task 119961 119961 boldsymbol x bold italic x or 119961 8594 8712 8477 N 8594 119961 superscript 8477 119873 vec boldsymbol x in mathbb R N over 8594 start ARG bold italic x end ARG 8712 blackboard R start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT A vector 119961 119961 boldsymbol x bold italic x with N 119873 N italic N entries denoted by bold lowercase letters 119935 8712 8477 M 215 N 119935 superscript 8477 119872 119873 boldsymbol X in mathbb R M times N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT A matrix 119935 119935 boldsymbol X bold italic X with size M 215 N 119872 119873 M times N italic M 215 italic N denoted by bold uppercase letters 120039 8712 8477 I 1 215 8943 215 I N 120039 superscript 8477 subscript 119868 1 8943 subscript 119868 119873 boldsymbol mathcal X in mathbb R I 1 times cdots times I N bold caligraphic X 8712 blackboard R start POSTSUPERSCRIPT italic I start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 215 8943 215 italic I start POSTSUBSCRIPT italic N end POSTSUBSCRIPT end POSTSUPERSCRIPT A tensor 120039 120039 boldsymbol mathcal X bold caligraphic X with size 8477 I 1 215 8943 215 I N superscript 8477 subscript 119868 1 8943 subscript 119868 119873 mathbb R I 1 times cdots times I N blackboard R start POSTSUPERSCRIPT italic I start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 215 8943 215 italic I start POSTSUBSCRIPT italic N end POSTSUBSCRIPT end POSTSUPERSCRIPT denoted by bold calligraphic letters 8902 i i 1 N superscript subscript superscript 8902 119894 119894 1 119873 star i i 1 N 8902 start POSTSUPERSCRIPT italic i end POSTSUPERSCRIPT start POSTSUBSCRIPT italic i 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT A set contains 8902 1 8943 8902 N superscript 8902 1 8943 superscript 8902 119873 star 1 cdots star N 8902 start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 8943 8902 start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT where 8902 8902 star 8902 could be anything e g scalar vector data pair learner etc x n 8712 8477 subscript 119909 119899 8477 x n in mathbb R italic x start POSTSUBSCRIPT italic n end POSTSUBSCRIPT 8712 blackboard R The n 119899 n italic n th entry for vector 119961 8712 8477 N n 8712 1 2 8943 N formulae sequence 119961 superscript 8477 119873 119899 1 2 8943 119873 boldsymbol x in mathbb R N n in 1 2 cdots N bold italic x 8712 blackboard R start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT italic n 8712 1 2 8943 italic N x m n subscript 119909 119898 119899 x m n italic x start POSTSUBSCRIPT italic m italic n end POSTSUBSCRIPT or 119935 m n 8712 8477 subscript delimited 119935 119898 119899 8477 boldsymbol X m n in mathbb R bold italic X start POSTSUBSCRIPT italic m italic n end POSTSUBSCRIPT 8712 blackboard R The m n 119898 119899 m n italic m italic n th entry of matrix 119935 8712 8477 M 215 N m 8712 1 2 8943 M n 8712 1 2 8943 N formulae sequence 119935 superscript 8477 119872 119873 formulae sequence 119898 1 2 8943 119872 119899 1 2 8943 119873 boldsymbol X in mathbb R M times N m in 1 2 cdots M n in 1 2 cdots N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT italic m 8712 1 2 8943 italic M italic n 8712 1 2 8943 italic N 119935 8857 119936 8712 8477 M 215 N direct product 119935 119936 superscript 8477 119872 119873 boldsymbol X odot boldsymbol Y in mathbb R M times N bold italic X 8857 bold italic Y 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT Element wise product of 119935 8712 8477 M 215 N 119935 superscript 8477 119872 119873 boldsymbol X in mathbb R M times N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT and 119936 8712 8477 M 215 N 119936 superscript 8477 119872 119873 boldsymbol Y in mathbb R M times N bold italic Y 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT which means the m n 119898 119899 m n italic m italic n th entry of 119935 8857 119936 direct product 119935 119936 boldsymbol X odot boldsymbol Y bold italic X 8857 bold italic Y is x m n 8290 y m n subscript 119909 119898 119899 subscript 119910 119898 119899 x m n y m n italic x start POSTSUBSCRIPT italic m italic n end POSTSUBSCRIPT italic y start POSTSUBSCRIPT italic m italic n end POSTSUBSCRIPT 119961 n 8712 8477 M superscript 119961 119899 superscript 8477 119872 boldsymbol x n in mathbb R M bold italic x start POSTSUPERSCRIPT italic n end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic M end POSTSUPERSCRIPT The n 119899 n italic n th column vector of matrix 119935 8712 8477 M 215 N n 8712 1 2 8943 N formulae sequence 119935 superscript 8477 119872 119873 119899 1 2 8943 119873 boldsymbol X in mathbb R M times N n in 1 2 cdots N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT italic n 8712 1 2 8943 italic N 119961 m 8712 8477 N subscript 119961 119898 superscript 8477 119873 boldsymbol x m in mathbb R N bold italic x start POSTSUBSCRIPT italic m end POSTSUBSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT The m 119898 m italic m th row vector of matrix 119935 8712 8477 M 215 N m 8712 1 2 8943 M formulae sequence 119935 superscript 8477 119872 119873 119898 1 2 8943 119872 boldsymbol X in mathbb R M times N m in 1 2 cdots M bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT italic m 8712 1 2 8943 italic M 119920 N 215 N 8712 8477 N 215 N subscript 119920 119873 119873 superscript 8477 119873 119873 boldsymbol I N times N in mathbb R N times N bold italic I start POSTSUBSCRIPT italic N 215 italic N end POSTSUBSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N 215 italic N end POSTSUPERSCRIPT The identity matrix of size N 215 N 119873 119873 N times N italic N 215 italic N which has ones on the diagonal and zeros elsewhere tr 119935 8712 8477 119935 8477 boldsymbol X in mathbb R bold italic X 8712 blackboard R The trace of a matrix 119935 8712 8477 N 215 N 119935 superscript 8477 119873 119873 boldsymbol X in mathbb R N times N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic N 215 italic N end POSTSUPERSCRIPT defined as the sum of its N 119873 N italic N components on the diagonal col 119935 8838 8477 M 119935 superscript 8477 119872 boldsymbol X subseteq mathbb R M bold italic X 8838 blackboard R start POSTSUPERSCRIPT italic M end POSTSUPERSCRIPT The column space of a matrix 119935 8712 8477 M 215 N 119935 superscript 8477 119872 119873 boldsymbol X in mathbb R M times N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT which consists of all linear combinations of its column vectors rank 119935 8712 8477 119935 8477 boldsymbol X in mathbb R bold italic X 8712 blackboard R The rank of matrix 119935 119935 boldsymbol X bold italic X defined as the maximum number of linearly independent column or row vectors of 119935 119935 boldsymbol X bold italic X vec 119935 8712 8477 M 8290 N 119935 superscript 8477 119872 119873 boldsymbol X in mathbb R MN bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M italic N end POSTSUPERSCRIPT The vectorization of the matrix 119935 8712 8477 M 215 N 119935 superscript 8477 119872 119873 boldsymbol X in mathbb R M times N bold italic X 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT in the row by row stacking way 119915 8712 8477 N 215 M superscript 119915 superscript 8477 119873 119872 boldsymbol D in mathbb R N times M bold italic D start POSTSUPERSCRIPT end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N 215 italic M end POSTSUPERSCRIPT The pseudoinverse of a matrix 119915 8712 8477 M 215 N 119915 superscript 8477 119872 119873 boldsymbol D in mathbb R M times N bold italic D 8712 blackboard R start POSTSUPERSCRIPT italic M 215 italic N end POSTSUPERSCRIPT 119926 N 8834 8477 N 215 N superscript 119926 119873 superscript 8477 119873 119873 boldsymbol O N subset mathbb R N times N bold italic O start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT 8834 blackboard R start POSTSUPERSCRIPT italic N 215 italic N end POSTSUPERSCRIPT The set of N 215 N 119873 119873 N times N italic N 215 italic N orthogonal matrices 119935 8712 119926 N 119935 superscript 119926 119873 boldsymbol X in boldsymbol O N bold italic X 8712 bold italic O start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT The column vectors 119961 1 8943 119961 N superscript 119961 1 8943 superscript 119961 119873 boldsymbol x 1 cdots boldsymbol x N bold italic x start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 8943 bold italic x start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT of matrix 119935 119935 boldsymbol X bold italic X are orthogonal 119930 N 8834 8477 N 215 N superscript 119930 119873 superscript 8477 119873 119873 boldsymbol S N subset mathbb R N times N bold italic S start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT 8834 blackboard R start POSTSUPERSCRIPT italic N 215 italic N end POSTSUPERSCRIPT The set of N 215 N 119873 119873 N times N italic N 215 italic N real symmetric matrices 119930 N 8834 119930 N superscript subscript 119930 119873 superscript 119930 119873 boldsymbol S N subset boldsymbol S N bold italic S start POSTSUBSCRIPT end POSTSUBSCRIPT start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT 8834 bold italic S start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT The subset of 119930 N superscript 119930 119873 boldsymbol S N bold italic S start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT that contains positive semidefinit matrices 8214 119960 8214 1 subscript norm 119960 1 boldsymbol w 1 8741 bold italic w 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm of a vector calculated as the sum of the absolute vector values 8214 119960 8214 2 subscript norm 119960 2 boldsymbol w 2 8741 bold italic w 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT The 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT norm of a vector calculated as the square root of the sum of the squared vector values 8214 119960 8214 8734 subscript norm 119960 boldsymbol w infty 8741 bold italic w 8741 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT The 8467 8734 subscript 8467 ell infty roman 8467 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT norm of a vector calculated as the maximum of the absolute vector values 8214 119934 8214 0 subscript norm 119934 0 boldsymbol W 0 8741 bold italic W 8741 start POSTSUBSCRIPT 0 end POSTSUBSCRIPT The 8467 0 subscript 8467 0 ell 0 roman 8467 start POSTSUBSCRIPT 0 end POSTSUBSCRIPT norm i e cardinality of a matrix defined as the number of nonzero components 8214 119934 8214 1 subscript norm 119934 1 boldsymbol W 1 8741 bold italic W 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm of a matrix calculated as the maximum of the 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm of the column vectors 8214 119934 8214 2 subscript norm 119934 2 boldsymbol W 2 8741 bold italic W 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT The 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT norm of a matrix calculated as its maximum singular value 8214 119934 8214 F subscript norm 119934 119865 boldsymbol W F 8741 bold italic W 8741 start POSTSUBSCRIPT italic F end POSTSUBSCRIPT The Frobenius norm of a matrix calculated as the square root of the sum of the squared matrix values 963 r 8290 119934 r 1 R superscript subscript subscript 120590 119903 119934 119903 1 119877 sigma r boldsymbol W r 1 R italic 963 start POSTSUBSCRIPT italic r end POSTSUBSCRIPT bold italic W start POSTSUBSCRIPT italic r 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic R end POSTSUPERSCRIPT The set of non increasing ordered singular values of matrix 119934 119934 boldsymbol W bold italic W 8214 119934 8214 8727 subscript norm 119934 boldsymbol W 8741 bold italic W 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT The trace norm of a matrix defined as the sum of its singular values i e 8721 r 1 R 963 r 8290 119934 superscript subscript 119903 1 119877 subscript 120590 119903 119934 sum r 1 R sigma r boldsymbol W 8721 start POSTSUBSCRIPT italic r 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic R end POSTSUPERSCRIPT italic 963 start POSTSUBSCRIPT italic r end POSTSUBSCRIPT bold italic W 8214 119934 8214 8734 subscript norm 119934 boldsymbol W infty 8741 bold italic W 8741 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT The 8467 8734 subscript 8467 ell infty roman 8467 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT norm of a matrix calculated as the maximum of the 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm of the row vectors 8214 119934 8214 p q subscript norm 119934 119901 119902 boldsymbol W p q 8741 bold italic W 8741 start POSTSUBSCRIPT italic p italic q end POSTSUBSCRIPT The 8467 p q subscript 8467 119901 119902 ell p q roman 8467 start POSTSUBSCRIPT italic p italic q end POSTSUBSCRIPT norm of a matrix defined as the q 119902 q italic q norm of the vector whose components are p 119901 p italic p norm of 119934 119934 boldsymbol W bold italic W 8217 s row vectors 8214 119934 8214 1 1 subscript norm 119934 1 1 boldsymbol W 1 1 8741 bold italic W 8741 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT The 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm of a matrix defined as the sum of the absolute matrix components 8214 119934 8214 1 2 subscript norm 119934 1 2 boldsymbol W 1 2 8741 bold italic W 8741 start POSTSUBSCRIPT 1 2 end POSTSUBSCRIPT The 8467 1 2 subscript 8467 1 2 ell 1 2 roman 8467 start POSTSUBSCRIPT 1 2 end POSTSUBSCRIPT norm of a matrix calculated as the 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT norm of the vector whose components are 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm of the row vectors 8214 119934 8214 2 1 subscript norm 119934 2 1 boldsymbol W 2 1 8741 bold italic W 8741 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT The 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm of a matrix calculated as the sum of the 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT norm of the row vectors ,Table 3 Summary of basic notations used in this paper ,In SS 1 1 we progressively introduce Multi Task Learning MTL starting with a broad sense and culminating in a formal definition Subsequently SS 1 2 explores the position of MTL within the Machine Learning ML landscape drawing comparisons with related paradigms such as Transfer Learning TL Few Shot Learning FSL lifelong learning Multi View Learning MVL to name a few SS 1 3 delves into the motivations for employing MTL offering insights from both explicit and subtle angles while also addressing how MTL benefits the involved tasks In SS 1 4 we delve deeper into the fundamental mechanisms and theories underpinning MTL specifically 1 regularization 2 inductive bias and 3 feature sharing providing an understanding of its underlying principles Finally SS 1 5 reviews existing surveys on MTL underscoring the unique contributions of our survey and laying out a structured roadmap for the remainder of this work The structure of our survey is depicted in Fig 2 Before delving into this survey readers can quickly refer to Table 1 for a list of acronyms not related to datasets institutions and newly proposed methods while an overview of mathematical notations is provided in Table 3 and Table 6 To comprehensively understand MTL we provide a formal definition of MTL Suppose we have a sample dataset 119935 119935 boldsymbol X bold italic X 119935 119935 boldsymbol X bold italic X 119935 X 119935 119935 X boldsymbol X boldsymbol X bold italic X bold italic X drawn from the feature space 119987 119987 mathcal X caligraphic X 119987 119987 mathcal X caligraphic X 119987 X 119987 119987 X mathcal X mathcal X caligraphic X caligraphic X and its respective ground truth label set 119936 119936 boldsymbol Y bold italic Y 119936 119936 boldsymbol Y bold italic Y 119936 Y 119936 119936 Y boldsymbol Y boldsymbol Y bold italic Y bold italic Y drawn from the label space 119988 119988 mathcal Y caligraphic Y 119988 119988 mathcal Y caligraphic Y 119988 Y 119988 119988 Y mathcal Y mathcal Y caligraphic Y caligraphic Y We can define experience 8496 8838 119935 119936 8496 119935 119936 mathcal E subseteq boldsymbol X boldsymbol Y caligraphic E 8838 bold italic X bold italic Y 8496 8838 119935 119936 8496 119935 119936 mathcal E subseteq boldsymbol X boldsymbol Y caligraphic E 8838 bold italic X bold italic Y 8496 8838 119935 119936 8496 E 8838 119935 119936 119935 X 119936 Y 8496 119935 119936 8496 119935 119936 8496 E 119935 119936 119935 X 119936 Y mathcal E subseteq boldsymbol X boldsymbol Y mathcal E subseteq boldsymbol X boldsymbol Y caligraphic E 8838 bold italic X bold italic Y caligraphic E bold italic X bold italic Y domain 119967 119987 P 8290 119935 119967 119987 119875 119935 mathcal D mathcal X P boldsymbol X caligraphic D caligraphic X italic P bold italic X 119967 119987 P 8290 119935 119967 119987 119875 119935 mathcal D mathcal X P boldsymbol X caligraphic D caligraphic X italic P bold italic X 119967 119987 P 8290 119935 119967 D 119987 P 8290 119935 119987 X P 8290 119935 P P 8290 119935 119935 X 119967 119987 119875 119935 119967 119987 119875 119935 119967 D 119987 119875 119935 119987 X 119875 119935 119875 P 119935 X mathcal D mathcal X P boldsymbol X mathcal D mathcal X P boldsymbol X caligraphic D caligraphic X italic P bold italic X caligraphic D caligraphic X italic P bold italic X and task 119983 119988 f 119983 119988 119891 mathcal T mathcal Y f caligraphic T caligraphic Y italic f 119983 119988 f 119983 119988 119891 mathcal T mathcal Y f caligraphic T caligraphic Y italic f 119983 119988 f 119983 T 119988 f 119988 Y f f 119983 119988 119891 119983 119988 119891 119983 T 119988 119891 119988 Y 119891 f mathcal T mathcal Y f mathcal T mathcal Y f caligraphic T caligraphic Y italic f caligraphic T caligraphic Y italic f where P 8290 119935 119875 119935 P boldsymbol X italic P bold italic X P 8290 119935 119875 119935 P boldsymbol X italic P bold italic X P 8290 119935 P P 8290 119935 119935 X 119875 119935 119875 119935 119875 P 119935 X P boldsymbol X P boldsymbol X italic P bold italic X italic P bold italic X is the distribution of 119935 119935 boldsymbol X bold italic X 119935 119935 boldsymbol X bold italic X 119935 X 119935 119935 X boldsymbol X boldsymbol X bold italic X bold italic X and f 119891 f italic f f 119891 f italic f f f 119891 119891 f f f italic f italic f maps a data sample 119961 8712 119935 119961 119935 boldsymbol x in boldsymbol X bold italic x 8712 bold italic X 119961 8712 119935 119961 119935 boldsymbol x in boldsymbol X bold italic x 8712 bold italic X 119961 8712 119935 119961 x 8712 119935 X 119961 119935 119961 119935 119961 x 119935 X boldsymbol x in boldsymbol X boldsymbol x in boldsymbol X bold italic x 8712 bold italic X bold italic x bold italic X to a prediction 119962 8712 119936 119962 119936 tilde boldsymbol y in boldsymbol Y over start ARG bold italic y end ARG 8712 bold italic Y 119962 8712 119936 119962 119936 tilde boldsymbol y in boldsymbol Y over start ARG bold italic y end ARG 8712 bold italic Y 119962 8712 119936 119962 119962 y 8712 119936 Y 119962 119936 119962 119936 119962 119962 y 119936 Y tilde boldsymbol y in boldsymbol Y tilde boldsymbol y in boldsymbol Y over start ARG bold italic y end ARG 8712 bold italic Y over start ARG bold italic y end ARG bold italic Y These predictive values consist of the predictive label set 119936 119962 119962 f 8290 119961 119961 8712 119935 119936 conditional set 119962 formulae sequence 119962 119891 119961 119961 119935 tilde boldsymbol Y tilde boldsymbol y tilde boldsymbol y f boldsymbol x boldsymbol x in boldsymbol X over start ARG bold italic Y end ARG over start ARG bold italic y end ARG over start ARG bold italic y end ARG italic f bold italic x bold italic x 8712 bold italic X 119936 119962 119962 f 8290 119961 119961 8712 119935 119936 conditional set 119962 formulae sequence 119962 119891 119961 119961 119935 tilde boldsymbol Y tilde boldsymbol y tilde boldsymbol y f boldsymbol x boldsymbol x in boldsymbol X over start ARG bold italic Y end ARG over start ARG bold italic y end ARG over start ARG bold italic y end ARG italic f bold italic x bold italic x 8712 bold italic X 119936 119962 119962 f 8290 119961 119961 8712 119935 119936 119936 Y 119962 119962 f 8290 119961 119961 8712 119935 119962 119962 y 119962 f 8290 119961 119961 8712 119935 119962 f 8290 119961 119962 119962 y f 8290 119961 f f 8290 119961 119961 x 119961 8712 119935 119961 x 8712 119935 X 119936 conditional set 119962 formulae sequence 119962 119891 119961 119961 119935 119936 conditional set 119962 formulae sequence 119962 119891 119961 119961 119935 119936 119936 Y conditional set 119962 formulae sequence 119962 119891 119961 119961 119935 conditional set conditional set 119962 119962 y formulae sequence 119962 119891 119961 119961 119935 formulae sequence formulae sequence 119962 119891 119961 119962 119962 y 119891 119961 119891 f 119961 x 119961 119935 119961 x 119935 X tilde boldsymbol Y tilde boldsymbol y tilde boldsymbol y f boldsymbol x boldsymbol x in boldsymbol X tilde boldsymbol Y tilde boldsymbol y tilde boldsymbol y f boldsymbol x boldsymbol x in boldsymbol X over start ARG bold italic Y end ARG over start ARG bold italic y end ARG over start ARG bold italic y end ARG italic f bold italic x bold italic x 8712 bold italic X over start ARG bold italic Y end ARG over start ARG bold italic y end ARG over start ARG bold italic y end ARG italic f bold italic x bold italic x bold italic X Following the ML settings we should define a measurement 119979 119936 119936 8466 119979 119936 119936 8466 mathcal P boldsymbol Y tilde boldsymbol Y mathcal L caligraphic P bold italic Y over start ARG bold italic Y end ARG caligraphic L 119979 119936 119936 8466 119979 119936 119936 8466 mathcal P boldsymbol Y tilde boldsymbol Y mathcal L caligraphic P bold italic Y over start ARG bold italic Y end ARG caligraphic L 119979 119936 119936 8466 119979 P 119936 119936 8466 119936 Y 119936 119936 Y 8466 L 119979 119936 119936 8466 119979 119936 119936 8466 119979 P 119936 119936 8466 119936 Y 119936 119936 Y 8466 L mathcal P boldsymbol Y tilde boldsymbol Y mathcal L mathcal P boldsymbol Y tilde boldsymbol Y mathcal L caligraphic P bold italic Y over start ARG bold italic Y end ARG caligraphic L caligraphic P bold italic Y over start ARG bold italic Y end ARG caligraphic L where 8466 8466 mathcal L caligraphic L 8466 8466 mathcal L caligraphic L 8466 L 8466 8466 L mathcal L mathcal L caligraphic L caligraphic L is a function to measure the distance between any pairs of 119962 119962 119962 119962 boldsymbol y tilde boldsymbol y bold italic y over start ARG bold italic y end ARG 119962 119962 119962 119962 boldsymbol y tilde boldsymbol y bold italic y over start ARG bold italic y end ARG 119962 119962 119962 y 119962 119962 y 119962 119962 119962 119962 119962 y 119962 119962 y boldsymbol y tilde boldsymbol y boldsymbol y tilde boldsymbol y bold italic y over start ARG bold italic y end ARG bold italic y over start ARG bold italic y end ARG More basic notations please refer to Table 3 Based on the definitions of four basic elements experience domain task and measurement above we first restate the general definition of machine learning by mitchell1997machine mitchell1997machine to a more exact form as follows To establish a unified formulation we start the review of traditional methods by defining a common framework The notations for subsequent discussions are summarized in Table 3 Building upon this we initiate our discussion with multiple standard regression models for each task as a paradigm The weights of these homogeneous models can be arranged into one weight matrix catalyzing a series of MTL studies through matrix regularization techniques in the traditional era We denote by 119935 t 119962 t t 1 T superscript subscript superscript 119935 119905 superscript 119962 119905 119905 1 119879 boldsymbol X t boldsymbol y t t 1 T bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 119935 t 119962 t t 1 T superscript subscript superscript 119935 119905 superscript 119962 119905 119905 1 119879 boldsymbol X t boldsymbol y t t 1 T bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 119935 t 119962 t t 1 T 119935 t 119962 t 119935 t 119962 t 119935 t 119935 X t t t 119962 t 119962 y t t t t 1 t t 1 1 T T superscript subscript superscript 119935 119905 superscript 119962 119905 119905 1 119879 superscript subscript superscript 119935 119905 superscript 119962 119905 119905 1 119879 superscript superscript subscript superscript 119935 119905 superscript 119962 119905 119905 1 subscript subscript superscript 119935 119905 superscript 119962 119905 superscript 119935 119905 superscript 119962 119905 superscript 119935 119905 superscript superscript 119935 X 119905 t superscript 119962 119905 superscript superscript 119962 y 119905 t 119905 1 119905 t 1 1 119879 T boldsymbol X t boldsymbol y t t 1 T boldsymbol X t boldsymbol y t t 1 T bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT our dataset across T 119879 T italic T T 119879 T italic T T T 119879 119879 T T T italic T italic T tasks For each task indexed by t 1 2 8943 T 119905 1 2 8943 119879 t 1 2 cdots T italic t 1 2 8943 italic T t 1 2 8943 T 119905 1 2 8943 119879 t 1 2 cdots T italic t 1 2 8943 italic T t 1 2 8943 T t t 1 2 8943 T 1 1 2 2 8943 T T 119905 1 2 8943 119879 119905 1 2 8943 119879 119905 t 1 2 8943 119879 1 1 2 2 8943 119879 T t 1 2 cdots T t 1 2 cdots T italic t 1 2 8943 italic T italic t 1 2 italic T we are given N t subscript 119873 119905 N t italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT N t subscript 119873 119905 N t italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT N t N N t t subscript 119873 119905 subscript 119873 119905 subscript subscript 119873 N 119905 t N t N t italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT samples with D 119863 D italic D D 119863 D italic D D D 119863 119863 D D D italic D italic D features i e 119935 t 8712 8477 N t 215 D superscript 119935 119905 superscript 8477 subscript 119873 119905 119863 boldsymbol X t in mathbb R N t times D bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 215 italic D end POSTSUPERSCRIPT 119935 t 8712 8477 N t 215 D superscript 119935 119905 superscript 8477 subscript 119873 119905 119863 boldsymbol X t in mathbb R N t times D bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 215 italic D end POSTSUPERSCRIPT 119935 t 8712 8477 N t 215 D 119935 t 119935 X t t t 8712 8477 N t 215 D 8477 R N t 215 D N t N N t t 215 x D D superscript 119935 119905 superscript 8477 subscript 119873 119905 119863 superscript 119935 119905 superscript 8477 subscript 119873 119905 119863 superscript 119935 119905 superscript superscript 119935 X 119905 t superscript 8477 subscript 119873 119905 119863 superscript superscript 8477 R subscript 119873 119905 119863 subscript 119873 119905 subscript subscript 119873 N 119905 t 119863 D boldsymbol X t in mathbb R N t times D boldsymbol X t in mathbb R N t times D bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 215 italic D end POSTSUPERSCRIPT bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT x italic D end POSTSUPERSCRIPT and the corresponding response values 119962 t 8712 8477 N t superscript 119962 119905 superscript 8477 subscript 119873 119905 boldsymbol y t in mathbb R N t bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end POSTSUPERSCRIPT 119962 t 8712 8477 N t superscript 119962 119905 superscript 8477 subscript 119873 119905 boldsymbol y t in mathbb R N t bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end POSTSUPERSCRIPT 119962 t 8712 8477 N t 119962 t 119962 y t t t 8712 8477 N t 8477 R N t N N t t superscript 119962 119905 superscript 8477 subscript 119873 119905 superscript 119962 119905 superscript 8477 subscript 119873 119905 superscript 119962 119905 superscript superscript 119962 y 119905 t superscript 8477 subscript 119873 119905 superscript superscript 8477 R subscript 119873 119905 subscript subscript 119873 N 119905 t boldsymbol y t in mathbb R N t boldsymbol y t in mathbb R N t bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT blackboard R start POSTSUPERSCRIPT italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end POSTSUPERSCRIPT However to minimize the rank of a matrix is NP hard vandenberghe1996semidefinite vandenberghe1996semidefinite due to the combinatorial nature of the rank function ji2009accelerated han2016multi ji2009accelerated han2016multi An alternative is to substitute the rank penalty with the trace of the rank for the symmetric positive semidefinite matrix mesbahi1999semi mesbahi1999semi but it excludes non symmetric or even non square matrices in real world applications fazel2001rank fazel2001rank generalized the trace heuristic to any matrix by introducing the trace norm a k a nuclear norm or Ky Fun k norm horn2012matrix horn2012matrix which is defined as the sum of a matrix s all singular values See Table 3 Unless explicitly stated otherwise we employ the notation provided in Tab 6 within the context of DL settings to expand upon and complement the information presented in Tab 3 
73,S2.SS1.89, Model Name Origin Year Type Matrix Regularizer Vector Formalization Regularized MTL KDD evgeniou2004regularized Group regularization Frobenius norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 t 1 T 8214 119960 t 8722 1 T 8290 8721 t 1 T 119960 t 8214 2 2 955 2 8290 8721 t 1 T 8214 119960 t 8214 2 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119905 1 119879 subscript superscript norm superscript 119960 119905 1 119879 superscript subscript 119905 1 119879 superscript 119960 119905 2 2 subscript 120582 2 superscript subscript 119905 1 119879 subscript superscript norm superscript 119960 119905 2 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum t 1 T boldsymbol w t frac 1 T sum t 1 T boldsymbol w t 2 2 lambda 2 sum t 1 T boldsymbol w t 2 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic T end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT Learning Multiple Tasks with Kernel Methods JMLR evgeniou2005learning Priori Sharing Adaptive penalty min 119933 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 t 1 T 119960 t 8868 8290 119933 8290 119960 t subscript 119933 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119905 1 119879 superscript superscript 119960 119905 top superscript 119933 superscript 119960 119905 min limits boldsymbol V boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum t 1 T boldsymbol w t top boldsymbol V boldsymbol w t roman min start POSTSUBSCRIPT bold italic V bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic V start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 160 160 s t 160 119933 8712 119930 D 119933 superscript subscript 119930 119863 boldsymbol V in boldsymbol S D bold italic V 8712 bold italic S start POSTSUBSCRIPT end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 119933 8712 119930 D 119933 superscript 119930 119863 boldsymbol V in boldsymbol S D bold italic V 8712 bold italic S start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT Alternating structure optimization JMLR ando2005framework Decomposition Frobenius norm min 119934 119933 920 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 920 8868 8290 119959 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 D 8214 119960 d 8214 2 2 subscript 119934 119933 920 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 superscript subscript norm superscript 119935 119905 superscript 119960 119905 superscript 920 top superscript 119959 119905 superscript 119962 119905 2 2 120582 superscript subscript 119889 1 119863 superscript subscript norm subscript 119960 119889 2 2 min limits boldsymbol W boldsymbol V Theta frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t Theta top boldsymbol v t boldsymbol y t 2 2 lambda sum d 1 D boldsymbol w d 2 2 roman min start POSTSUBSCRIPT bold italic W bold italic V roman 920 end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT roman 920 start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic v start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 160 160 s t 160 920 8290 920 8868 119920 h 215 h 920 superscript 920 top subscript 119920 8462 8462 Theta Theta top boldsymbol I h times h roman 920 roman 920 start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic I start POSTSUBSCRIPT italic h 215 italic h end POSTSUBSCRIPT Multi task feature selection Tech Rep 1 obozinski2006multi Group sparse learning 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 D 8214 119960 d 8214 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum d 1 D boldsymbol w d 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT Multi task Lasso Thesis 2 zhang2006a Group sparse learning 8467 8734 1 subscript 8467 1 ell infty 1 roman 8467 start POSTSUBSCRIPT 8734 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 D 8214 119960 d 8214 8734 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum d 1 D boldsymbol w d infty roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT Multi task feature learning NeurIPS argyriou2006multi Group sparse learning feature learning 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm min 119932 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119932 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 D 8214 119960 d 8214 2 2 subscript 119932 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 119932 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 2 2 min limits boldsymbol U boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol U boldsymbol w t boldsymbol y t 2 2 lambda sum d 1 D boldsymbol w d 2 2 roman min start POSTSUBSCRIPT bold italic U bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic U bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 160 160 s t 160 119932 8712 119926 D 119932 superscript 119926 119863 boldsymbol U in boldsymbol O D bold italic U 8712 bold italic O start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT Convex multi task feature learning Mach Lea argyriou2008convex Feature learning Adaptive penalty min 119933 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 t 1 T 119960 t 8868 8290 119933 8290 119960 t subscript 119933 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119905 1 119879 superscript superscript 119960 119905 top superscript 119933 superscript 119960 119905 min limits boldsymbol V boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum t 1 T boldsymbol w t top boldsymbol V boldsymbol w t roman min start POSTSUBSCRIPT bold italic V bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic V start POSTSUPERSCRIPT end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 160 160 s t 160 119933 8712 119930 D 119933 superscript subscript 119930 119863 boldsymbol V in boldsymbol S D bold italic V 8712 bold italic S start POSTSUBSCRIPT end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT tr 119933 8804 1 119933 1 boldsymbol V leq 1 bold italic V 8804 1 col 119934 8838 119934 absent boldsymbol W subseteq bold italic W 8838 col 119933 119933 boldsymbol V bold italic V Low rank MTL ICML ji2009accelerated Low rank learning Trace norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8214 119934 8214 8727 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 subscript norm 119934 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda boldsymbol W roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8741 bold italic W 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT Convex ASO ICML chen2009convex 8212 8212 min 119932 920 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119958 t 8722 119962 t 8214 2 2 955 8290 951 8290 1 8722 951 8290 tr 8290 119932 8868 8290 951 8290 119920 920 8868 8290 920 8722 1 8290 119932 s t 920 8290 920 8868 119920 h 215 h formulae sequence subscript 119932 920 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 superscript subscript norm superscript 119935 119905 superscript 119958 119905 superscript 119962 119905 2 2 120582 120578 1 120578 tr superscript 119932 top superscript 120578 119920 superscript 920 top 920 1 119932 119904 119905 920 superscript 920 top subscript 119920 8462 8462 min limits boldsymbol U Theta frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol u t boldsymbol y t 2 2 lambda eta 1 eta text tr boldsymbol U top eta boldsymbol I Theta top Theta 1 boldsymbol U s t Theta Theta top boldsymbol I h times h roman min start POSTSUBSCRIPT bold italic U roman 920 end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic u start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 italic 951 1 italic 951 tr bold italic U start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT italic 951 bold italic I roman 920 start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT roman 920 start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT bold italic U italic s italic t roman 920 roman 920 start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic I start POSTSUBSCRIPT italic h 215 italic h end POSTSUBSCRIPT Dirty block sparse model NeurIPS jalali2010dirty Group sparse learning decomposition 8467 8734 1 subscript 8467 1 ell infty 1 roman 8467 start POSTSUBSCRIPT 8734 1 end POSTSUBSCRIPT norm 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119956 t 119939 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 8214 119956 d 8214 1 955 2 8290 8721 d 1 D 8214 119939 d 8214 8734 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119956 119905 superscript 119939 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript norm subscript 119956 119889 1 subscript 120582 2 superscript subscript 119889 1 119863 subscript norm subscript 119939 119889 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol s t boldsymbol b t boldsymbol y t 2 2 lambda 1 sum d 1 D boldsymbol s d 1 lambda 2 sum d 1 D boldsymbol b d infty roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic s start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic b start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic s start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic b start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 8734 end POSTSUBSCRIPT 160 160 s t 160 119934 119930 119913 119934 119930 119913 boldsymbol W boldsymbol S boldsymbol B bold italic W bold italic S bold italic B Sparse multi task Lasso NeurIPS lee2010adaptive Group sparse learning Weighted 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm weighted 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 961 d 8290 8214 119960 d 8214 2 955 2 8290 8721 d 1 D 952 d 8290 8214 119960 d 8214 1 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript 120588 119889 subscript norm subscript 119960 119889 2 subscript 120582 2 superscript subscript 119889 1 119863 subscript 120579 119889 subscript norm subscript 119960 119889 1 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D rho d boldsymbol w d 2 lambda 2 sum d 1 D theta d boldsymbol w d 1 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT italic 961 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT italic 952 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT cdashline 1 6 Weighted 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm weighted 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 961 d 8290 8214 119960 d 8214 2 955 2 8290 8721 d 1 D 952 d 8290 8214 119960 d 8214 1 log 8289 Z 8290 120646 120637 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript 120588 119889 subscript norm subscript 119960 119889 2 subscript 120582 2 superscript subscript 119889 1 119863 subscript 120579 119889 subscript norm subscript 119960 119889 1 119885 120646 120637 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D rho d boldsymbol w d 2 lambda 2 sum d 1 D theta d boldsymbol w d 1 log Z boldsymbol rho boldsymbol theta roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT italic 961 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT italic 952 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT roman log italic Z bold italic 961 bold italic 952 Adaptive multi task Lasso NeurIPS lee2010adaptive Group sparse learning adaptive penalty P 8290 119934 120646 120637 1 Z 8290 120646 120637 8290 8719 d 1 D 8719 t 1 T exp 8289 8722 952 d 8290 w n t 215 8719 d 1 D exp 8289 8722 961 d 8290 8214 119856 d 8214 2 119875 conditional 119934 120646 120637 1 119885 120646 120637 superscript subscript product 119889 1 119863 superscript subscript product 119905 1 119879 subscript 120579 119889 subscript 119908 119899 119905 superscript subscript product 119889 1 119863 subscript 120588 119889 subscript norm subscript 119856 119889 2 P boldsymbol W boldsymbol rho boldsymbol theta frac 1 Z boldsymbol rho boldsymbol theta prod d 1 D prod t 1 T exp theta d lvert w n t rvert times prod d 1 D exp rho d mathbf w d 2 italic P bold italic W bold italic 961 bold italic 952 divide start ARG 1 end ARG start ARG italic Z bold italic 961 bold italic 952 end ARG 8719 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8719 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT roman exp italic 952 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT italic w start POSTSUBSCRIPT italic n italic t end POSTSUBSCRIPT 215 8719 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT roman exp italic 961 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 bold w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT min 119820 0 8230 119820 T 8289 947 0 8290 8214 119820 0 8722 119816 8214 F 2 8721 t 1 T 947 t 8290 8214 119820 t 8214 F 2 8721 i j 8712 J t j 8800 i d t 2 8290 119857 i 119857 j 8721 i j k 8712 S t 958 i 8290 j 8290 k subscript subscript 119820 0 8230 subscript 119820 119879 subscript 120574 0 superscript subscript norm subscript 119820 0 119816 119865 2 superscript subscript 119905 1 119879 delimited subscript 120574 119905 superscript subscript norm subscript 119820 119905 119865 2 subscript formulae sequence 119894 119895 subscript 119869 119905 119895 119894 superscript subscript 119889 119905 2 subscript 119857 119894 subscript 119857 119895 subscript 119894 119895 119896 subscript 119878 119905 subscript 120585 119894 119895 119896 min limits mathbf M 0 ldots mathbf M T gamma 0 mathbf M 0 mathbf I F 2 sum nolimits t 1 T left gamma t mathbf M t F 2 sum nolimits i j in J t j neq i d t 2 mathbf x i mathbf x j sum nolimits i j k in S t xi ijk right roman min start POSTSUBSCRIPT bold M start POSTSUBSCRIPT 0 end POSTSUBSCRIPT 8230 bold M start POSTSUBSCRIPT italic T end POSTSUBSCRIPT end POSTSUBSCRIPT italic 947 start POSTSUBSCRIPT 0 end POSTSUBSCRIPT 8741 bold M start POSTSUBSCRIPT 0 end POSTSUBSCRIPT bold I 8741 start POSTSUBSCRIPT italic F end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic 947 start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 8741 bold M start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT italic F end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 8721 start POSTSUBSCRIPT italic i italic j 8712 italic J start POSTSUBSCRIPT italic t end POSTSUBSCRIPT italic j 8800 italic i end POSTSUBSCRIPT italic d start POSTSUBSCRIPT italic t end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT bold x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic i italic j italic k 8712 italic S start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end POSTSUBSCRIPT italic 958 start POSTSUBSCRIPT italic i italic j italic k end POSTSUBSCRIPT Large margin multi task metric learning NeurIPS parameswaran2010large Priori Sharing Frobenius norm s t 8704 t 8704 i j k 8712 S t d t 2 8290 119857 i 119857 k 8722 d t 2 8290 119857 i 119857 j 8805 1 8722 958 i 8290 j 8290 k 958 i 8290 j 8290 k 8805 0 119820 0 119820 1 8230 119820 T 8805 0 for all 119905 for all 119894 119895 119896 subscript 119878 119905 formulae sequence superscript subscript 119889 119905 2 subscript 119857 119894 subscript 119857 119896 superscript subscript 119889 119905 2 subscript 119857 119894 subscript 119857 119895 1 subscript 120585 119894 119895 119896 formulae sequence subscript 120585 119894 119895 119896 0 subscript 119820 0 subscript 119820 1 8230 subscript 119820 119879 0 forall t forall i j k in S t colon quad d t 2 mathbf x i mathbf x k d t 2 mathbf x i mathbf x j geq 1 xi ijk xi ijk geq 0 mathbf M 0 mathbf M 1 ldots mathbf M T geq 0 8704 italic t 8704 italic i italic j italic k 8712 italic S start POSTSUBSCRIPT italic t end POSTSUBSCRIPT italic d start POSTSUBSCRIPT italic t end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT bold x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold x start POSTSUBSCRIPT italic k end POSTSUBSCRIPT italic d start POSTSUBSCRIPT italic t end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT bold x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 8805 1 italic 958 start POSTSUBSCRIPT italic i italic j italic k end POSTSUBSCRIPT italic 958 start POSTSUBSCRIPT italic i italic j italic k end POSTSUBSCRIPT 8805 0 bold M start POSTSUBSCRIPT 0 end POSTSUBSCRIPT bold M start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8230 bold M start POSTSUBSCRIPT italic T end POSTSUBSCRIPT 8805 0 Hierarchical multitask structured output learning NeurIPS gornitz2011hierarchical Priori Sharing Frobenius norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 1 2 8290 8721 t 1 T 8214 119960 8214 2 2 8722 955 8290 119960 T 8290 119960 p subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 1 2 superscript subscript 119905 1 119879 superscript subscript norm 119960 2 2 120582 superscript 119960 119879 subscript 119960 119901 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 frac 1 2 sum t 1 T boldsymbol w 2 2 lambda boldsymbol w T boldsymbol w p roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 bold italic w start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT italic p end POSTSUBSCRIPT where p 119901 p italic p is the parent node low rank learning Robust MTL KDD chen2011integrating Decomposition group sparse learning Trace norm 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 8214 119935 t 8290 119949 t 119956 t 8722 119962 t 8214 2 2 955 1 8290 8214 119923 8214 8727 955 2 8290 8721 t 1 T 8214 119956 t 8214 2 subscript 119934 1 2 superscript subscript 119905 1 119879 superscript subscript norm superscript 119935 119905 superscript 119949 119905 superscript 119956 119905 superscript 119962 119905 2 2 subscript 120582 1 subscript norm 119923 subscript 120582 2 superscript subscript 119905 1 119879 subscript norm subscript 119956 119905 2 min limits boldsymbol W frac 1 2 sum t 1 T boldsymbol X t boldsymbol l t boldsymbol s t boldsymbol y t 2 2 lambda 1 boldsymbol L lambda 2 sum t 1 T boldsymbol s t 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic l start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic s start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8741 bold italic L 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic s start POSTSUBSCRIPT italic t end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 160 160 s t 160 119934 119923 119930 119934 119923 119930 boldsymbol W boldsymbol L boldsymbol S bold italic W bold italic L bold italic S Temporal group Lasso KDD zhou2011multi Group sparse learning Frobenius norm 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 8214 119960 d 8214 2 2 955 2 8290 8721 t 1 T 8722 1 8214 119960 t 8722 119960 t 1 8214 2 2 955 3 8290 8721 d 1 D 8214 119960 d 8214 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 superscript subscript norm subscript 119960 119889 2 2 subscript 120582 2 superscript subscript 119905 1 119879 1 superscript subscript norm superscript 119960 119905 superscript 119960 119905 1 2 2 subscript 120582 3 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D boldsymbol w d 2 2 lambda 2 sum t 1 T 1 boldsymbol w t boldsymbol w t 1 2 2 lambda 3 sum d 1 D boldsymbol w d 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T 1 end POSTSUPERSCRIPT 8741 bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t 1 end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 start POSTSUBSCRIPT 3 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT Clustered MTL NeurIPS zhou2011clustered task clustering Clustering penalty 8467 2 2 subscript 8467 2 2 ell 2 2 roman 8467 start POSTSUBSCRIPT 2 2 end POSTSUBSCRIPT norm min 119934 119917 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 tr 8290 119934 8868 8290 119934 8722 tr 8290 119917 8868 8290 119934 8868 8290 119934 8290 119917 955 2 8290 8721 t 1 T 8214 119960 t 8214 2 2 subscript 119934 119917 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 superscript subscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 tr superscript 119934 top 119934 tr superscript 119917 top superscript 119934 top 119934 119917 subscript 120582 2 superscript subscript 119905 1 119879 subscript superscript norm superscript 119960 119905 2 2 min limits boldsymbol W boldsymbol F frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 text tr boldsymbol W top boldsymbol W text tr boldsymbol F top boldsymbol W top boldsymbol W boldsymbol F lambda 2 sum t 1 T boldsymbol w t 2 2 roman min start POSTSUBSCRIPT bold italic W bold italic F end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT tr bold italic W start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic W tr bold italic F start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic W start POSTSUPERSCRIPT 8868 end POSTSUPERSCRIPT bold italic W bold italic F italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT s t 8290 119917 t j 1 n j 8290 if 8290 t 8712 119966 j 8290 otherwise 8290 0 s t subscript 119917 119905 119895 1 subscript 119899 119895 if 119905 subscript 119966 119895 otherwise 0 text s t boldsymbol F t j 1 sqrt n j text if t in mathcal C j text otherwise 0 s t bold italic F start POSTSUBSCRIPT italic t italic j end POSTSUBSCRIPT 1 square root start ARG italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT end ARG if italic t 8712 caligraphic C start POSTSUBSCRIPT italic j end POSTSUBSCRIPT otherwise 0 t 1 8943 T 119905 1 8943 119879 t 1 cdots T italic t 1 8943 italic T where n j subscript 119899 119895 n j italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT is the task in the j 119895 j italic j th cluster 119966 j subscript 119966 119895 mathbf mathcal C j caligraphic C start POSTSUBSCRIPT italic j end POSTSUBSCRIPT Decomposition sparse learning Sparse and low rank MTL TKDD chen2012learning low rank learning 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm trace norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 D 8214 119953 d 8214 1 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119889 1 119863 subscript norm subscript 119953 119889 1 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum d 1 D boldsymbol p d 1 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic p start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 160 160 s t 160 119934 119927 119928 8214 119928 8214 8727 8804 964 formulae sequence 119934 119927 119928 subscript norm 119928 120591 boldsymbol W boldsymbol P boldsymbol Q boldsymbol Q leq tau bold italic W bold italic P bold italic Q 8741 bold italic Q 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT 8804 italic 964 Convex fused sparse group Lasso KDD zhou2012modeling Group sparse learning 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 8214 119960 d 8214 1 955 2 8290 8721 t 1 T 8722 1 8214 119960 t 8722 119960 t 1 8214 1 955 3 8290 8721 d 1 D 8214 119960 d 8214 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 1 subscript 120582 2 superscript subscript 119905 1 119879 1 subscript norm superscript 119960 119905 superscript 119960 119905 1 1 subscript 120582 3 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D boldsymbol w d 1 lambda 2 sum t 1 T 1 boldsymbol w t boldsymbol w t 1 1 lambda 3 sum d 1 D boldsymbol w d 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T 1 end POSTSUPERSCRIPT 8741 bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t 1 end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 3 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT Adaptive multi task elastic net SDM chen2012adaptive Group sparse learning 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm Frobenius norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 8214 119960 d 8214 2 955 2 8290 8721 d 1 D 8214 119960 d 8214 2 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript norm subscript 119960 119889 2 subscript 120582 2 superscript subscript 119889 1 119863 superscript subscript norm subscript 119960 119889 2 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D boldsymbol w d 2 lambda 2 sum d 1 D boldsymbol w d 2 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT Multi level Lasso ICML lozano2012multi Decomposition sparse learning 8467 1 1 subscript 8467 1 1 ell 1 1 roman 8467 start POSTSUBSCRIPT 1 1 end POSTSUBSCRIPT norm adaptive penalty min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 952 d 955 2 8290 8721 d 1 D 8214 120632 d 8214 1 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript 120579 119889 subscript 120582 2 superscript subscript 119889 1 119863 subscript norm subscript 120632 119889 1 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D theta d lambda 2 sum d 1 D boldsymbol boldsymbol gamma d 1 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT italic 952 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic 947 start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 160 160 s t 160 119934 120637 8594 8290 120498 8290 120490 120637 8594 8805 120782 formulae sequence 119934 8594 120637 120498 120490 8594 120637 0 boldsymbol W vec boldsymbol theta boldsymbol Lambda boldsymbol Gamma vec boldsymbol theta geq boldsymbol 0 bold italic W over 8594 start ARG bold italic 952 end ARG bold 923 bold 915 over 8594 start ARG bold italic 952 end ARG 8805 bold 0 Robust multi task feature learning KDD gong2012robust Decomposition group sparse learning 8467 2 1 subscript 8467 2 1 ell 2 1 roman 8467 start POSTSUBSCRIPT 2 1 end POSTSUBSCRIPT norm 8467 1 2 subscript 8467 1 2 ell 1 2 roman 8467 start POSTSUBSCRIPT 1 2 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 8290 8721 d 1 D 8214 119953 d 8214 2 955 2 8290 8721 d 1 D 8214 119954 d 8214 1 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 superscript subscript 119889 1 119863 subscript norm subscript 119953 119889 2 subscript 120582 2 superscript subscript 119889 1 119863 superscript subscript norm subscript 119954 119889 1 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda 1 sum d 1 D boldsymbol p d 2 lambda 2 sqrt sum d 1 D boldsymbol q d 1 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic p start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT square root start ARG 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT 8741 bold italic q start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT end ARG 160 160 s t 160 119934 119927 119928 119934 119927 119928 boldsymbol W boldsymbol P boldsymbol Q bold italic W bold italic P bold italic Q Multi stage multi task feature learning NeurIPS gong2012multi Sparse learning Capped 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT norm 160 zhang2010analysis min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 d 1 R min 8289 8214 119960 d 8214 1 964 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119889 1 119877 subscript norm subscript 119960 119889 1 120591 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum d 1 R min boldsymbol w d 1 tau roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic d 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic R end POSTSUPERSCRIPT roman min 8741 bold italic w start POSTSUBSCRIPT italic d end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic 964 Convex formulation for MTL IJCAI zhang2012convex Priori sharing Clustering penalty min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 1 2 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript 120582 1 2 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 frac lambda 1 2 roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT divide start ARG italic 955 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT end ARG start ARG 2 end ARG tr 119934 8290 119934 T 955 2 2 119934 superscript 119934 119879 subscript 120582 2 2 boldsymbol W boldsymbol W T frac lambda 2 2 bold italic W bold italic W start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG italic 955 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT end ARG start ARG 2 end ARG tr 119934 8290 120512 8722 1 8290 119934 T 119934 superscript 120512 1 superscript 119934 119879 boldsymbol W boldsymbol Omega 1 boldsymbol W T bold italic W bold 937 start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT bold italic W start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 160 160 s t 160 120512 8712 119930 D 120512 superscript subscript 119930 119863 boldsymbol Omega in boldsymbol S D bold 937 8712 bold italic S start POSTSUBSCRIPT end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT tr 120512 1 120512 1 boldsymbol Omega 1 bold 937 1 Multi linear multi task learning ICML romera2013multilinear Low rank learning Overlapped tensor trace norm min 120038 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 k 1 N 8214 119934 k 8214 8727 subscript 120038 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119896 1 119873 subscript norm subscript 119934 119896 min limits boldsymbol mathcal W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum k 1 N boldsymbol W k roman min start POSTSUBSCRIPT bold caligraphic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic k 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT 8741 bold italic W start POSTSUBSCRIPT italic k end POSTSUBSCRIPT 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT where 119934 k subscript 119934 119896 boldsymbol W k bold italic W start POSTSUBSCRIPT italic k end POSTSUBSCRIPT is the mode k 119896 k italic k unfolding of tensor 120038 8712 8477 D 215 I 2 215 8943 215 I N 120038 superscript 8477 119863 subscript 119868 2 8943 subscript 119868 119873 boldsymbol mathcal W in mathbb R D times I 2 times cdots times I N bold caligraphic W 8712 blackboard R start POSTSUPERSCRIPT italic D 215 italic I start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 215 8943 215 italic I start POSTSUBSCRIPT italic N end POSTSUBSCRIPT end POSTSUPERSCRIPT Regularization approach to learn MTL TKDD zhang2014regularization Priori sharing Clustering penalty 8467 2 2 subscript 8467 2 2 ell 2 2 roman 8467 start POSTSUBSCRIPT 2 2 end POSTSUBSCRIPT norm min 119933 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 2 8290 8721 t 1 T 8214 119960 t 8214 2 2 subscript 119933 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 limit from 120582 2 superscript subscript 119905 1 119879 superscript subscript norm superscript 119960 119905 2 2 min limits boldsymbol V boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 frac lambda 2 sum t 1 T boldsymbol w t 2 2 roman min start POSTSUBSCRIPT bold italic V bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT divide start ARG italic 955 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT tr 119934 8290 120512 8722 1 8290 119934 T d 119934 superscript 120512 1 superscript 119934 119879 119889 boldsymbol W boldsymbol Omega 1 boldsymbol W T d bold italic W bold 937 start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT bold italic W start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic d ln 120512 120512 boldsymbol Omega bold 937 160 160 s t 160 120512 8712 119930 D 120512 superscript subscript 119930 119863 boldsymbol Omega in boldsymbol S D bold 937 8712 bold italic S start POSTSUBSCRIPT end POSTSUBSCRIPT start POSTSUPERSCRIPT italic D end POSTSUPERSCRIPT Multi linear multi task learning NeurIPS wimalawarne2014multitask Low rank learning Scaled latent tensor trace norm min 120038 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 inf 120038 1 8943 120038 N 120038 955 8290 8721 k 1 N I k 8722 1 2 8290 8214 119934 k k 8214 8727 subscript 120038 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 subscript infimum superscript 120038 1 8943 superscript 120038 119873 120038 120582 superscript subscript 119896 1 119873 superscript subscript 119868 119896 1 2 subscript norm superscript subscript 119934 119896 119896 min limits boldsymbol mathcal W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 inf boldsymbol mathcal W 1 cdots boldsymbol mathcal W N boldsymbol mathcal W lambda sum k 1 N I k 1 2 boldsymbol W k k roman min start POSTSUBSCRIPT bold caligraphic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT roman inf start POSTSUBSCRIPT bold caligraphic W start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 8943 bold caligraphic W start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT bold caligraphic W end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic k 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic N end POSTSUPERSCRIPT italic I start POSTSUBSCRIPT italic k end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 2 end POSTSUPERSCRIPT 8741 bold italic W start POSTSUBSCRIPT italic k end POSTSUBSCRIPT start POSTSUPERSCRIPT italic k end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 8727 end POSTSUBSCRIPT where 120038 8712 8477 D 215 I 2 215 8943 215 I N 120038 superscript 8477 119863 subscript 119868 2 8943 subscript 119868 119873 boldsymbol mathcal W in mathbb R D times I 2 times cdots times I N bold caligraphic W 8712 blackboard R start POSTSUPERSCRIPT italic D 215 italic I start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 215 8943 215 italic I start POSTSUBSCRIPT italic N end POSTSUBSCRIPT end POSTSUPERSCRIPT is a tensor Task Tree model KDD han2015learning task clustering 8467 2 2 subscript 8467 2 2 ell 2 2 roman 8467 start POSTSUBSCRIPT 2 2 end POSTSUBSCRIPT norm min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 8721 h 1 H 119960 h t 8722 119962 t 8214 2 2 8721 h 1 H 955 h 8290 8721 i lt j T 8214 119960 h i 8722 119960 h j 8214 2 2 s t 8290 119960 h 8722 1 i 8722 119960 h 8722 1 j 10928 119960 h i 8722 119960 h j 8704 h 8805 2 8704 i lt j formulae sequence succeeds or equals subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 superscript subscript norm superscript 119935 119905 superscript subscript 8462 1 119867 superscript subscript 119960 8462 119905 superscript 119962 119905 2 2 superscript subscript 8462 1 119867 subscript 120582 8462 superscript subscript 119894 119895 119879 subscript superscript norm superscript subscript 119960 8462 119894 superscript subscript 119960 8462 119895 2 2 s t superscript subscript 119960 8462 1 119894 superscript subscript 119960 8462 1 119895 superscript subscript 119960 8462 119894 superscript subscript 119960 8462 119895 formulae sequence for all 8462 2 for all 119894 119895 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t sum h 1 H boldsymbol w h t boldsymbol y t 2 2 sum h 1 H lambda h sum i lt j T boldsymbol w h i boldsymbol w h j 2 2 text s t boldsymbol w h 1 i boldsymbol w h 1 j succeq boldsymbol w h i boldsymbol w h j forall h geq 2 forall i lt j roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8721 start POSTSUBSCRIPT italic h 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic H end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 8721 start POSTSUBSCRIPT italic h 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic H end POSTSUPERSCRIPT italic 955 start POSTSUBSCRIPT italic h end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic i lt italic j end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT 8741 bold italic w start POSTSUBSCRIPT italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT italic i end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT italic j end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT s t bold italic w start POSTSUBSCRIPT italic h 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic i end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT italic h 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic j end POSTSUPERSCRIPT 10928 bold italic w start POSTSUBSCRIPT italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT italic i end POSTSUPERSCRIPT bold italic w start POSTSUBSCRIPT italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT italic j end POSTSUPERSCRIPT 8704 italic h 8805 2 8704 italic i lt italic j Reduced rank multi stage MTL AAAI han2016multi Low rank learning Capped trace norm 160 sun2013robust min 119934 8289 1 2 8290 8721 t 1 T 1 N t 8290 8214 119935 t 8290 119960 t 8722 119962 t 8214 2 2 955 8290 8721 r 1 R min 8289 963 r 8290 119934 964 subscript 119934 1 2 superscript subscript 119905 1 119879 1 subscript 119873 119905 subscript superscript norm superscript 119935 119905 superscript 119960 119905 superscript 119962 119905 2 2 120582 superscript subscript 119903 1 119877 subscript 120590 119903 119934 120591 min limits boldsymbol W frac 1 2 sum t 1 T frac 1 N t boldsymbol X t boldsymbol w t boldsymbol y t 2 2 lambda sum r 1 R min sigma r boldsymbol W tau roman min start POSTSUBSCRIPT bold italic W end POSTSUBSCRIPT divide start ARG 1 end ARG start ARG 2 end ARG 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT divide start ARG 1 end ARG start ARG italic N start POSTSUBSCRIPT italic t end POSTSUBSCRIPT end ARG 8741 bold italic X start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic w start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic y start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8741 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic 955 8721 start POSTSUBSCRIPT italic r 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic R end POSTSUPERSCRIPT roman min italic 963 start POSTSUBSCRIPT italic r end POSTSUBSCRIPT bold italic W italic 964 Model Name Origin Year MTL Strategy Backbone Sharing Modality Task Measurement Loss Function Availability 1 TCDCN ECCV zhang2014facial Early stopping CNN Hard Image Facial landmark detection head pose estimation Mean error mErr 160 burgos2013robust Mean squared error MSE gender classification age estimation expression failure rate 160 dantone2012real cross entropy CE loss Official recognition facial attribute inference ACL MTL ML IJCNLP dong2015multi 8212 RNN Hard Text Multiple target language translation BLEU 4 160 papineni2002bleu Delta CE loss 8212 Vanilla Part Of Speech POS Chunking Combinatory Cascading ACL sogaard goldberg 2016 deep Cascading LSTM Hard Text Categorical Grammar CCG Supertagging F1 score Micro F1 score CE loss 8212 Surface normals estimation normals semantic mErr median error medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT in angular Cross stitch segmentation semseg object detection attribute distance within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT pixel accuracy pixacc networks CVPR misra2016cross 8212 CNN Soft Image prediction mIoU fwIU mAP CE loss Unofficial ASP MTL aka Hard amp CE loss adversarial loss AdvMTL arXiv liu2017adversarial Adversarial training LSTM Soft Text Text classifications Error rate orthogonality constraint Official Cascading adding Part Of Speech POS tagging chunking parsing Accuracy acc F1 MSE unlabeled attachment CE loss softmax loss JMT EMNLP hashimoto jmt 2017 EMNLP2017 constraints LSTM Soft Text semantic relatedness textual entailment score UAS labeled attachment score LAS KL divergence Unofficial Object detection mask estimation object Mask regression loss MNCs CVPR dai2016instance Cascading CNN Hard Image categorization mAP IoU softmax loss Official FAFS CVPR lu2017fully NAS CNN Hard Image person attribute classification Acc recall CE loss Official Hard amp MRN NeurIPS long2017learning Task conditioning CNN Soft Image classifications on different domains Acc CE loss Official Depth scene parsing contour rel 160 eigen2014depth RMSE log10 mErr CE loss softmax loss PAD Net CVPR xu2018pad Mutual distillation CNN Hard Image prediction normals acc with threshold 948 120575 delta italic 948 acc 948 120575 delta italic 948 IoU acc Euclidean loss 8212 MT A adv subscript A adv text A text adv A start POSTSUBSCRIPT adv end POSTSUBSCRIPT N CVPR liu2018multi Adversarial training CNN Hard Image font glyph identity pose illumination Recognition rate CE loss adversarial loss 8212 cross task rel berHu loss 160 laina2016deeper TRL ECCV zhang2018joint attention CNN Hard Image Depth estimation depth semseg RMSE acc 948 120575 delta italic 948 pixacc mean acc mIoU CE loss uncertainty loss 8212 MMoE KDD ma2018modeling MoE MLP Hard amp Tabular Income education marriage prediction Area Under the Curve AUC CE loss Unofficial soft data engagement satisfaction in recommendation Tabular Soft Order ICLR meyerson2018beyond feature fusion CNN MLP Soft data image Classification attribute recognition mErr CE loss 8212 classification colorization edge denoised GREAT4MTL arXiv sinha2018gradient adversarial training CNN Hard Image reconstruction depth normal keypoint Err RMSE 1 8722 cos 8290 8901 8901 1 cos 8901 8901 1 text cos cdot cdot 1 cos 8901 8901 CE loss 8212 Sluice Adding constraints Hard amp Chunking entity recognition NER semantic networks AAAI ruder2019latent early stopping LSTM Soft Text role labeling SRL POS tagging Acc CE loss Official CNN NER Entity Mention Detection EMD Relation F1 score precision recall MUC B3 CEAFe HMTL AAAI sanh2019hierarchical cascading LSTM Hard Text Extraction RE Coreference Resolution CR moosavi2016coreference CE loss Unofficial CNN Segment labeling Named Entity Labeling CRF loss CE loss ranking DCMTL AAAI gong2019deep cascading LSTM Hard Text NEL slot filling F1 score precision recall loss 160 vu2016bi Official Normals semseg age estimation gender mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU pixacc mean NDDR CNN CVPR gao2019nddr feature fusion CNN Soft Image classification median absolute error absErr acc CE loss Official cross task RMSE rel acc with t 119905 t italic t mErr medError within CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss berHu loss PAP CVPR zhang2019pattern attention CNN Hard Image Semseg depth normals t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU mean accuracy mAcc pixacc affinity loss 160 zhang2019pattern 8212 MT A atten subscript A atten text A text atten A start POSTSUBSCRIPT atten end POSTSUBSCRIPT N Hard amp Semseg depth normals 10 classifications mIoU pixacc mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT amp DWA CVPR liu2019end Adaptive weighting CNN Soft Image visual domain decathlon 2 8201 8201 8201 absErr real error accuracy CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss dot product Official Semseg depth edge normals human parts mIoU osdF mErr maximum F measure maxF ASTMT CVPR maninis2019attentive attention single tasking CNN Hard Image saliency estimation albedo RMSE 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official ML GCN CVPR chen2019multi Graph based CNN GCN Hard Image Multi label recognition precision recall F1 CE loss Official RD4MTL arXiv meng2019representation Adversarial training CNN Hard Image Classifications Acc CE loss adversarial loss Official MTL NAS CVPR gao2020mtl NAS CNN Adaptive Image Semseg normals object classification scene mErr medErr Within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU pixacc CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Official classification Acc Semseg edge depth keypoint detection point BMTN BMVC vandenhende2019branched NAS CNN Adaptive Image attribute classification mIoU pixacc 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT Acc CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT Official PSD CVPR zhou2020pattern Distillation CNN Hard amp soft Image Semseg depth normals RMSE rel acc with t 119905 t italic t mIoU mean accuracy CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss berHu loss 8212 pixacc mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT ECCV distillation Hard amp mIoU pixacc absErr rel mErr medErr within KD4MTL Workshop li2020knowledge knowledge CNN soft Image Semseg depth normals classification t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT Acc CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss dot product Official MTI Net ECCV vandenhende2020mti multi task CNN Hard amp Image Semseg depth edges detection edges normals mIoU RMSE mErr optimal dataset scale F CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official distillation Soft saliency estimation human parts measure odsF 160 martin2004learning 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT NAS Regression face attribute prediction semseg LTB ICML guo2020learning task grouping CNN Soft Image normals depth keypoints edges Acc CE cos mean absErr CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cosine loss 8212 CNN amp AAMTRL ICML mao2020adaptive adversarial training LSTM Hard Text Classifications Relatedness evolution acc influence of task Any 1 Lipschitz loss 8212 Hard amp Tabular Sub tasks in the recommendation systems CGC amp PLE RecSys tang2020progressive MoE MLP soft data income education marriage prediction AUC MSE MTL gain CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Unofficial TSNs ICCV sun2021task task relationship learning CNN Hard Image Semseg depth edges normals mIoU RMSE mErr odsF 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official task conditioning saliency estimation human parts knowledge distillation Classification detection semseg depth MuST ICCV ghiasi2021multi task conditioning CNN Hard Image normals Acc mIoU RMSE odsF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 8212 AuxSegNet ICCV xu2021leveraging cross task CNN Hard amp Image Semseg classification saliency detection mIoU precision recall Multi label softmax Official attention Soft loss CE loss cross task Hard amp Semseg depth estimation edges normals ATRC ICCV bruggemann2021exploring attention CNN soft Image saliency estimation human parts mIoU RMSE mErr odsF maxF 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official DSelect k NeurIPS hazimeh2021dselect MoE MLP CNN Hard amp Tabular engagement satisfaction task classification Total loss Acc AUC RMSE expert CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Official soft data Image Hard amp 16 Language understanding tasks e g textual Acc Spearman correlation 160 spearman1961proof Matthews MT TaG ArXiv gupta2022sparsely MoE Transformer soft Text entailment sentiment classification etc correlation coefficient 160 matthews1975comparison CE loss MSE 8212 Hard amp Tabular CrossDistil AAAI yang2022cross distillation MLP soft data Finish watching like AUC multi AUC 160 hand2001simple CE loss 8212 MulT CVPR bhattacharjee2022mult cross task CNN amp Hard Image Semseg depth reshading normals MTL gain mErr of CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official attention Transformer keypoints edges domain generalization rotate loss 160 zamir2018taskonomy cross task attention CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cross task balancing spec Hard amp Semseg depth saliency detection task contrastive loss MTFormer ECCV xu2022mtformer kendall2018multi Transformer soft Image human parts mIoU RMSE 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT uncertainty loss 8212 MQTransformer arXiv xu2022multi cross task Transformer Hard amp Image Semseg depth edges normals saliency mIoU RMSE mErr odsF maxF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 8212 attention Soft estimation human parts Image MetaLink ICLR cao2022relational Graph based MLP GNN Hard Graph Classification mAP ROC AUC CE loss Official DeMT AAAI zhang2023demt cross task CNN amp Hard amp Image Semseg depth edges normals saliency mIoU RMSE mErr odsF maxF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official attention Transformer Soft estimation human parts 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT cross task Hard amp CE loss berHu loss cosine mTEB WACV lopes2023cross attention CNN soft Image Semseg depth normals edges 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT mIoU RMSE mErr F1 loss 160 guizilini2021geometric Official OKD MTL WACV jacob2023online distillation task Transformer Hard amp Image Semseg depth normals 916 p subscript 916 119901 Delta p roman 916 start POSTSUBSCRIPT italic p end POSTSUBSCRIPT mIoU pixacc absErr rel mErr medErr Adaptive feature distillation loss 8212 weighting Soft within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cosine loss Hard amp AdaMV MoE ICCV chen2023adamv MoE Transformer soft Image classification detection Seg Acc Average Precision AP CE loss Official Notation Description b B 119887 119861 b B italic b italic B Batch size l 8290 r 119897 119903 lr italic l italic r Learning rate 119987 l t 8712 8477 B 215 H 215 W 215 C mathcal X l t in mathbb R B times H times W times C caligraphic X start POSTSUBSCRIPT italic l end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic B 215 italic H 215 italic W 215 italic C end POSTSUPERSCRIPT Feature maps output from l 119897 l italic l th layer of t 119905 t italic t th task where B H W C B H W C italic B italic H italic W italic C are batch size height width and channel 119986 8712 8477 S 215 S 215 C in 215 C out 119986 superscript 8477 119878 119878 subscript 119862 in subscript 119862 out mathcal W in mathbb R S times S times C text in times C text out caligraphic W 8712 blackboard R start POSTSUPERSCRIPT italic S 215 italic S 215 italic C start POSTSUBSCRIPT in end POSTSUBSCRIPT 215 italic C start POSTSUBSCRIPT out end POSTSUBSCRIPT end POSTSUPERSCRIPT Convolution filter where S 119878 S italic S denotes the size of filter and C in C out subscript 119862 in subscript 119862 out C text in C text out italic C start POSTSUBSCRIPT in end POSTSUBSCRIPT italic C start POSTSUBSCRIPT out end POSTSUBSCRIPT denote the number of input and output channels respectively exp 8290 8901 exp 8901 text exp cdot exp 8901 Exponential function 963 8290 8901 120590 8901 sigma cdot italic 963 8901 Sigmoid function where 963 8290 x 1 1 exp 8290 8722 x 120590 119909 1 1 exp 119909 sigma x 1 1 text exp x italic 963 italic x 1 1 exp italic x softmax 8290 8901 softmax 8901 text softmax cdot softmax 8901 Softmax function where softmax 8290 119961 j exp 8290 x j 8721 i exp 8290 x i subscript delimited softmax 119961 119895 exp subscript 119909 119895 subscript 119894 exp subscript 119909 119894 text softmax boldsymbol x j text exp x j sum i text exp x i softmax bold italic x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT exp italic x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic i end POSTSUBSCRIPT exp italic x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT for any entry index j 119895 j italic j sim 8290 8901 8901 sim 8901 8901 text sim cdot cdot sim 8901 8901 An arbitrary similarity function e g cosine similarity cos 8901 8901 8901 8901 cdot cdot 8901 8901 8857 direct product odot 8857 The element wise dot product L 8290 N 8290 8901 119871 119873 8901 LN cdot italic L italic N 8901 Layer norm M 8290 H 8290 S 8290 A 8290 q k v 119872 119867 119878 119860 119902 119896 119907 MHSA q k v italic M italic H italic S italic A italic q italic k italic v Multi head self attention operator C 8290 O 8290 N 8290 V 119986 8290 8901 119862 119874 119873 subscript 119881 119986 8901 CONV mathcal W cdot italic C italic O italic N italic V start POSTSUBSCRIPT caligraphic W end POSTSUBSCRIPT 8901 Convolution operation parametrized by 119986 119986 mathcal W caligraphic W R 8290 E 8290 S 8290 H 8290 A 8290 P 8290 E 8290 8901 119877 119864 119878 119867 119860 119875 119864 8901 RESHAPE cdot italic R italic E italic S italic H italic A italic P italic E 8901 Reshape operation to rearrange the original feature maps in 8477 H 215 W 215 C superscript 8477 119867 119882 119862 mathbb R H times W times C blackboard R start POSTSUPERSCRIPT italic H 215 italic W 215 italic C end POSTSUPERSCRIPT space into a new 8477 H 8290 W 215 C superscript 8477 119867 119882 119862 mathbb R HW times C blackboard R start POSTSUPERSCRIPT italic H italic W 215 italic C end POSTSUPERSCRIPT space Algorithm Venue Year Method Convergence Highlight Availability 1 Uncertainty Weighting CVPR kendall2018multi Dynamic Weighting 8212 Optimize 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT and 119934 119934 boldsymbol W bold italic W simultaneously Official GradNorm ICML chen2018gradnorm Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the average gradient norm of each tasks and the relative progress achieved for each tasks Unofficial MGDA MTL NeurIPS sener2018multi Multi Objective Opt Asymptotic Convergence Seminal work which proposes to use MOO to solve deep MTL problems based on multi gradient descent algorithm Official RMTL Thesis liu2018exploration Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the relative progress achieved for each tasks Official LBTW AAAI liu2019loss Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT using the reinforcement learning strategy Official DWA CVPR liu2019end Dynamic Weighting 8212 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is adapted to both samples and tasks Official MLDT CVPR zheng2019pyramidal Dynamic Weighting 8212 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is adapted to the likelihood of a loss reduction Official Pareto MTL NeurIPS lin2019pareto Multi Objective Opt Asymptotic Convergence Attemp to incorporate user 8217 s preference into the solution Official Controllable Pareto MTL arXiv lin2020controllable Multi Objective Opt 8212 Use a hypernetwork to learn the entire Pareto front Official PCGrad NeurIPS yu2020gradient Gradient Correction 8212 Projecting onto orthogonal subspace to mitigate the gradient conflicts Official GradDrop NeurIPS chen2020just Gradient Correction 8212 Only keep gradients are consistent in signs in each update Official Continuous Pareto MTL ICML ma2020efficient Multi Objective Opt 8212 Construct a continuous frst order approximation of the local Pareto set Official EPO Search ICML mahapatra2020multi Multi Objective Opt 8212 Find a Pareto stationary solution to exactly match a user 8217 s preference Require losses to be non negative Official AuxiLearn ICLR navon2021auxiliary Bi level Opt 8212 Learn to combine losses in a nonlinear fashion Official IMTL ICLR liu2021towards Gradient Correction Find 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT such that the aggregated gradient 8721 t 1 T 945 t 8290 9661 8290 8466 t 8290 119934 superscript subscript 119905 1 119879 superscript 120572 119905 9661 superscript 8466 119905 119934 sum t 1 T alpha t triangledown mathcal L t boldsymbol W 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 9661 caligraphic L start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic W has equal projection onto the raw gradients of individual tasks Unofficial GradVac ICLR wang2021gradient Dynamic Weighting 8212 Encourage more geometrically aligned parameter updates for close tasks Unofficial PHN ICLR navon2021learning Multi Objective Opt 8212 Use a hypernetwork to learn the entire Pareto front Official CAGrad NeurIPS liu2021conflictaverse Gradient Correction Asymptotic Convergence The search direction is find by solving a subproblem that is similar to MGDA Official SVGD NeurIPS liu2021profiling Multi Objective Opt Convergence rate for strongly convex and third order continuously differentiable functions Integrate MGDA with Stein variational gradient descent and Langevin dynamics to obtain diverse solutions Official COSMOS ICDM ruchte2021scalable 8212 8212 A single optimization run to approximate the full set of the Pareto front by combining preferences vectors sampled from Dirichlet distribution and training data Official HV Maximization arXiv deist2021multi 8212 8212 Utilize hyper volume to approximate sample level Pareto front Official PNG UAI ye2022optimization Multi Objective Opt Convergence rate for convex losses Minimize preference loss over the Pareto front manifold optimization while only using the first order information 8212 RLW amp RGW TMLR lin2022reasonable Dynamic Weighting Converge to a neighborhood of the optimal solution under strongly convex assumption Sample the weights 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT from a given distribution at each step Unofficial Nash MTL ICML navon2022multi Multi Objective Opt Asymptotic Convergence Formulate the problem of finding a common descent direction as a bargaining game Official X WC MGDA ICML momma2022multi Dynamic Weighting 8212 Lift the restriction of non negativity requirement on losses in EPO search 8212 Rotograd ICLR javaloy2022rotograd Dynamic Weighting Gradient Correction 8212 Dynamic Weighting via gradient norm Gradient Correction via rotating the feature space Official MoCo 160 fernando2023mitigating ICLR 2023 Multi Objective Opt Convergence rates for convex amp nonconvex losses Stochastic Gradient amp Variance Reduction Official Recon ICLR shi2023recon Gradient Correction 8212 Turn shared parameters that most likely to cause gradient conflicts into task specific parameters Official Aligned MTL CVPR Senushkin 2023 CVPR Gradient Correction 8212 Use the condition number of the linear system to measure the severity of gradient dominance and conflicting issues Official Achievement based MTL ICCV yun2023achievement Dynamic Weighting 8212 Use training progress to dynamically weight tasks and use geometric mean to average loss from tasks Official FULLER ICCV huang2023fuller Dynamic Weighting 8212 Use gradient norm of different tasks to adjust the weights for tasks 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the average gradient norm of each tasks and the relative progress achieved for each tasks Seminal work which proposes to use MOO to solve deep MTL problems based on multi gradient descent algorithm Find a Pareto stationary solution to exactly match a user 8217 s preference Require losses to be non negative Find 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT such that the aggregated gradient 8721 t 1 T 945 t 8290 9661 8290 8466 t 8290 119934 superscript subscript 119905 1 119879 superscript 120572 119905 9661 superscript 8466 119905 119934 sum t 1 T alpha t triangledown mathcal L t boldsymbol W 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 9661 caligraphic L start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic W has equal projection onto the raw gradients of individual tasks Convergence rate for strongly convex and third order continuously differentiable functions Integrate MGDA with Stein variational gradient descent and Langevin dynamics to obtain diverse solutions A single optimization run to approximate the full set of the Pareto front by combining preferences vectors sampled from Dirichlet distribution and training data Minimize preference loss over the Pareto front manifold optimization while only using the first order information Converge to a neighborhood of the optimal solution under strongly convex assumption Sample the weights 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT from a given distribution at each step Formulate the problem of finding a common descent direction as a bargaining game Dynamic Weighting Gradient Correction Dynamic Weighting via gradient norm Gradient Correction via rotating the feature space Convergence rates for convex amp nonconvex losses Turn shared parameters that most likely to cause gradient conflicts into task specific parameters Use the condition number of the linear system to measure the severity of gradient dominance and conflicting issues Use training progress to dynamically weight tasks and use geometric mean to average loss from tasks Use gradient norm of different tasks to adjust the weights for tasks Dataset Source Year Modality Task Synopsis Task Sample Availability School Data ILEA mortimore1988school Table Regression Predicting student exam scores based on 27 school features 139 15 362 Official SARCOS Data Humanoid Robotics 2000 Table Regression Estimate inverse dynamics model 7 44 484 4449 Official Computer Survey Data Survey lenk1996hierarchical Table Regression Likelihood of purchasing personal computers 179 Climate Dataset Sensor network 2017 now Table Regression Real time climate data collected from four climate stations 7 Official 20 Newsgroups Netnews articles Lang95 Text Classification Hierarchical text classification 20 19 000 Official Reuters 21578 Collection Reuters 1996 Text Classification Reuters news documents with hierarchical categories 90 21 578 Official MultiMNIST Dataset MNIST sabour2017dynamic Image Classification Classify the digits on the different positions 2 Official ImageCLEF 2014 Caltech ImageNet Pascal Bing 2014 Image Classification Benchmark dataset for domain adaptation 4 2 400 Official Office Caltech Dataset Office Caltech gong2012geodesic Image Classification Benchmark dataset for the annotation and retrieval of images 4 2 533 Official Office 31 Dataset Amazon DSLR Webcam saenko2010adapting Image Classification Objects commonly encountered in office settings 3 4 110 Official Office Home Dataset Office venkateswara2017deep Image Classification Object recognition and domain adaptation in the era of deep learning 4 15 588 Official DomainNet Dataset UDA peng2019moment Image Classification Multi source unsupervised domain adaptation research 6 600 000 Official EMMa Dataset Amazon standley2023extensible Image Text Classification Amazon product listings for category prediction 2 800 000 Official SYNTHIA Dataset European Union ros2016synthia Image Classification A synthetic dataset for semantic segmentation 13 400 Official SVHN Dataset Stanford yang2021few Image Classification A digit classification benchmark dataset 600 000 Official CelebA Dataset MMLAB liu2018large Image Classification A large scale face attributes dataset 40 200 000 Official CityScapes Dataset Daimler AG cordts2016cityscapes Image Dense prediction Semantic urban scene understanding 5 000 Official NYU Depth Dataset V2 New York University silberman2012indoor Image Dense prediction Indoor scene understanding with per pixel labels 3 35 064 Official PASCAL VOC Project University of Oxford everingham2010pascal Image Dense prediction Object recognition with multiple tasks Official Taskonomy Dataset Standard zamir2018taskonomy Image Dense prediction Diverse dataset with 26 tasks for task transfer learning 26 4 000 000 Official STREET Amazon ribeiro2023street Text Reasoning The multi task structured reasoning and explanation benchmark VKITTI2 Dataset Naver cabon2020virtual Video Segmentation A video dataset which is automatically labeled with ground truth 5 Official XTREME Carnegie Mellon hu2020xtreme Text Translation QA A multilingual benchmark for evaluating cross lingual generalisation 9 400 000 Deepfashion Dataset Shopping Websites liu2016deepfashion Image Classification A large scale clothes dataset with comprehensive annotations 2 800 000 Official ACE05 Dataset News 2005 Text Classification A large corpus with annotated entities relations and events 3 52 615 Official ATIS Dataset Airline hemphill etal 1990 atis Text Classification A dataset with 17 unique intent categories 3 5 871 Official Library Language Supported Methods RMTL R Sparse structure learning 160 tibshirani1996regression multi task feature selection 160 obozinski2006multi low rank MTL 160 ji2009accelerated pong2010trace graph based regularised MTL 160 widmer2010leveraging multi task clustering 160 gu2009learning MALSAR Matlab Sparse structure learning 160 tibshirani1996regression regularized MTL 160 evgeniou2004regularized multi task feature selection 160 obozinski2006multi dirty block sparse model 160 jalali2010dirty low rank MTL 160 ji2009accelerated pong2010trace convex ASO 160 chen2009convex sparse amp low rank MTL 160 chen2012learning clustered MTL 160 zhou2011clustered robust MTL 160 chen2011integrating robust multi task feature learning 160 gong2012robust Temporal group Lasso 160 zhou2011multi convex fused sparse group Lasso 160 zhou2012modeling incomplete multi source feature learning 160 yuan2012multi multi stage multi task feature learning 160 gong2012multi multi task clustering 160 gu2009learning LibMTL Python Cross stitch 160 misra2016cross GradNorm 160 chen2018gradnorm Uncertainty Weighting 160 kendall2018multi MGDA MTL 160 sener2018multi MMoE 160 ma2018modeling MultiNet 160 chennupati2019multinet LTB 160 guo2020learning MTAN amp DWA 160 liu2019end PCGrad 160 yu2020gradient GradDrop 160 chen2020just CGC amp PLE 160 tang2020progressive IMTL 160 liu2021towards GradVac 160 wang2021gradient CAGrad 160 liu2021conflictaverse DSelect k 160 hazimeh2021dselect RLW amp RGW 160 lin2022reasonable Nash MTL 160 navon2022multi ,Table 4 Summary of regularization technique used in MTL Figure 6 The Bayesian graph for adaptive sparse multi task Lasso model a Feedforward Neural Networks b Recurrent Neural Networks Figure 7 Hard parameter sharing in FNNs and RNNs a The most early version of hard parameters sharing The connections between inputs and hidden neurons jointly transform features which are then utilized for Task 1 to Task T 119879 T italic T b A modern day RNN used for multiple target language translation which jointly transforms features from shared sequence based representations T 119879 T italic T T T 119879 119879 T T T italic T italic T b A modern day RNN used for multiple target language translation which jointly transforms features from shared sequence based representations h 1 8943 h L 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT represent the sequence of bidirectional recurrent representations where h 1 8943 h L 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT h 1 8943 h L 119961 h 1 h h 1 1 8943 h L 119961 h h L 119961 L L 119961 x subscript 8462 1 8943 subscript 8462 subscript 119871 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 subscript 8462 1 subscript subscript 8462 h 1 1 8943 subscript 8462 subscript 119871 119961 subscript subscript 8462 h subscript 119871 119961 subscript subscript 119871 L 119961 x h 1 cdots h L boldsymbol x h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT represent the sequence of bidirectional recurrent representations where L 119961 subscript 119871 119961 L boldsymbol x italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT is the number of tokens for the source sentence L 119961 subscript 119871 119961 L boldsymbol x italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT L 119961 L L 119961 x subscript 119871 119961 subscript 119871 119961 subscript subscript 119871 L 119961 x L boldsymbol x L boldsymbol x italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT is the number of tokens for the source sentence 119961 119961 boldsymbol x bold italic x 119961 119961 boldsymbol x bold italic x 119961 x 119961 119961 x boldsymbol x boldsymbol x bold italic x bold italic x s i t superscript subscript 119904 119894 119905 s i t italic s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is is a recurrent neural network hidden state at time s i t superscript subscript 119904 119894 119905 s i t italic s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT s i t s s i i t t t superscript subscript 119904 119894 119905 superscript subscript 119904 119894 119905 superscript superscript subscript 119904 119894 subscript subscript 119904 s 119894 i 119905 t s i t s i t italic s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic s start POSTSUBSCRIPT italic i end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is is a recurrent neural network hidden state at time i 119894 i italic i for the i 119894 i italic i i i 119894 119894 i i i italic i italic i for the t 119905 t italic t th task which is estimated based on the combination of t 119905 t italic t t t 119905 119905 t t t italic t italic t th task which is estimated based on the combination of h 1 8943 h L 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT weighted by h 1 8943 h L 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT h 1 8943 h L 119961 h 1 h h 1 1 8943 h L 119961 h h L 119961 L L 119961 x subscript 8462 1 8943 subscript 8462 subscript 119871 119961 subscript 8462 1 8943 subscript 8462 subscript 119871 119961 subscript 8462 1 subscript subscript 8462 h 1 1 8943 subscript 8462 subscript 119871 119961 subscript subscript 8462 h subscript 119871 119961 subscript subscript 119871 L 119961 x h 1 cdots h L boldsymbol x h 1 cdots h L boldsymbol x italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 8943 italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT italic h start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic h start POSTSUBSCRIPT italic L start POSTSUBSCRIPT bold italic x end POSTSUBSCRIPT end POSTSUBSCRIPT weighted by A t superscript 119860 119905 A t italic A start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT A t superscript 119860 119905 A t italic A start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT A t A A t t t superscript 119860 119905 superscript 119860 119905 superscript superscript 119860 A 119905 t A t A t italic A start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic A start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT Table 5 Summary of deep MTL models a Cross stitch unit b Sluice block c NDDR unit d Soft Order e KD4MTL pipeline f MuST pipeline g OKD MTL pipeline h CrossDistil pipeline i PAD module j MTAN module k MTI Net l PAP module m PSD module n ASTMT o ATRC module p DeMT block q FAFS r BMTN s MTL NAS module t ASP MTL u MTA adv adv text adv start FLOATSUBSCRIPT adv end FLOATSUBSCRIPT N adv adv text adv start FLOATSUBSCRIPT adv end FLOATSUBSCRIPT adv adv adv adv adv adv adv adv text adv text adv start FLOATSUBSCRIPT adv end FLOATSUBSCRIPT start FLOATSUBSCRIPT adv end FLOATSUBSCRIPTN v RD4MTL w GREAT4MTL x ML GCN y MetaLink Figure 8 Frameworks of deep learning techniques used in MTL a d Feature fusion cross stitch networks Sluice Network NDDR CNN and Soft Order e h Knowledge distillation KD4MTL MuST OKD MTL and CrossDistill i p Attention PAD MTAN MTI Net PAP PSD ASTMT ATRC and DeMT q s NAS FAFS BMTN and MTL NAS t w Adversarial MTL ASP MTL MTAN RD4MTL and AAMTRL x y Graph ML GCN and MetaLink a Hard sharing b Soft sharing c Adaptive sharing Figure 9 Architecture taxonomy proposed by ruder2017overview for deep multi task sharing a Hard parameter sharing b soft parameter sharing and c adaptive sharing The 1D arrows indicate computations within the neural networks involving learnable parameters The 2D shapes and 3D cubes represent the final responses and extracted features respectively ruder2017overview for deep multi task sharing a Hard parameter sharing b soft parameter sharing and c adaptive sharing The 1D arrows indicate computations within the neural networks involving learnable parameters The 2D shapes and 3D cubes represent the final responses and extracted features respectively Table 6 Summary of notations used in Sec 2 2 2 2 a TCDCN b Fast RCNN Figure 10 Two of the earliest applications of hard parameter sharing in CNNs A the Tasks Constrained Deep Convolutional Network TCDCN which jointly extracts common features from human faces for multiple tasks such as landmark detection head pose estimation and facial attribute inference and B the Fast Region based Convolutional Network method Fast R CNN where each region of interest RoI is projected into a fixed size feature map first and then mapped to a feature vector used for both object probability prediction and bounding box offsets regression a Vanilla Cascading b Cascading with Prediction Shortcuts c Cascading with Prediction and Feature Shortcuts d Cascading with Prediction and Residual Shortcuts Figure 11 The taxonomy of cascading structures into four categories A the vanilla cascading structure B the cascading structure with prediction shortcuts C the cascading structure with prediction and feature shortcuts and D the cascading structure with prediction and residual shortcuts Figure 12 The computational details of Context Pooling CP a Dominant Gradients Issue b Conflicting Gradients Issue Figure 13 a dominant gradients issue The update direction 119941 119941 boldsymbol d bold italic d is dominated by the negative gradient of the loss of task 1 b 119941 119941 boldsymbol d bold italic d 119941 d 119941 119941 d boldsymbol d boldsymbol d bold italic d bold italic d is dominated by the negative gradient of the loss of task 1 b conflicting gradients issue When 945 t t 1 3 superscript subscript superscript 120572 119905 119905 1 3 alpha t t 1 3 italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT 3 end POSTSUPERSCRIPT are not properly set the update direction 945 t t 1 3 superscript subscript superscript 120572 119905 119905 1 3 alpha t t 1 3 italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT 3 end POSTSUPERSCRIPT 945 t t 1 3 945 t 945 t 945 a t t t t 1 t t 1 1 3 3 superscript subscript superscript 120572 119905 119905 1 3 superscript subscript superscript 120572 119905 119905 1 3 superscript superscript subscript superscript 120572 119905 119905 1 subscript subscript superscript 120572 119905 superscript 120572 119905 superscript superscript 120572 119905 t 119905 1 119905 t 1 1 3 3 alpha t t 1 3 alpha t t 1 3 italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT 3 end POSTSUPERSCRIPT italic a start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT 3 end POSTSUPERSCRIPT are not properly set the update direction 119941 119941 boldsymbol d bold italic d can decreases the loss of task 1 and 3 while increases the loss of task 2 Therefore the pefromance on the task 2 is compromised 119941 119941 boldsymbol d bold italic d 119941 d 119941 119941 d boldsymbol d boldsymbol d bold italic d bold italic d can decreases the loss of task 1 and 3 while increases the loss of task 2 Therefore the pefromance on the task 2 is compromised a Conflicting b Non conflicting c Projecting 119944 i subscript 119944 119894 boldsymbol g i bold italic g start POSTSUBSCRIPT italic i end POSTSUBSCRIPT to 119944 i subscript 119944 119894 boldsymbol g i bold italic g start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 119944 i 119944 g i i subscript 119944 119894 subscript 119944 119894 subscript subscript 119944 g 119894 i boldsymbol g i boldsymbol g i bold italic g start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold italic g start POSTSUBSCRIPT italic i end POSTSUBSCRIPT to 119951 j subscript 119951 119895 boldsymbol n j bold italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 119951 j subscript 119951 119895 boldsymbol n j bold italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 119951 j 119951 n j j subscript 119951 119895 subscript 119951 119895 subscript subscript 119951 n 119895 j boldsymbol n j boldsymbol n j bold italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT bold italic n start POSTSUBSCRIPT italic j end POSTSUBSCRIPT d Projecting 119944 j subscript 119944 119895 boldsymbol g j bold italic g start POSTSUBSCRIPT italic j end POSTSUBSCRIPT to 119944 j subscript 119944 119895 boldsymbol g j bold italic g start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 119944 j 119944 g j j subscript 119944 119895 subscript 119944 119895 subscript subscript 119944 g 119895 j boldsymbol g j boldsymbol g j bold italic g start POSTSUBSCRIPT italic j end POSTSUBSCRIPT bold italic g start POSTSUBSCRIPT italic j end POSTSUBSCRIPT to 119951 i subscript 119951 119894 boldsymbol n i bold italic n start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 119951 i subscript 119951 119894 boldsymbol n i bold italic n start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 119951 i 119951 n i i subscript 119951 119894 subscript 119951 119894 subscript subscript 119951 n 119894 i boldsymbol n i boldsymbol n i bold italic n start POSTSUBSCRIPT italic i end POSTSUBSCRIPT bold italic n start POSTSUBSCRIPT italic i end POSTSUBSCRIPT Figure 14 Demonstration of gradient projection technique used in yu2020gradient yu2020gradient Figure 15 An illustration of weak and strict Pareto minimizers and Pareto front We emphasize that the circles and crosses on the curve are NOT weak and strict Pareto minimizers Instead those 119934 8727 superscript 119934 boldsymbol W bold italic W start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT s that generate circles and crosses are weak and strict Pareto minimizers respectively In this figure all circles and crosses are Pareto stationary points We remark that in this example the Pareto front is convex and continuous The Pareto front can also be non convex and or discontinued for example see 119934 8727 superscript 119934 boldsymbol W bold italic W start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 119934 8727 119934 W 8727 superscript 119934 superscript 119934 superscript superscript 119934 W boldsymbol W boldsymbol W bold italic W start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT bold italic W start POSTSUPERSCRIPT end POSTSUPERSCRIPTs that generate circles and crosses are weak and strict Pareto minimizers respectively In this figure all circles and crosses are Pareto stationary points We remark that in this example the Pareto front is convex and continuous The Pareto front can also be non convex and or discontinued for example see liu2020review liu2020review Table 7 Algorithms for the MTL as a multi objective optimization a MoE b Multi Router MoE c Single Router MoE Figure 16 The taxonomy of a MoE into two categories b Multi Router MoE c Single Router MoE Figure 17 An example of MultiKernel predicting the probability of a data sample t 119905 t italic t belonging to task t 119905 t italic t t t 119905 119905 t t t italic t italic t belonging to task T 2 subscript 119879 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT MultiKernel conducts the prediction based on T 2 subscript 119879 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT T 2 T T 2 2 subscript 119879 2 subscript 119879 2 subscript subscript 119879 T 2 2 T 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT MultiKernel conducts the prediction based on T 2 subscript 119879 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT and its domains whose hierarchical relation is extracted from the predefined tree Specifically ellipses are domains and squares are tasks T 2 subscript 119879 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT T 2 T T 2 2 subscript 119879 2 subscript 119879 2 subscript subscript 119879 T 2 2 T 2 T 2 italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic T start POSTSUBSCRIPT 2 end POSTSUBSCRIPT and its domains whose hierarchical relation is extracted from the predefined tree Specifically ellipses are domains and squares are tasks a Downstream Fine Tuning b Task Prompting c Unified Generalist Model Figure 18 The taxonomy of PFMs of MTL into three categories A Downstream Task Fine Tuning B Task Prompting C Unified Generalist Model Figure 19 The Framework of unified generalist model which can unify architectures tasks and modalities through a simple seq2seq learning architecture Table 8 Summary of common datasets used in MTL Table 9 Summary of library for MTL ,
74,S2.T5, Model Name Origin Year MTL Strategy Backbone Sharing Modality Task Measurement Loss Function Availability 1 TCDCN ECCV zhang2014facial Early stopping CNN Hard Image Facial landmark detection head pose estimation Mean error mErr 160 burgos2013robust Mean squared error MSE gender classification age estimation expression failure rate 160 dantone2012real cross entropy CE loss Official recognition facial attribute inference ACL MTL ML IJCNLP dong2015multi 8212 RNN Hard Text Multiple target language translation BLEU 4 160 papineni2002bleu Delta CE loss 8212 Vanilla Part Of Speech POS Chunking Combinatory Cascading ACL sogaard goldberg 2016 deep Cascading LSTM Hard Text Categorical Grammar CCG Supertagging F1 score Micro F1 score CE loss 8212 Surface normals estimation normals semantic mErr median error medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT in angular Cross stitch segmentation semseg object detection attribute distance within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT pixel accuracy pixacc networks CVPR misra2016cross 8212 CNN Soft Image prediction mIoU fwIU mAP CE loss Unofficial ASP MTL aka Hard amp CE loss adversarial loss AdvMTL arXiv liu2017adversarial Adversarial training LSTM Soft Text Text classifications Error rate orthogonality constraint Official Cascading adding Part Of Speech POS tagging chunking parsing Accuracy acc F1 MSE unlabeled attachment CE loss softmax loss JMT EMNLP hashimoto jmt 2017 EMNLP2017 constraints LSTM Soft Text semantic relatedness textual entailment score UAS labeled attachment score LAS KL divergence Unofficial Object detection mask estimation object Mask regression loss MNCs CVPR dai2016instance Cascading CNN Hard Image categorization mAP IoU softmax loss Official FAFS CVPR lu2017fully NAS CNN Hard Image person attribute classification Acc recall CE loss Official Hard amp MRN NeurIPS long2017learning Task conditioning CNN Soft Image classifications on different domains Acc CE loss Official Depth scene parsing contour rel 160 eigen2014depth RMSE log10 mErr CE loss softmax loss PAD Net CVPR xu2018pad Mutual distillation CNN Hard Image prediction normals acc with threshold 948 120575 delta italic 948 acc 948 120575 delta italic 948 IoU acc Euclidean loss 8212 MT A adv subscript A adv text A text adv A start POSTSUBSCRIPT adv end POSTSUBSCRIPT N CVPR liu2018multi Adversarial training CNN Hard Image font glyph identity pose illumination Recognition rate CE loss adversarial loss 8212 cross task rel berHu loss 160 laina2016deeper TRL ECCV zhang2018joint attention CNN Hard Image Depth estimation depth semseg RMSE acc 948 120575 delta italic 948 pixacc mean acc mIoU CE loss uncertainty loss 8212 MMoE KDD ma2018modeling MoE MLP Hard amp Tabular Income education marriage prediction Area Under the Curve AUC CE loss Unofficial soft data engagement satisfaction in recommendation Tabular Soft Order ICLR meyerson2018beyond feature fusion CNN MLP Soft data image Classification attribute recognition mErr CE loss 8212 classification colorization edge denoised GREAT4MTL arXiv sinha2018gradient adversarial training CNN Hard Image reconstruction depth normal keypoint Err RMSE 1 8722 cos 8290 8901 8901 1 cos 8901 8901 1 text cos cdot cdot 1 cos 8901 8901 CE loss 8212 Sluice Adding constraints Hard amp Chunking entity recognition NER semantic networks AAAI ruder2019latent early stopping LSTM Soft Text role labeling SRL POS tagging Acc CE loss Official CNN NER Entity Mention Detection EMD Relation F1 score precision recall MUC B3 CEAFe HMTL AAAI sanh2019hierarchical cascading LSTM Hard Text Extraction RE Coreference Resolution CR moosavi2016coreference CE loss Unofficial CNN Segment labeling Named Entity Labeling CRF loss CE loss ranking DCMTL AAAI gong2019deep cascading LSTM Hard Text NEL slot filling F1 score precision recall loss 160 vu2016bi Official Normals semseg age estimation gender mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU pixacc mean NDDR CNN CVPR gao2019nddr feature fusion CNN Soft Image classification median absolute error absErr acc CE loss Official cross task RMSE rel acc with t 119905 t italic t mErr medError within CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss berHu loss PAP CVPR zhang2019pattern attention CNN Hard Image Semseg depth normals t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU mean accuracy mAcc pixacc affinity loss 160 zhang2019pattern 8212 MT A atten subscript A atten text A text atten A start POSTSUBSCRIPT atten end POSTSUBSCRIPT N Hard amp Semseg depth normals 10 classifications mIoU pixacc mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT amp DWA CVPR liu2019end Adaptive weighting CNN Soft Image visual domain decathlon 2 8201 8201 8201 absErr real error accuracy CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss dot product Official Semseg depth edge normals human parts mIoU osdF mErr maximum F measure maxF ASTMT CVPR maninis2019attentive attention single tasking CNN Hard Image saliency estimation albedo RMSE 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official ML GCN CVPR chen2019multi Graph based CNN GCN Hard Image Multi label recognition precision recall F1 CE loss Official RD4MTL arXiv meng2019representation Adversarial training CNN Hard Image Classifications Acc CE loss adversarial loss Official MTL NAS CVPR gao2020mtl NAS CNN Adaptive Image Semseg normals object classification scene mErr medErr Within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT mIoU pixacc CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Official classification Acc Semseg edge depth keypoint detection point BMTN BMVC vandenhende2019branched NAS CNN Adaptive Image attribute classification mIoU pixacc 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT Acc CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT Official PSD CVPR zhou2020pattern Distillation CNN Hard amp soft Image Semseg depth normals RMSE rel acc with t 119905 t italic t mIoU mean accuracy CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss berHu loss 8212 pixacc mErr medErr within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT ECCV distillation Hard amp mIoU pixacc absErr rel mErr medErr within KD4MTL Workshop li2020knowledge knowledge CNN soft Image Semseg depth normals classification t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT Acc CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss dot product Official MTI Net ECCV vandenhende2020mti multi task CNN Hard amp Image Semseg depth edges detection edges normals mIoU RMSE mErr optimal dataset scale F CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official distillation Soft saliency estimation human parts measure odsF 160 martin2004learning 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT NAS Regression face attribute prediction semseg LTB ICML guo2020learning task grouping CNN Soft Image normals depth keypoints edges Acc CE cos mean absErr CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cosine loss 8212 CNN amp AAMTRL ICML mao2020adaptive adversarial training LSTM Hard Text Classifications Relatedness evolution acc influence of task Any 1 Lipschitz loss 8212 Hard amp Tabular Sub tasks in the recommendation systems CGC amp PLE RecSys tang2020progressive MoE MLP soft data income education marriage prediction AUC MSE MTL gain CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Unofficial TSNs ICCV sun2021task task relationship learning CNN Hard Image Semseg depth edges normals mIoU RMSE mErr odsF 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official task conditioning saliency estimation human parts knowledge distillation Classification detection semseg depth MuST ICCV ghiasi2021multi task conditioning CNN Hard Image normals Acc mIoU RMSE odsF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 8212 AuxSegNet ICCV xu2021leveraging cross task CNN Hard amp Image Semseg classification saliency detection mIoU precision recall Multi label softmax Official attention Soft loss CE loss cross task Hard amp Semseg depth estimation edges normals ATRC ICCV bruggemann2021exploring attention CNN soft Image saliency estimation human parts mIoU RMSE mErr odsF maxF 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official DSelect k NeurIPS hazimeh2021dselect MoE MLP CNN Hard amp Tabular engagement satisfaction task classification Total loss Acc AUC RMSE expert CE loss 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT loss Official soft data Image Hard amp 16 Language understanding tasks e g textual Acc Spearman correlation 160 spearman1961proof Matthews MT TaG ArXiv gupta2022sparsely MoE Transformer soft Text entailment sentiment classification etc correlation coefficient 160 matthews1975comparison CE loss MSE 8212 Hard amp Tabular CrossDistil AAAI yang2022cross distillation MLP soft data Finish watching like AUC multi AUC 160 hand2001simple CE loss 8212 MulT CVPR bhattacharjee2022mult cross task CNN amp Hard Image Semseg depth reshading normals MTL gain mErr of CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official attention Transformer keypoints edges domain generalization rotate loss 160 zamir2018taskonomy cross task attention CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cross task balancing spec Hard amp Semseg depth saliency detection task contrastive loss MTFormer ECCV xu2022mtformer kendall2018multi Transformer soft Image human parts mIoU RMSE 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT uncertainty loss 8212 MQTransformer arXiv xu2022multi cross task Transformer Hard amp Image Semseg depth edges normals saliency mIoU RMSE mErr odsF maxF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss 8212 attention Soft estimation human parts Image MetaLink ICLR cao2022relational Graph based MLP GNN Hard Graph Classification mAP ROC AUC CE loss Official DeMT AAAI zhang2023demt cross task CNN amp Hard amp Image Semseg depth edges normals saliency mIoU RMSE mErr odsF maxF CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss Official attention Transformer Soft estimation human parts 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT cross task Hard amp CE loss berHu loss cosine mTEB WACV lopes2023cross attention CNN soft Image Semseg depth normals edges 916 m subscript 916 119898 Delta m roman 916 start POSTSUBSCRIPT italic m end POSTSUBSCRIPT mIoU RMSE mErr F1 loss 160 guizilini2021geometric Official OKD MTL WACV jacob2023online distillation task Transformer Hard amp Image Semseg depth normals 916 p subscript 916 119901 Delta p roman 916 start POSTSUBSCRIPT italic p end POSTSUBSCRIPT mIoU pixacc absErr rel mErr medErr Adaptive feature distillation loss 8212 weighting Soft within t 8728 superscript 119905 t circ italic t start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT CE loss 8467 1 subscript 8467 1 ell 1 roman 8467 start POSTSUBSCRIPT 1 end POSTSUBSCRIPT loss cosine loss Hard amp AdaMV MoE ICCV chen2023adamv MoE Transformer soft Image classification detection Seg Acc Average Precision AP CE loss Official ,Table 5 Summary of deep MTL models ,This section begins with a discussion of the architecture taxonomy commonly adopted in deep MTL which serves as the backbone for the rest of the method overview In the following we summarize the feature propagation techniques that include feature fusion see SS 2 2 1 cascading see SS 2 2 2 distillation see SS 2 2 3 and cross task attention see SS 2 2 4 These techniques encourage networks to automatically combine the features learned from different tasks addressing the crucial challenge of effectively and efficiently utilizing the rich features enabled by DL SS 2 2 5 presents an overview of task balancing techniques in deep MTL incorporating the linear combination of different tasks through three essential factors gradient loss and learning speed The comparison and recalibration of these factors aim to coordinate diverse tasks during the model weight update process We will discuss this section from the point of gradient correction and dynamic weighting In contrast SS 2 2 6 explores MOO in the context of MTL which aims to simultaneously optimize potentially conflicting objective functions Other promising topics covered include adversarial multi task training see SS 2 2 7 MoE see SS 2 2 8 GCN based MTL see SS 2 2 9 and NAS for MTL see SS 2 2 10 The summary of deep MTL models is presented in Table 5 and representative DL frameworks in MTL are illustrated in Fig 8 
75,S2.T6, Notation Description b B 119887 119861 b B italic b italic B Batch size l 8290 r 119897 119903 lr italic l italic r Learning rate 119987 l t 8712 8477 B 215 H 215 W 215 C mathcal X l t in mathbb R B times H times W times C caligraphic X start POSTSUBSCRIPT italic l end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic B 215 italic H 215 italic W 215 italic C end POSTSUPERSCRIPT Feature maps output from l 119897 l italic l th layer of t 119905 t italic t th task where B H W C B H W C italic B italic H italic W italic C are batch size height width and channel 119986 8712 8477 S 215 S 215 C in 215 C out 119986 superscript 8477 119878 119878 subscript 119862 in subscript 119862 out mathcal W in mathbb R S times S times C text in times C text out caligraphic W 8712 blackboard R start POSTSUPERSCRIPT italic S 215 italic S 215 italic C start POSTSUBSCRIPT in end POSTSUBSCRIPT 215 italic C start POSTSUBSCRIPT out end POSTSUBSCRIPT end POSTSUPERSCRIPT Convolution filter where S 119878 S italic S denotes the size of filter and C in C out subscript 119862 in subscript 119862 out C text in C text out italic C start POSTSUBSCRIPT in end POSTSUBSCRIPT italic C start POSTSUBSCRIPT out end POSTSUBSCRIPT denote the number of input and output channels respectively exp 8290 8901 exp 8901 text exp cdot exp 8901 Exponential function 963 8290 8901 120590 8901 sigma cdot italic 963 8901 Sigmoid function where 963 8290 x 1 1 exp 8290 8722 x 120590 119909 1 1 exp 119909 sigma x 1 1 text exp x italic 963 italic x 1 1 exp italic x softmax 8290 8901 softmax 8901 text softmax cdot softmax 8901 Softmax function where softmax 8290 119961 j exp 8290 x j 8721 i exp 8290 x i subscript delimited softmax 119961 119895 exp subscript 119909 119895 subscript 119894 exp subscript 119909 119894 text softmax boldsymbol x j text exp x j sum i text exp x i softmax bold italic x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT exp italic x start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 8721 start POSTSUBSCRIPT italic i end POSTSUBSCRIPT exp italic x start POSTSUBSCRIPT italic i end POSTSUBSCRIPT for any entry index j 119895 j italic j sim 8290 8901 8901 sim 8901 8901 text sim cdot cdot sim 8901 8901 An arbitrary similarity function e g cosine similarity cos 8901 8901 8901 8901 cdot cdot 8901 8901 8857 direct product odot 8857 The element wise dot product L 8290 N 8290 8901 119871 119873 8901 LN cdot italic L italic N 8901 Layer norm M 8290 H 8290 S 8290 A 8290 q k v 119872 119867 119878 119860 119902 119896 119907 MHSA q k v italic M italic H italic S italic A italic q italic k italic v Multi head self attention operator C 8290 O 8290 N 8290 V 119986 8290 8901 119862 119874 119873 subscript 119881 119986 8901 CONV mathcal W cdot italic C italic O italic N italic V start POSTSUBSCRIPT caligraphic W end POSTSUBSCRIPT 8901 Convolution operation parametrized by 119986 119986 mathcal W caligraphic W R 8290 E 8290 S 8290 H 8290 A 8290 P 8290 E 8290 8901 119877 119864 119878 119867 119860 119875 119864 8901 RESHAPE cdot italic R italic E italic S italic H italic A italic P italic E 8901 Reshape operation to rearrange the original feature maps in 8477 H 215 W 215 C superscript 8477 119867 119882 119862 mathbb R H times W times C blackboard R start POSTSUPERSCRIPT italic H 215 italic W 215 italic C end POSTSUPERSCRIPT space into a new 8477 H 8290 W 215 C superscript 8477 119867 119882 119862 mathbb R HW times C blackboard R start POSTSUPERSCRIPT italic H italic W 215 italic C end POSTSUPERSCRIPT space ,Table 6 Summary of notations used in Sec 2 2 2 2 ,In SS 1 1 we progressively introduce Multi Task Learning MTL starting with a broad sense and culminating in a formal definition Subsequently SS 1 2 explores the position of MTL within the Machine Learning ML landscape drawing comparisons with related paradigms such as Transfer Learning TL Few Shot Learning FSL lifelong learning Multi View Learning MVL to name a few SS 1 3 delves into the motivations for employing MTL offering insights from both explicit and subtle angles while also addressing how MTL benefits the involved tasks In SS 1 4 we delve deeper into the fundamental mechanisms and theories underpinning MTL specifically 1 regularization 2 inductive bias and 3 feature sharing providing an understanding of its underlying principles Finally SS 1 5 reviews existing surveys on MTL underscoring the unique contributions of our survey and laying out a structured roadmap for the remainder of this work The structure of our survey is depicted in Fig 2 Before delving into this survey readers can quickly refer to Table 1 for a list of acronyms not related to datasets institutions and newly proposed methods while an overview of mathematical notations is provided in Table 3 and Table 6 Unless explicitly stated otherwise we employ the notation provided in Tab 6 within the context of DL settings to expand upon and complement the information presented in Tab 3 
76,S2.T7, Algorithm Venue Year Method Convergence Highlight Availability 1 Uncertainty Weighting CVPR kendall2018multi Dynamic Weighting 8212 Optimize 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT and 119934 119934 boldsymbol W bold italic W simultaneously Official GradNorm ICML chen2018gradnorm Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the average gradient norm of each tasks and the relative progress achieved for each tasks Unofficial MGDA MTL NeurIPS sener2018multi Multi Objective Opt Asymptotic Convergence Seminal work which proposes to use MOO to solve deep MTL problems based on multi gradient descent algorithm Official RMTL Thesis liu2018exploration Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the relative progress achieved for each tasks Official LBTW AAAI liu2019loss Dynamic Weighting 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT using the reinforcement learning strategy Official DWA CVPR liu2019end Dynamic Weighting 8212 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is adapted to both samples and tasks Official MLDT CVPR zheng2019pyramidal Dynamic Weighting 8212 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is adapted to the likelihood of a loss reduction Official Pareto MTL NeurIPS lin2019pareto Multi Objective Opt Asymptotic Convergence Attemp to incorporate user 8217 s preference into the solution Official Controllable Pareto MTL arXiv lin2020controllable Multi Objective Opt 8212 Use a hypernetwork to learn the entire Pareto front Official PCGrad NeurIPS yu2020gradient Gradient Correction 8212 Projecting onto orthogonal subspace to mitigate the gradient conflicts Official GradDrop NeurIPS chen2020just Gradient Correction 8212 Only keep gradients are consistent in signs in each update Official Continuous Pareto MTL ICML ma2020efficient Multi Objective Opt 8212 Construct a continuous frst order approximation of the local Pareto set Official EPO Search ICML mahapatra2020multi Multi Objective Opt 8212 Find a Pareto stationary solution to exactly match a user 8217 s preference Require losses to be non negative Official AuxiLearn ICLR navon2021auxiliary Bi level Opt 8212 Learn to combine losses in a nonlinear fashion Official IMTL ICLR liu2021towards Gradient Correction Find 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT such that the aggregated gradient 8721 t 1 T 945 t 8290 9661 8290 8466 t 8290 119934 superscript subscript 119905 1 119879 superscript 120572 119905 9661 superscript 8466 119905 119934 sum t 1 T alpha t triangledown mathcal L t boldsymbol W 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 9661 caligraphic L start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic W has equal projection onto the raw gradients of individual tasks Unofficial GradVac ICLR wang2021gradient Dynamic Weighting 8212 Encourage more geometrically aligned parameter updates for close tasks Unofficial PHN ICLR navon2021learning Multi Objective Opt 8212 Use a hypernetwork to learn the entire Pareto front Official CAGrad NeurIPS liu2021conflictaverse Gradient Correction Asymptotic Convergence The search direction is find by solving a subproblem that is similar to MGDA Official SVGD NeurIPS liu2021profiling Multi Objective Opt Convergence rate for strongly convex and third order continuously differentiable functions Integrate MGDA with Stein variational gradient descent and Langevin dynamics to obtain diverse solutions Official COSMOS ICDM ruchte2021scalable 8212 8212 A single optimization run to approximate the full set of the Pareto front by combining preferences vectors sampled from Dirichlet distribution and training data Official HV Maximization arXiv deist2021multi 8212 8212 Utilize hyper volume to approximate sample level Pareto front Official PNG UAI ye2022optimization Multi Objective Opt Convergence rate for convex losses Minimize preference loss over the Pareto front manifold optimization while only using the first order information 8212 RLW amp RGW TMLR lin2022reasonable Dynamic Weighting Converge to a neighborhood of the optimal solution under strongly convex assumption Sample the weights 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT from a given distribution at each step Unofficial Nash MTL ICML navon2022multi Multi Objective Opt Asymptotic Convergence Formulate the problem of finding a common descent direction as a bargaining game Official X WC MGDA ICML momma2022multi Dynamic Weighting 8212 Lift the restriction of non negativity requirement on losses in EPO search 8212 Rotograd ICLR javaloy2022rotograd Dynamic Weighting Gradient Correction 8212 Dynamic Weighting via gradient norm Gradient Correction via rotating the feature space Official MoCo 160 fernando2023mitigating ICLR 2023 Multi Objective Opt Convergence rates for convex amp nonconvex losses Stochastic Gradient amp Variance Reduction Official Recon ICLR shi2023recon Gradient Correction 8212 Turn shared parameters that most likely to cause gradient conflicts into task specific parameters Official Aligned MTL CVPR Senushkin 2023 CVPR Gradient Correction 8212 Use the condition number of the linear system to measure the severity of gradient dominance and conflicting issues Official Achievement based MTL ICCV yun2023achievement Dynamic Weighting 8212 Use training progress to dynamically weight tasks and use geometric mean to average loss from tasks Official FULLER ICCV huang2023fuller Dynamic Weighting 8212 Use gradient norm of different tasks to adjust the weights for tasks 8212 Adjust 945 t superscript 120572 119905 alpha t italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT is based on the average gradient norm of each tasks and the relative progress achieved for each tasks Seminal work which proposes to use MOO to solve deep MTL problems based on multi gradient descent algorithm Find a Pareto stationary solution to exactly match a user 8217 s preference Require losses to be non negative Find 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT such that the aggregated gradient 8721 t 1 T 945 t 8290 9661 8290 8466 t 8290 119934 superscript subscript 119905 1 119879 superscript 120572 119905 9661 superscript 8466 119905 119934 sum t 1 T alpha t triangledown mathcal L t boldsymbol W 8721 start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 9661 caligraphic L start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold italic W has equal projection onto the raw gradients of individual tasks Convergence rate for strongly convex and third order continuously differentiable functions Integrate MGDA with Stein variational gradient descent and Langevin dynamics to obtain diverse solutions A single optimization run to approximate the full set of the Pareto front by combining preferences vectors sampled from Dirichlet distribution and training data Minimize preference loss over the Pareto front manifold optimization while only using the first order information Converge to a neighborhood of the optimal solution under strongly convex assumption Sample the weights 945 t t 1 T superscript subscript superscript 120572 119905 119905 1 119879 alpha t t 1 T italic 945 start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT start POSTSUBSCRIPT italic t 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT from a given distribution at each step Formulate the problem of finding a common descent direction as a bargaining game Dynamic Weighting Gradient Correction Dynamic Weighting via gradient norm Gradient Correction via rotating the feature space Convergence rates for convex amp nonconvex losses Turn shared parameters that most likely to cause gradient conflicts into task specific parameters Use the condition number of the linear system to measure the severity of gradient dominance and conflicting issues Use training progress to dynamically weight tasks and use geometric mean to average loss from tasks Use gradient norm of different tasks to adjust the weights for tasks ,Table 7 Algorithms for the MTL as a multi objective optimization ,To conclude this section a comprehensive list to our best knowledge to include all existing optimization methods in SS 2 2 5 SS 2 2 6 is summarized in Table 7 
77,S4.T8, Dataset Source Year Modality Task Synopsis Task Sample Availability School Data ILEA mortimore1988school Table Regression Predicting student exam scores based on 27 school features 139 15 362 Official SARCOS Data Humanoid Robotics 2000 Table Regression Estimate inverse dynamics model 7 44 484 4449 Official Computer Survey Data Survey lenk1996hierarchical Table Regression Likelihood of purchasing personal computers 179 Climate Dataset Sensor network 2017 now Table Regression Real time climate data collected from four climate stations 7 Official 20 Newsgroups Netnews articles Lang95 Text Classification Hierarchical text classification 20 19 000 Official Reuters 21578 Collection Reuters 1996 Text Classification Reuters news documents with hierarchical categories 90 21 578 Official MultiMNIST Dataset MNIST sabour2017dynamic Image Classification Classify the digits on the different positions 2 Official ImageCLEF 2014 Caltech ImageNet Pascal Bing 2014 Image Classification Benchmark dataset for domain adaptation 4 2 400 Official Office Caltech Dataset Office Caltech gong2012geodesic Image Classification Benchmark dataset for the annotation and retrieval of images 4 2 533 Official Office 31 Dataset Amazon DSLR Webcam saenko2010adapting Image Classification Objects commonly encountered in office settings 3 4 110 Official Office Home Dataset Office venkateswara2017deep Image Classification Object recognition and domain adaptation in the era of deep learning 4 15 588 Official DomainNet Dataset UDA peng2019moment Image Classification Multi source unsupervised domain adaptation research 6 600 000 Official EMMa Dataset Amazon standley2023extensible Image Text Classification Amazon product listings for category prediction 2 800 000 Official SYNTHIA Dataset European Union ros2016synthia Image Classification A synthetic dataset for semantic segmentation 13 400 Official SVHN Dataset Stanford yang2021few Image Classification A digit classification benchmark dataset 600 000 Official CelebA Dataset MMLAB liu2018large Image Classification A large scale face attributes dataset 40 200 000 Official CityScapes Dataset Daimler AG cordts2016cityscapes Image Dense prediction Semantic urban scene understanding 5 000 Official NYU Depth Dataset V2 New York University silberman2012indoor Image Dense prediction Indoor scene understanding with per pixel labels 3 35 064 Official PASCAL VOC Project University of Oxford everingham2010pascal Image Dense prediction Object recognition with multiple tasks Official Taskonomy Dataset Standard zamir2018taskonomy Image Dense prediction Diverse dataset with 26 tasks for task transfer learning 26 4 000 000 Official STREET Amazon ribeiro2023street Text Reasoning The multi task structured reasoning and explanation benchmark VKITTI2 Dataset Naver cabon2020virtual Video Segmentation A video dataset which is automatically labeled with ground truth 5 Official XTREME Carnegie Mellon hu2020xtreme Text Translation QA A multilingual benchmark for evaluating cross lingual generalisation 9 400 000 Deepfashion Dataset Shopping Websites liu2016deepfashion Image Classification A large scale clothes dataset with comprehensive annotations 2 800 000 Official ACE05 Dataset News 2005 Text Classification A large corpus with annotated entities relations and events 3 52 615 Official ATIS Dataset Airline hemphill etal 1990 atis Text Classification A dataset with 17 unique intent categories 3 5 871 Official ,Table 8 Summary of common datasets used in MTL ,
78,S4.T9, Library Language Supported Methods RMTL R Sparse structure learning 160 tibshirani1996regression multi task feature selection 160 obozinski2006multi low rank MTL 160 ji2009accelerated pong2010trace graph based regularised MTL 160 widmer2010leveraging multi task clustering 160 gu2009learning MALSAR Matlab Sparse structure learning 160 tibshirani1996regression regularized MTL 160 evgeniou2004regularized multi task feature selection 160 obozinski2006multi dirty block sparse model 160 jalali2010dirty low rank MTL 160 ji2009accelerated pong2010trace convex ASO 160 chen2009convex sparse amp low rank MTL 160 chen2012learning clustered MTL 160 zhou2011clustered robust MTL 160 chen2011integrating robust multi task feature learning 160 gong2012robust Temporal group Lasso 160 zhou2011multi convex fused sparse group Lasso 160 zhou2012modeling incomplete multi source feature learning 160 yuan2012multi multi stage multi task feature learning 160 gong2012multi multi task clustering 160 gu2009learning LibMTL Python Cross stitch 160 misra2016cross GradNorm 160 chen2018gradnorm Uncertainty Weighting 160 kendall2018multi MGDA MTL 160 sener2018multi MMoE 160 ma2018modeling MultiNet 160 chennupati2019multinet LTB 160 guo2020learning MTAN amp DWA 160 liu2019end PCGrad 160 yu2020gradient GradDrop 160 chen2020just CGC amp PLE 160 tang2020progressive IMTL 160 liu2021towards GradVac 160 wang2021gradient CAGrad 160 liu2021conflictaverse DSelect k 160 hazimeh2021dselect RLW amp RGW 160 lin2022reasonable Nash MTL 160 navon2022multi ,Table 9 Summary of library for MTL ,
79,Sx4.T1, Table 1 Key demographics from the NCTE dataset LEP indicates that the students have Limited English Proficiency FRPL indicates that they receive Free or Reduced Price Lunches in the given school year and SPED indicates Special Education status Statistic Teachers Students Number 313 12661 Male 16 3 49 4 Female 82 4 49 6 No data 1 3 1 African American 22 4 42 3 Asian 2 6 7 4 Hispanic 2 9 22 6 White 64 2 22 7 Other 3 6 3 9 No data 0 1 LEP 19 7 No data 1 2 SPED 12 3 No data 1 2 FRPL 64 9 No data 1 2 ,Table 1 Key demographics from the NCTE dataset LEP indicates that the students have Limited English Proficiency FRPL indicates that they receive Free or Reduced Price Lunches in the given school year and SPED indicates Special Education status ,Table 1 shows the key demographics of the populations of students and teachers in the recordings The statistics show that the students are balanced by gender but the vast majority of them come from minority racial backgrounds 72 3 The majority of the students received free or reduced price lunches 64 9 indicating that they likely come from low income households A sizable percentage of the students exhibited limited English proficiency 19 7 or special educational status 12 3 On the other hand the teachers were mostly female 82 4 and White 64 2 This disparity between students and teachers demographics is within the national statistics Demszky and Hill 2022 
80,Sx4.T2, Table 2 Key statistics from the transcribed portion of the NCTE dataset Teacher refers to the percentage of speech duration attributed to the teacher AFAM refers to African American FF indicates a far field microphone and NN indicates a near field microphone Teacher Students Used for training and validation Class ID Gender Race Male AFAM Asian Hispanic White Other Students Teacher Microphone 144 Female White 85 62 0 8 31 0 13 84 NF 622 Female AFAM 53 16 11 47 26 0 19 79 NF 2619 Female White 48 76 5 0 19 0 21 84 FF 2709 Female White 63 13 29 13 42 4 24 68 NF 2944 Female White 54 46 4 8 38 4 26 84 NF 4724 Female White 46 46 0 11 43 0 28 92 NF Total 56 42 8 14 34 2 131 NF Used for testing and analysis 13 Male Asian 61 9 65 13 9 4 23 78 NF 4106 Female AFAM 50 14 50 21 0 14 12 81 NF 4352 Male White 41 34 10 7 48 0 29 88 NF 4651 Female AFAM 55 27 14 9 50 0 22 80 FF Total 51 23 32 11 31 3 88 NF ,Table 2 Key statistics from the transcribed portion of the NCTE dataset Teacher refers to the percentage of speech duration attributed to the teacher AFAM refers to African American FF indicates a far field microphone and NN indicates a near field microphone ,Out of these recordings 6 were initially randomly chosen to be transcribed to create a low resource unbalanced problem The duration of this subset is about 5 15 hours with the duration of each file between 45 60 minutes Key statistics about the transcribed recordings are in Table 2 All teachers are White women except for one who is an African American woman The student s gender is mostly balanced throughout the entire dataset with about 56 of the students being male All classes but one have a 45 65 male population Class 144 has 85 male students The majority of the students in the dataset come from minority racial backgrounds 66 with African American students making up 42 of the total student population and both Asian and Hispanic students making up 22 This racial imbalance in the dataset presents a unique challenge and an interesting question to be answered will pretraining on a racially diverse dataset improve the fairness and decrease the racial bias of the ASR system All but one recording in the dataset was recorded by a near field microphone and the recording of class 2619 comes from a far field microphone This distinction is by design to test how well the ASR system generalizes to microphone configurations unseen in the training data and whether pretraining on different microphone set ups improves this generalization In this section we discuss the results from the unseen NCTE test set As previously discussed and according to Table 2 the test set has a different racial makeup with better representations of Asian students and the presence of two male teachers one of them being Asian while all the teachers in the train validation subset are White women except for one African American woman This test shows how well the models generalize to different demographics unseen during training In the training validation dataset we note that the performance of class 622 is worse than similar classes with the same noise conditions and levels According to Table 4 comparing class 622 to classes 144 2709 and 4724 all of which have similar acoustic properties we see that all three models presented in this table have higher WER for this particular file with the worst offender being W2V Robust without CPT Our proposed CPT does narrow the gap and improves the performance by 8 however the gap still exists By referencing Table 2 we see that this class is the only class with an African American teacher and has the highest percentage of Hispanic students and the second lowest number of students in the dataset Looking at the test set results in Table 5 we see a similar pattern The best performing class with all the models is class 4352 where the teacher is a White man We note again the presence of some inaccuracies in the test set transcriptions resulting from inaccurate timestamps however these inaccuracies are common and constant between all four classes so the comparison between them still stands Classes 4106 and 4651 score worse with all models with the narrowest gap being with our W2V Robust CPT model about 10 By referencing Table 2 we see that class 4106 has a female African American teacher with the majority of the students being Asian Class 4651 also had a female African American teacher with a majority of White students Class 13 also scores lower than class 4352 where the teacher is an Asian man and upon manual inspection we note the presence of a light accent However his speech is perfectly enunciated and clear Upon further inspection of all the files we find that these classes are not noisier than the 4352 The teacher in class 4352 wore a lanyard microphone which interfered with his shirt buttons and caused a clicking sound throughout the recording Class 4352 also has the highest number of students in class All of these findings suggest that the ASR system should perform worse and not better in class 4352 but they don t 
81,Sx5.T3, Table 3 Average cross validation WER finetuning results starting from different Wav2vec2 0 checkpoints with and without continued pretraining as well as comparison with the small English only checkpoint of Whisper Whisper FT refers to finetuning Whisper on the target dataset The standard deviation of the WER is between brackets All models decoded with an n gram LM use our LM trained on race aware deanonymized NCTE Text corpus WER STD Model LM NCTE In house Whisper 24 46 12 37 30 15 12 99 Whisper FT 19 14 6 77 28 53 14 07 No Continued Pretraining W2V LV60K None 39 11 13 01 37 82 12 30 5 gram LM 30 39 14 48 33 56 10 86 XLS R None 38 19 10 39 39 12 13 60 5 gram LM 29 02 10 96 32 49 10 76 W2V Robust None 35 07 11 85 36 36 11 54 5 gram LM 27 99 13 28 31 49 9 97 Pretraining from Scratch W2V SCR None 47 34 5 73 51 39 6 83 5 gram LM 30 25 15 44 38 59 12 93 Continued Pretraining W2V LV60K CPT None 22 52 4 89 32 26 8 92 5 gram LM 18 13 5 50 26 72 7 72 XLS R CPT None 26 53 5 13 32 16 10 78 5 gram LM 19 37 4 91 26 80 8 00 W2V Robust CPT None 25 04 5 28 30 97 9 99 5 gram LM 17 71 5 06 26 50 8 09 ,Table 3 Average cross validation WER finetuning results starting from different Wav2vec2 0 checkpoints with and without continued pretraining as well as comparison with the small English only checkpoint of Whisper Whisper FT refers to finetuning Whisper on the target dataset The standard deviation of the WER is between brackets All models decoded with an n gram LM use our LM trained on race aware deanonymized NCTE Text corpus ,Table 3 shows the average cross validation WER across all folds for every model Without CPT finetuning W2V Robust yields superior performance This shows that initial pretraining on noisy data improves the noise robustness to unseen noises in different domains Additionally XLS R yields slightly better results than W2V LV60K which shows that training on much more data from other languages can be useful especially since some of these recordings might be noisier than LV 60K LM decoding improves results by 6 on average which is in line with the results from the Appendix of the seminal Wav2vec2 0 paper Baevski et al 2020 There is a slight performance gap between NCTE and In house indicating that the NCTE task is slightly easier which is in line with the noise levels observed upon manual inspection of the recordings The standard deviation in the results is quite high which shows that each recording has unique characteristics and thus the folds perform differently on each one showing that although we are considering classroom recordings to be one domain the kind of classroom environment be it collaborative learning or purely instructional affects the noise level and the amount of adult teacher s speech versus students speech which affects the performance One thing to note is that one of the files comes from a far field microphone to test how well the ASR system generalizes to unseen microphone configuration which we expand on in Table 4 Looking at the third and fourth sections of Table 3 the advantage of CPT when compared to pretraining from scratch is immediately clear Pretraining from scratch yields the worst performance in the entire table Previous works Zhu et al 2022 have shown that pretraining on noisy data yields superior performance to pretraining on clean data but in their tests they pretrained both configurations on the same dataset but augmented the training data with noise for noisy pretraining However in our experiments the clean pretraining data is at least 12 times bigger than the noisy pretraining data which explains why clean self supervised pretraining yields better performance in our experiments One interesting thing to note is that the gap between the LM decoded and the Viterbi decoded results when pretraining from scratch is much higher than with other configurations This suggests that the model does learn useful acoustical representations but does not learn enough linguistic properties during pretraining which is accounted for by LM decoding This result also suggests that initial pretraining on clean adult speech even from other languages learns useful linguistic representations that are not sufficiently learned from noisy in domain pretraining from scratch Note the wide gap between the Viterbi decoded WER from W2V LV60K CPT and W2V SCR of almost 25 In that regard CPT has the benefit of utilizing linguistic representations from clean and out of domain speech as well as learning the acoustic properties and further learning useful linguistic properties from in domain data Looking at CPT results in the fourth section of Table 3 we can see that for NCTE with LM decoding CPT improves WER by 10 73 on average which proves that CPT is a powerful tool for domain adaptation when labeled data is scarce and unlabeled data is plentiful For the In house dataset where the domain is slightly different CPT is still very effective yielding a performance improvement of about 5 on average proving that CPT is effective in domain adaptation in a generalizable fashion It is also noticeable how the standard deviation decreases in both datasets down to 5 06 in NCTE and 8 09 in the In house dataset This shows that performing CPT on diverse classroom environments and noise conditions improves the ability of the model to generalize to these conditions 
82,Sx5.T4, Table 4 Detailed WER results on each fold of the cross validation on both the NCTE and in house datasets for the off the shelf and the CPT version of W2V Robust and the finetuned small English only Whisper checkpoint NCTE Fold Whisper FT W2V Robust W2V Robust CPT 144 14 78 19 86 15 20 622 16 97 26 31 18 77 2619 28 4 54 03 27 15 2709 12 74 19 09 13 12 2944 26 98 28 07 17 61 4724 14 95 20 55 14 43 Average 19 14 27 99 17 71 In House Fold Whisper FT W2V Robust W2V Robust CPT OH 1 19 76 18 55 15 42 OH 2 16 42 19 89 16 88 DC 1 28 79 29 42 24 90 DC 2 26 42 37 32 30 34 CA 1 55 77 49 03 41 54 CA 2 24 04 34 47 29 91 Average 28 53 31 45 26 50 ,Table 4 Detailed WER results on each fold of the cross validation on both the NCTE and in house datasets for the off the shelf and the CPT version of W2V Robust and the finetuned small English only Whisper checkpoint ,Table 3 shows the average cross validation WER across all folds for every model Without CPT finetuning W2V Robust yields superior performance This shows that initial pretraining on noisy data improves the noise robustness to unseen noises in different domains Additionally XLS R yields slightly better results than W2V LV60K which shows that training on much more data from other languages can be useful especially since some of these recordings might be noisier than LV 60K LM decoding improves results by 6 on average which is in line with the results from the Appendix of the seminal Wav2vec2 0 paper Baevski et al 2020 There is a slight performance gap between NCTE and In house indicating that the NCTE task is slightly easier which is in line with the noise levels observed upon manual inspection of the recordings The standard deviation in the results is quite high which shows that each recording has unique characteristics and thus the folds perform differently on each one showing that although we are considering classroom recordings to be one domain the kind of classroom environment be it collaborative learning or purely instructional affects the noise level and the amount of adult teacher s speech versus students speech which affects the performance One thing to note is that one of the files comes from a far field microphone to test how well the ASR system generalizes to unseen microphone configuration which we expand on in Table 4 In this section we do a more detailed analysis of the results We start by discussing the results in Table 4 which shows the WER of each fold of the cross validation in Whisper FT and W2V Robust with and without CPT In the training validation dataset we note that the performance of class 622 is worse than similar classes with the same noise conditions and levels According to Table 4 comparing class 622 to classes 144 2709 and 4724 all of which have similar acoustic properties we see that all three models presented in this table have higher WER for this particular file with the worst offender being W2V Robust without CPT Our proposed CPT does narrow the gap and improves the performance by 8 however the gap still exists By referencing Table 2 we see that this class is the only class with an African American teacher and has the highest percentage of Hispanic students and the second lowest number of students in the dataset 
83,Sx5.T5, Table 5 Test set WER results on the held out files from the NCTE dataset not used in cross validation training Each entry is the average WER of all the cross validation versions of a particular model when tested on a particular recording in the test set with the standard deviation in brackets Average shows the total average WER of each model Model Whisper Whisper FT W2V Robust W2V Robust CPT 13 26 02 25 89 1 70 25 55 0 66 25 07 0 73 4106 25 10 35 65 8 17 30 59 0 90 25 79 0 26 4352 22 27 22 13 0 41 26 15 0 59 22 30 0 26 4651 50 74 39 82 6 88 35 19 0 43 29 47 0 40 Average 31 03 31 12 30 12 25 66 ,Table 5 Test set WER results on the held out files from the NCTE dataset not used in cross validation training Each entry is the average WER of all the cross validation versions of a particular model when tested on a particular recording in the test set with the standard deviation in brackets Average shows the total average WER of each model ,From Table 5 CPT improves the WER on average from 30 12 to 25 66 with the decreased standard deviation showing less variation in performance with different training data This shows that CPT smoothes out the differences caused by different representations in the training data When compared to Whisper both the finetuned and non finetuned versions of Whisper perform worse than both W2V Robust models and finetuning seems to even harm the performance on average indicating that Whisper overfits Looking at the test set results in Table 5 we see a similar pattern The best performing class with all the models is class 4352 where the teacher is a White man We note again the presence of some inaccuracies in the test set transcriptions resulting from inaccurate timestamps however these inaccuracies are common and constant between all four classes so the comparison between them still stands Classes 4106 and 4651 score worse with all models with the narrowest gap being with our W2V Robust CPT model about 10 By referencing Table 2 we see that class 4106 has a female African American teacher with the majority of the students being Asian Class 4651 also had a female African American teacher with a majority of White students Class 13 also scores lower than class 4352 where the teacher is an Asian man and upon manual inspection we note the presence of a light accent However his speech is perfectly enunciated and clear Upon further inspection of all the files we find that these classes are not noisier than the 4352 The teacher in class 4352 wore a lanyard microphone which interfered with his shirt buttons and caused a clicking sound throughout the recording Class 4352 also has the highest number of students in class All of these findings suggest that the ASR system should perform worse and not better in class 4352 but they don t 
84,A1.T6, Table 6 Full results from cross validation finetuning off all finetuned Wav2vec2 0 models both with and without CPT as well as Whisper small en both finetuned and not off the shelf Fine tuning data NCTE Model Whisper Whisper FT W2V SCR W2V LV60K W2V LV60K CPT XLS R XLS R CPT W2V Robust W2V Robust CPT LM None 5 g LM None 5 g LM None 5 g LM None 5 g LM None 5 g LM None 5 g LM None 5 g LM 144 20 38 14 78 43 31 27 85 31 01 21 75 22 41 14 78 32 12 22 31 22 57 15 51 26 81 19 86 21 48 15 20 622 21 43 16 97 47 80 30 55 36 08 27 66 26 53 19 13 36 72 28 09 29 32 21 46 33 22 26 31 26 79 18 77 2619 47 6 28 4 57 75 42 86 62 86 58 48 34 58 28 39 57 77 50 50 35 47 28 04 57 58 54 03 34 58 27 15 2709 14 34 12 74 41 38 22 84 30 80 20 60 21 02 13 03 30 68 21 17 22 32 14 75 27 24 19 09 20 10 13 12 2944 27 98 26 98 48 20 26 64 45 02 32 22 25 65 18 05 40 99 28 56 26 20 19 31 37 87 28 07 25 17 17 61 4724 15 14 95 45 60 30 74 28 91 21 60 22 94 15 40 30 86 23 46 23 29 17 17 27 70 20 55 22 14 14 43 Average 24 46 12 37 19 14 6 77 47 34 5 73 30 25 6 83 39 11 13 01 30 39 14 48 25 52 4 89 18 13 5 50 38 19 10 39 29 02 10 96 26 53 5 13 19 37 4 91 35 07 11 85 27 99 13 28 25 04 5 28 17 71 5 06 Fine tuning data In House OH 1 27 65 19 76 33 60 23 34 23 25 20 40 18 75 15 84 24 34 20 01 19 81 16 68 21 77 18 55 18 61 15 42 OH 2 16 84 16 42 33 53 24 86 26 03 24 04 21 14 17 53 23 43 21 40 18 64 17 23 21 83 19 97 17 80 16 88 DC 1 23 73 28 79 52 01 37 70 35 99 32 43 31 33 24 69 39 31 32 50 30 18 24 61 35 63 29 44 30 62 24 90 DC 2 32 76 26 42 59 55 43 49 44 73 37 03 37 27 30 15 47 75 36 05 39 50 30 56 44 65 37 70 36 81 30 34 CA 1 54 44 55 77 72 88 57 33 56 45 50 99 48 33 40 69 58 64 49 09 51 23 41 43 56 85 49 42 48 61 41 54 CA 2 25 48 24 04 56 74 44 80 40 44 36 46 36 74 31 54 41 26 35 86 33 59 30 30 37 43 33 84 33 40 29 91 Averge 30 15 12 99 28 53 14 07 51 39 15 44 38 59 12 93 37 82 12 30 33 56 10 86 32 26 8 92 26 74 7 72 39 12 13 60 32 49 10 76 32 16 10 78 26 80 8 00 36 36 11 54 31 49 9 74 30 97 9 99 26 50 8 09 ,Table 6 Full results from cross validation finetuning off all finetuned Wav2vec2 0 models both with and without CPT as well as Whisper small en both finetuned and not off the shelf ,
85,Pt0.A1.T5, Method Scale paper Scale sup Boundary 948 1 delta 1 8593 8593 uparrow RMS 8595 8595 downarrow 948 1 delta 1 8593 8593 uparrow RMS 8595 8595 downarrow 949 D 8203 B 8203 E a 8203 c 8203 c subscript superscript 120576 119886 119888 119888 119863 119861 119864 varepsilon acc DBE 8595 8595 downarrow 949 D 8203 B 8203 E c 8203 o 8203 m 8203 p subscript superscript 120576 119888 119900 119898 119901 119863 119861 119864 varepsilon comp DBE 8595 8595 downarrow PR 8475 8475 mathcal R FT 95 418 3 992 96 197 3 601 3 301 1 947 Ours 955 1 8203 e 8722 1 120582 1 superscript 119890 1 lambda 1e 1 95 359 3 982 96 235 3 589 2 848 1 790 Ours 955 1 120582 1 lambda 1 95 077 4 222 96 063 3 689 2 494 1 714 ,Table 5 Quantitative Comparison on CityScapes Scale paper and Scale sup indicate the scale evaluation on all valid pixels in main paper and on non boundary pixel presented in supplementary materials respectively ,We follow the implementation of the monocular depth estimation challenge111https github com jspenmar monodepth benchmark 53 54 to calculate the eD B Ec o m psubscriptsuperscriptcompDBE varepsilon comp DBE and eD B Ec o m psubscriptsuperscriptcompDBE varepsilon comp DBE To calculate the expanded GT depth edge map we utilize a Gaussian blur with kernel size k 7k7k 7 All pixels with value 1absent1 1 are set as 1 as the expanded edge after the gaussian blur The precision recall and F 1 Score used in our paper can be calculated with the depth prediction edge MM mathbf M and the final GT edge map M M hat textbf M We present the results in Tab 5 Our approach achieves significant improvement on both of these metrics However as Fig 9 illustrates GT depth values near edges exhibit higher errors potentially skewing the scale metrics To more accurately assess our method s effectiveness this supplementary section presents scale metrics for non boundary regions For implementation we re use the final GT edge map M M hat textbf M from the boundary metric and apply an additional Gaussian blur kernel size k 7k7k 7 to it as shown in Fig 8 The comparative results are displayed in Tab 5 indicating the effectiveness of our method on maintaining the scale accuracy 
86,Pt0.A2.T6, Methods ETH3D ScanNet REL 8595 8595 downarrow RMS 8595 8595 downarrow log 10 subscript log 10 mathrm textbf log 10 8595 8595 downarrow REL 8595 8595 downarrow RMS 8595 8595 downarrow log 10 subscript log 10 mathrm textbf log 10 8595 8595 downarrow PR 8475 8475 mathcal R FT 0 147 1 431 0 061 0 145 0 268 0 059 Ours 0 145 1 368 0 060 0 145 0 268 0 059 ,Table 6 Quantitative Comparison on ETH3D and ScanNet We present the scale metrics ,We present additional qualitative results for CityScapes Fig 10 ETH3D Fig 11 and ScanNet Fig 12 We also present scale quantitative comparisons on ETH3D and ScanNet in Tab 6 Here we calculate the result based on all valid GT pixels as described in our main paper We present the results from Ours l 11 lambda 1 on the CityScapes dataset We use l 55 lambda 5 and l 33 lambda 3 to train our model on ETH3D and ScanNet respectively Coincidentally the metrics on ScanNet are exactly the same 
87,S4.T1, Method 948 1 delta 1 8593 8593 uparrow REL 8595 8595 downarrow RMS 8595 8595 downarrow SiLog 8595 8595 downarrow SEE 8595 8595 downarrow Reference iDisc 160 41 96 940 0 053 1 404 8 502 1 070 ICCV 2023 SMD Net 160 56 97 774 0 044 1 282 7 389 0 883 CVPR 2021 Graph GDSR 160 11 97 932 0 044 1 264 7 469 0 872 CVPR 2022 BoostingDepth 160 37 98 104 0 044 1 123 6 662 0 939 CVPR 2021 ZoeDepth 160 2 97 717 0 046 1 289 7 448 0 914 ZoeDepth PF P 16 P 16 textsc P 16 160 30 98 419 0 040 1 088 6 212 0 838 CVPR 2024 ZoeDepth PF P 49 P 49 textsc P 49 160 30 98 450 0 039 1 075 6 131 0 846 ZoeDepth PF R 128 R 128 textsc R 128 160 30 98 469 0 039 1 066 6 085 0 849 ZoeDepth PR P 16 P 16 textsc P 16 98 821 0 033 0 892 5 417 0 750 Ours ZoeDepth PR P 49 P 49 textsc P 49 98 859 0 033 0 870 5 319 0 751 ZoeDepth PR R 128 R 128 textsc R 128 98 864 0 033 0 872 5 377 0 738 Depth Anything 160 61 97 773 0 041 1 235 7 192 0 911 CVPR 2024 Depth Anything PF P 16 P 16 textsc P 16 160 30 98 558 0 036 1 015 5 883 0 811 CVPR 2024 Depth Anything PF P 49 P 49 textsc P 49 160 30 98 607 0 035 0 987 5 746 0 812 Depth Anything PF R 128 R 128 textsc R 128 160 30 98 616 0 035 0 984 5 775 0 813 Depth Anything PR P 16 P 16 textsc P 16 98 826 0 033 0 889 5 289 0 768 Ours Depth Anything PR P 49 P 49 textsc P 49 98 878 0 033 0 860 5 149 0 767 Depth Anything PR R 128 R 128 textsc R 128 98 878 0 033 0 860 5 206 0 759 ,Table 1 Quantitative comparison on UnrealStereo4K We color code the corresponding best competitor and our method within each block PF and PR are short for PatchFusion 30 and PatchRefiner respectively The reported numbers are from 30 ,
88,S4.T2, Method Data Scale Boundary 119982 119982 mathcal S 8475 8475 mathcal R 948 1 delta 1 8593 8593 uparrow REL 8595 8595 downarrow RMS 8595 8595 downarrow Precision 8593 8593 uparrow Recall 8593 8593 uparrow F1 8593 8593 uparrow ZoeDepth 160 2 10003 94 502 0 070 4 406 13 32 37 59 19 26 ZoeDepth 160 2 FT 10003 10003 94 498 0 071 4 418 12 93 37 89 18 89 PatchRefiner zero shot 10003 5 705 0 399 12 203 28 68 51 28 36 34 PatchRefiner 10003 95 284 0 066 4 047 16 67 39 53 23 04 PatchRefiner FT 10003 10003 95 418 0 065 3 992 17 09 40 92 23 68 PatchRefiner mix 160 42 10003 10003 89 108 0 112 4 732 23 08 41 17 29 26 PatchRefiner DSD 10003 10003 95 359 0 066 3 982 18 92 48 78 26 84 ,Table 2 Quantitative comparison on CityScapes FT and MIX are short for fine tuning and mixed data strategies which are our main competitors Baseline is highlighted in gray and each of the three competitors in a different color ,In the field of high resolution depth estimation the prevailing state of the art methodology 30 trains models using synthetic datasets which offers paired high resolution images and corresponding dense high resolution depth ground truth maps 56 20 This synthetic training regime while beneficial in a controlled setting introduces significant challenges when models are applied to real world data due to the intrinsic domain gap between synthetic and real world environments This gap often manifests as substantial scale errors during inference on real domain datasets as shown in Tab 2 Student Model Training with both Pseudo Labels and Ground Truth Depth In the subsequent phase the teacher model is frozen and the student model is trained on the real domain dataset RR mathcal R utilizing both the ground truth depth labels D D tilde mathbf D and pseudo labels D D hat mathbf D generated by the teacher model As identified in the limitations of real domain datasets while the ground truth labels offer accurate depth information they miss crucial information near boundaries essential for high resolution depth learning To address this the student model is guided by the teacher s pseudo labels which excel near boundaries However these pseudo labels while sharp exhibit scale discrepancies due to the domain gap between synthetic and real world data as shown in Tab 2 Real Domain Dataset Tab 2 and Fig 6 7 delineate the performance disparity when leveraging synthetic data for real domain learning Although the synthetic trained model excels in boundary details it fails in scale accuracy due to the domain gap Sole training on real domain data enhances baseline s scale prediction yet falls short in detail accuracy due to the missing depth ground truth around boundaries Neither fine tuning nor mixed training substantially elevates performance across scale and detail metrics reflecting the inherent challenges in our task Contrastingly our strategy propels the model to notable gains in boundary accuracy 19 2 increase in boundary recall while sustaining scale precision comparable to the baseline model 
89,S4.T3, Method Type of output Feature Levels 948 1 delta 1 8593 8593 uparrow REL 8595 8595 downarrow RMS 8595 8595 downarrow SiLog 8595 8595 downarrow SEE 8595 8595 downarrow PatchFusion 160 30 Direct 6 features 98 419 0 040 1 088 6 212 0 838 PatchRefiner 119811 c subscript 119811 119888 mathbf D c residual 1 features 98 734 0 034 0 926 5 550 0 782 119811 c subscript 119811 119888 mathbf D c residual 2 features 98 814 0 033 0 905 5 511 0 750 119811 c subscript 119811 119888 mathbf D c residual 3 features 98 815 0 034 0 900 5 583 0 753 119811 c subscript 119811 119888 mathbf D c residual 4 features 98 815 0 033 0 899 5 494 0 752 119811 c subscript 119811 119888 mathbf D c residual 5 features 98 814 0 033 0 894 5 468 0 752 119811 c subscript 119811 119888 mathbf D c residual 6 features 98 821 0 033 0 892 5 417 0 750 119811 d subscript 119811 119889 mathbf D d residual 6 features 98 804 0 033 0 899 5 448 0 753 Direct 6 features 98 749 0 034 0 925 5 591 0 765 ,Table 3 Ablation study of architecture variations and formulation of final depth prediction Ours is highlighted with color We highlight the best in bold ,We first ablate the effectiveness of our architecture design Utilizing a ZoeDepth model we extract six levels of intermediate features during the forward pass represented as F f1 f2 f6Fsubscriptf1subscriptf2subscriptf6 mathcal F f 1 f 2 cdots f 6 with progressively increasing resolution We then sequentially omit lower resolution feature maps from the refiner s decoder input As depicted in Tab 3 performance diminishes with fewer feature maps yet even with solely the highest resolution feature map our model outperforms PatchFusion 
90,S4.T4, Variations 8466 p 8203 l subscript 8466 119901 119897 mathcal L pl Scale Boundary 948 1 delta 1 8593 8593 uparrow REL 8595 8595 downarrow RMS 8595 8595 downarrow Precision 8593 8593 uparrow Recall 8593 8593 uparrow F1 8593 8593 uparrow Baseline PR 119982 119982 mathcal S 5 705 0 399 12 20 28 68 51 28 36 34 Baseline PR 8475 8475 mathcal R 95 418 0 065 3 992 17 09 40 92 23 68 8466 s 8203 i 8203 l 8203 o 8203 g subscript 8466 119904 119894 119897 119900 119892 mathcal L silog 82 550 0 155 5 914 26 34 58 35 35 81 8466 s 8203 i 8203 l 8203 o 8203 g subscript 8466 119904 119894 119897 119900 119892 mathcal L silog mask 81 425 0 148 6 513 30 04 46 89 36 19 8466 r 8203 a 8203 n 8203 k subscript 8466 119903 119886 119899 119896 mathcal L rank 95 413 0 066 3 973 18 38 47 81 26 12 8466 s 8203 s 8203 i subscript 8466 119904 119904 119894 mathcal L ssi 95 465 0 065 3 974 19 26 44 01 26 39 8466 r 8203 a 8203 n 8203 k subscript 8466 119903 119886 119899 119896 mathcal L rank 8466 s 8203 s 8203 i subscript 8466 119904 119904 119894 mathcal L ssi Ours 95 359 0 066 3 982 18 92 48 78 26 84 955 1 120582 1 lambda 1 95 077 0 069 4 222 21 32 58 48 30 90 955 3 8203 e 120582 3 119890 lambda 3e 1 95 296 0 068 4 086 20 11 53 83 28 91 955 3 8203 e 120582 3 119890 lambda 3e 2 95 462 0 065 3 953 18 01 43 90 25 11 ,Table 4 Variations of Lp lsubscriptLpl mathcal L pl We analyse various options for Lp lsubscriptLpl mathcal L pl and compare them against PR SS mathcal S and PR RR mathcal R that serve as baselines for training on Synthetic and Real data individually We set l l1 l2subscript1subscript2 lambda lambda 1 lambda 2 to analyze the influence of DSD weight The highlighted result is achieved with l 1 e 11superscripte1 lambda 1e 1 ,We evaluate the efficacy of our Detail and Scale Disentangling DSD loss against the conventional scale invariant loss Ls i l o gsubscriptLsilog mathcal L silog for pseudo label supervision Tab 4 illustrates that while Ls i l o gsubscriptLsilog mathcal L silog aids in detail transfer from the teacher to the student model it compromises scale accuracy due to significant discrepancies in the pseudo labels A masking approach focusing only on areas lacking depth does not mitigate this issue indicating pervasive negative effects In contrast the combination of Lr a n ksubscriptLrank mathcal L rank and Ls s isubscriptLssi mathcal L ssi within Lp lsubscriptLpl mathcal L pl not only improves detail fidelity but also maintains scale accuracy demonstrating that ranking constraints and scale shift invariance are effective orthogonal strategies for enhancing high resolution detail without sacrificing scale accuracy This study examines the impact of different DSD loss weights l1subscript1 lambda 1 and l2subscript2 lambda 2 to elucidate the loss s efficiency As shown in Tab 4 and Fig 7 increasing the loss weight marginally affects scale accuracy while significantly improving boundaries validating the DSD loss s role in balancing detail enhancement and scale preservation Notably when both weights are set to 111 the model s boundary recall surpasses the teacher s performance Furthermore when DSD loss achieves a comparable boundary metric to Ls i l o gsubscriptLsilog mathcal L silog it exhibits a smaller performance decline underscoring its effectiveness 
91,A1.EGx1, s 8290 i 8290 m 8290 q g E Q 8290 q T 8290 E C 8290 g 119904 119894 119898 119902 119892 subscript 119864 119876 superscript 119902 119879 subscript 119864 119862 119892 displaystyle sim q g E Q q T E C g italic s italic i italic m italic q italic g italic E start POSTSUBSCRIPT italic Q end POSTSUBSCRIPT italic q start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT italic g ,,
92,A1.EGx2, s 8290 i 8290 m 8290 q g E Q 8290 q T 8290 E C 8290 g 8776 max g 8242 8838 g 8289 E Q 8290 q T 8290 E C 8290 g 8242 119904 119894 119898 119902 119892 subscript 119864 119876 superscript 119902 119879 subscript 119864 119862 119892 subscript superscript 119892 8242 119892 subscript 119864 119876 superscript 119902 119879 subscript 119864 119862 superscript 119892 8242 displaystyle sim q g E Q q T E C g approx max g prime subseteq g E Q q T E C g prime italic s italic i italic m italic q italic g italic E start POSTSUBSCRIPT italic Q end POSTSUBSCRIPT italic q start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT italic g 8776 roman max start POSTSUBSCRIPT italic g start POSTSUPERSCRIPT 8242 end POSTSUPERSCRIPT 8838 italic g end POSTSUBSCRIPT italic E start POSTSUBSCRIPT italic Q end POSTSUBSCRIPT italic q start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT italic g start POSTSUPERSCRIPT 8242 end POSTSUPERSCRIPT ,,
93,S3.T1.1.1, Dataset Corpus source Avg Doc Length of Documents of Text cases Metric NQ Wikipedia 800 3M 3 610 EM HotpotQA Wikipedia 130 5 2M 7 405 EM Qasper Science 4 7K 416 371 F1 MultiFieldQA en Multi field 6 9K 150 150 F1 ,Table 1 An overview of the four datasets used in our experiments is provided Corpus source refers to the origin of the retrieval corpus We selected NQ and HotpotQA from Wikipedia Qasper from scientific documents and MultifieldQA en from multi field documents The two Wikipedia based datasets utilize a massive retrieval corpus containing millions of short documents In contrast the other two datasets employ a smaller corpus consisting of hundreds of long documents ,Our proposed methods were tested on four question answering datasets The basic statistics are shown in Table 1 Additionally we have provided some examples in Appendix A 4 
94,S3.T1.1.1.1.1.3.1, Avg Doc Length ,Table 1 An overview of the four datasets used in our experiments is provided Corpus source refers to the origin of the retrieval corpus We selected NQ and HotpotQA from Wikipedia Qasper from scientific documents and MultifieldQA en from multi field documents The two Wikipedia based datasets utilize a massive retrieval corpus containing millions of short documents In contrast the other two datasets employ a smaller corpus consisting of hundreds of long documents ,Our proposed methods were tested on four question answering datasets The basic statistics are shown in Table 1 Additionally we have provided some examples in Appendix A 4 
95,S3.T2.1.1, Retrieval Unit Corpus Size Num of Retrieval Units Average Num of Tokens Answer Recall AR Corpus Test Set Passage 22M 1 120 130 52 24 100 12K 14K 89 92 200 24K 28K 91 30 Document 3M 1 820 4K 69 45 5 4K 18K 85 37 10 8K 34K 88 12 Grouped Documents 600K 1 4K 6K 71 69 4 16K 25K 86 30 8 32K 50K 88 53 ,Table 2 The table illustrates the retrieval performance on NQ Employing a long context retriever with an average number of tokens for each retrieval unit up to 6K compresses the corpus size by up to 30 times from 22M to 600K enhancing top 1 answer recall by approximately 20 points from 52 24 to 71 69 Furthermore long context retriever requires significantly fewer retrieval units 10x fewer to achieve comparable results Therefore integrating long context retrieval significantly alleviates the burden of retriever ,Table 2 and Table 3 have shown the retrieval results on NQ and HotpotQA In the NQ dataset we utilize three different retrieval units ranging from shorter to longer passage document and grouped documents In the table we have mentioned two kinds of average number of tokens in each retrieval unit one for the entire corpus and one for each test set The retrieval units for each test case can sometimes be much longer than the average size across the whole corpus as the corpus might include some Wikipedia pages with very few words while the test cases may focus more on longer documents Generally our long context retriever at the document level and grouped document level uses retrieval units containing an average of 6K tokens By using longer retrieval units there are several advantages 1 It will significantly alleviate the burden on the retriever by compressing the corpus size by approximately 30 times from 22M to 600K The top 1 answer recall improves by about 20 points from 52 24 to 71 69 We could use significantly fewer retrieval units to achieve comparable retrieval performance For instance 8 retrieval units at the grouped document level can achieve similar recall as 100 retrieval units at the passage level 2 It could provide more comprehensive information to the reader In the original passage level RAG setup information might be incomplete due to the chunking operation In the HotpotQA dataset we observe similar results One notable difference is that in HotpotQA the retrieval units are only at the document level and grouped document level as HotpotQA uses only abstract paragraphs from each Wikipedia page 
96,S3.T3.1.1, Retrieval Unit Corpus Size Num of Retrieval Units Average Num of Tokens Recall R Answer Recall AR Corpus Test Set Document 5 2M 2 130 200 30 01 47 75 100 6 5K 10K 74 84 84 67 200 13K 20K 79 68 88 34 Grouped Documents 500K 2 1K 8K 56 30 72 49 8 4K 29K 74 71 84 40 ,Table 3 The table illustrates the retrieval performance on HotpotQA Similar to the findings on NQ a long context retrieval could significantly alleviate the burden on the retriever component ,Table 2 and Table 3 have shown the retrieval results on NQ and HotpotQA In the NQ dataset we utilize three different retrieval units ranging from shorter to longer passage document and grouped documents In the table we have mentioned two kinds of average number of tokens in each retrieval unit one for the entire corpus and one for each test set The retrieval units for each test case can sometimes be much longer than the average size across the whole corpus as the corpus might include some Wikipedia pages with very few words while the test cases may focus more on longer documents Generally our long context retriever at the document level and grouped document level uses retrieval units containing an average of 6K tokens By using longer retrieval units there are several advantages 1 It will significantly alleviate the burden on the retriever by compressing the corpus size by approximately 30 times from 22M to 600K The top 1 answer recall improves by about 20 points from 52 24 to 71 69 We could use significantly fewer retrieval units to achieve comparable retrieval performance For instance 8 retrieval units at the grouped document level can achieve similar recall as 100 retrieval units at the passage level 2 It could provide more comprehensive information to the reader In the original passage level RAG setup information might be incomplete due to the chunking operation In the HotpotQA dataset we observe similar results One notable difference is that in HotpotQA the retrieval units are only at the document level and grouped document level as HotpotQA uses only abstract paragraphs from each Wikipedia page 
97,S3.T4.1, Model Granularity AR 1 BGE Large 512 tokens chunk 71 7 E5 Mistral 7B 4000 tokens chunk 54 2 E5 Mistral 7B entire grouped retrieval unit 23 4 ,Table 4 Different methods to encode the long retrieval unit in the long retriever Using a general embedding model and approximating by maximizing the similarity scores between the query and all chunks within the retrieval unit is better than using the existing long embedding model to encode the entire context ,We approximate it by maximizing the scores of all chunks g superscriptg g prime italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT within the retrieval unit gggitalic g akin to the MaxP design in Dai Callan 2019 We consider different levels of granularity of chunk g superscriptg g prime italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT including 512 tokens 4K tokens and encoding the entire gggitalic g completely The empirical study about this settings is in Table 4 With this similarity score setup we will retrieve the top kkkitalic k retrieval units closest to the given query For efficient retrieval we precompute the embedding of each retrieval unit g superscriptg g prime italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT and predict the exact inner product search index in FAISS Johnson et al 2019 As discussed in Section 2 2 it s very challenging to employ an encoder EC subscriptECE C cdot italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT to map the retrieval unit gggitalic g to a ddditalic d dimensional vector when gggitalic g is very long Therefore we use an approximation in our proposed system Table 4 demonstrates that our approximation sim q g EQ q TEC g maxg g EQ q TEC g simqgsubscriptEQsuperscriptqTsubscriptECgsubscriptsuperscriptg gsubscriptEQsuperscriptqTsubscriptECsuperscriptg sim q g E Q q T E C g approx max g prime subseteq g E Q q T E C g prime italic s italic i italic m italic q italic g italic E start POSTSUBSCRIPT italic Q end POSTSUBSCRIPT italic q start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT italic g roman max start POSTSUBSCRIPT italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic g end POSTSUBSCRIPT italic E start POSTSUBSCRIPT italic Q end POSTSUBSCRIPT italic q start POSTSUPERSCRIPT italic T end POSTSUPERSCRIPT italic E start POSTSUBSCRIPT italic C end POSTSUBSCRIPT italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT is much more effective than encoding the entire long context directly We compare three methods 1 Using the general embedding model bge large en v1 5 Xiao et al 2023 with g superscriptg g prime italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT selected as text of 512 token size 2 Using long embedding model E5 Mistral 7B Zhu et al 2024a with g superscriptg g prime italic g start POSTSUPERSCRIPT end POSTSUPERSCRIPT selected as the whole document which has an average size of 4K tokens 3 Using long embeddings model E5 Mistral 7B with no approximation we encode the entire gggitalic g which is composed of multiple documents directly The average size of gggitalic g is 6K tokens We can notice from the table that our approximation by taking the maximum score between the query and each text piece from the long context produces much better results than encoding them directly using the long embedding model We believe that future advancements in long embedding models which focus on encoding long contexts or multiple documents will further enhance our framework and reduce memory consumption 
98,S3.T5.1, NQ EM Closed Book GPT 4 Turbo Achiam et 160 al 2023 41 2 Gemini 1 5 Pro Reid et 160 al 2024 47 8 Claude 3 Opus Anthropic 2024 49 2 Fully supervised RAG REALM Guu et 160 al 2020 40 4 DPR Karpukhin et 160 al 2020 41 5 RAG Lewis et 160 al 2020 44 5 RETRO Borgeaud et 160 al 2022 45 5 RePAQ Lewis et 160 al 2021 47 8 FID Izacard amp Grave 2020b 51 4 EMDR 2 Singh et 160 al 2021 52 5 FID KD Izacard amp Grave 2021 54 7 R2 D2 Fajcik et 160 al 2021 55 9 Atlas Izacard et 160 al 2022 64 0 No Fine tuning RAG REPLUG Shi et 160 al 2023 44 7 REPLUG LSR Shi et 160 al 2023 45 5 LongRAG Gemini 1 5 Pro Recall 4 units 58 6 LongRAG GPT 4o Recall 4 units 62 7 ,Table 5 The tables show the QA results on the NQ test dataset left and Hotpot QA dev set right We compare the results with three groups of baselines closed book which involves directly prompting state of the art LLMs with 16 shot in context examples fully supervised RAG where the RAG framework is used and the model is fully supervised and trained on the training data and No Fine tuning RAG which employs the RAG framework without any tuning ,For NQ and HotpotQA we compare our model with several groups of strong previous models as baselines The first group is Closed Book These baselines mean that no retrieval component is used instead state of the art LLMs are employed to directly obtain the final result We evaluate our results on Gemini 1 5 pro Reid et al 2024 Claude 3 Opus Anthropic 2024 and GPT 4 Turbo Achiam et al 2023 All models are evaluated on 16 shot in context learning with direct prompting The second group is Fully supervised RAG and these baselines involve full supervised fine tuning on the training dataset The third group is No Fine tuning RAG and these baselines doesn t involve any supervised fine tuning The QA results on NQ and HotpotQA are presented in Table 5 On the NQ dataset LongRAG achieves a 62 7 exact match rate which is on par of the strongest fine tuned RAG model like Atlas On the HotpotQA dataset LongRAG achieves a 64 3 exact match rate which is also close to the SoTA fully supervised RAG frameworks 
99,S3.T5.2, HotpotQA EM Closed Book Claude 3 Opus Anthropic 2024 32 8 Gemini 1 5 Pro Reid et 160 al 2024 33 9 GPT 4 Turbo Achiam et 160 al 2023 42 4 Fully supervised RAG DrKIT Dhingra et 160 al 2020 42 1 Transformer XH 160 Zhao et 160 al 2019 51 6 QAMAT Chen et 160 al 2023b 57 6 HGN Fang et 160 al 2019 59 7 PathRetriever Asai et 160 al 2019 60 0 HopRetrieve Li et 160 al 2021 62 1 MDR Xiong et 160 al 2020b 62 3 HopRetrieve plus Li et 160 al 2021 66 5 AISO Zhu et 160 al 2021 68 1 COS Ma et 160 al 2023 68 2 No Fine tuning RAG DSP Khattab et 160 al 2022 51 4 PromptRank Khalifa et 160 al 2023 55 7 LongRAG Gemini 1 5 Pro Recall 8 units 57 5 LongRAG GPT 4o Recall 8 units 64 3 ,Table 5 The tables show the QA results on the NQ test dataset left and Hotpot QA dev set right We compare the results with three groups of baselines closed book which involves directly prompting state of the art LLMs with 16 shot in context examples fully supervised RAG where the RAG framework is used and the model is fully supervised and trained on the training data and No Fine tuning RAG which employs the RAG framework without any tuning ,For NQ and HotpotQA we compare our model with several groups of strong previous models as baselines The first group is Closed Book These baselines mean that no retrieval component is used instead state of the art LLMs are employed to directly obtain the final result We evaluate our results on Gemini 1 5 pro Reid et al 2024 Claude 3 Opus Anthropic 2024 and GPT 4 Turbo Achiam et al 2023 All models are evaluated on 16 shot in context learning with direct prompting The second group is Fully supervised RAG and these baselines involve full supervised fine tuning on the training dataset The third group is No Fine tuning RAG and these baselines doesn t involve any supervised fine tuning The QA results on NQ and HotpotQA are presented in Table 5 On the NQ dataset LongRAG achieves a 62 7 exact match rate which is on par of the strongest fine tuned RAG model like Atlas On the HotpotQA dataset LongRAG achieves a 64 3 exact match rate which is also close to the SoTA fully supervised RAG frameworks 
100,S3.T6.1.1, Retrieval Unit Num of Retrieval Units Qasper MutilfieldQA en Passage 1 15 5 38 9 10 20 6 47 3 100 22 6 51 3 200 21 9 50 9 Document 1 26 3 49 4 2 25 9 50 2 5 23 9 57 5 10 21 6 56 8 ,Table 6 This table presents the QA results on two non Wiki datasets Qasper and MultifieldQA en The results are evaluated based on token level F1 Both datasets contain long documents averaging at least 4K tokens The results demonstrate that our LongRAG which operates on long retrieval units achieves better performance compared to traditional RAG which operates on short retrieval units ,For datasets that generate long answers such as Qasper and MultifieldQA en we use the token level F1 score F1 as the evaluation metric For Qasper and MultifieldQA en since we repurpose the datasets from single document QA to a RAG task we do not directly compare the results with previous models Instead we compare the performance of traditional RAG which operates on 200 token passages with our LongRAG which operates on entire documents ranging from 4K to 6K tokens The results are shown in Table 6 We observe that using long retrieval units at the whole document level performs better than using hundreds of short chunked retrieval units On the Qasper dataset gathering 100 short retrieval units of 200 tokens each into the reader achieves a 22 6 F1 score while using a single long retrieval unit of 5K tokens achieves a 26 3 F1 score Similarly on the MultifieldQA en dataset gathering 100 short retrieval units of 200 tokens each into the reader results in a 51 3 F1 score whereas using five long retrieval units of 7K tokens each results in a 57 5 F1 score 
101,A1.T7.1, Method Prompt Closed Book Here are some examples of questions and their corresponding answer each with a 8220 Question 8221 field and an 8220 Answer 8221 field Answer the question directly and don 8217 t output other thing 8220 Question 8221 8230 8220 Answer 8221 8230 8220 Question 8221 8230 8220 Answer 8221 8230 8230 8220 Question 8221 8230 8220 Answer 8221 8230 Answer the following question 8220 Question 8221 who is the owner of reading football club 8220 Answer 8221 LongRAG Turn 1 Go through the following context and then answer the question The context is a list of Wikipedia documents ordered by title 8230 Each Wikipedia document contains a title field and a text field The context is 8220 Title 8221 8230 8220 Text 8221 8230 8220 Title 8221 8230 8220 Text 8221 8230 8230 8220 Title 8221 8230 8220 Text 8221 8230 Find the useful documents from the context then answer the question when did the philadelphia eagles play in the super bowl last Answer the question directly Your response should be very concise Turn 2 You have been provided with a question and its long answer Your task is to derive a very concise short answer from the given long answer It 8217 s important to ensure that the output short answer remains as simple as possible Here a few examples 8220 Question 8221 8230 8220 Long Answer 8221 8230 8220 Short Answer 8221 8230 8220 Question 8221 8230 8220 Long Answer 8221 8230 8220 Short Answer 8221 8230 8230 8220 Question 8221 8230 8220 Long Answer 8221 8230 8220 Short Answer 8221 8230 Extract the short answer of the following question and long answer 8220 Question 8221 when did the philadelphia eagles play in the super bowl last 8220 Long Answer 8221 The Philadelphia Eagles last played in the Super Bowl on February 4 2018 in Super Bowl LII 8220 Short Answer 8221 ,Table 7 Here are the prompts we used for all the experiments For the closed book method we use 16 shot in context examples For LongRAG we use a two turn approach to extract the final answer The first turn doesn t require any in context examples and generate a longer answer typically ranging from a few words to a few sentences In the second turn we use 8 shot in context examples to calibrate and extract the exact short answer which is typically just a few words ,We leverage Gemini 1 5 Pro and GPT 4o as the reader in our LongRAG framework The prompt we use for our experiments are in Table 7 For Wiki based datasets such as NQ and HotpotQA which generate short answers typically less than 5 tokens we use EM Exact Match rate as the evaluation metric We also refine the standard exact match rate definition to more fairly evaluate LongRAG s performance More details can be found in Section A 2 We have put out prompts used for the experiments in Table 7 For the closed book method we use 16 shot in context examples For LongRAG we use a two turn approach to extract the final answer In the first turn the long retrieved context and the question are concatenated as input and we do not use any in context examples here due to the context being around 30K tokens Empirically we found it beneficial to let the reader generate a longer answer initially typically ranging from a few words to a few sentences In the second turn we use 8 shot in context examples to guide the reader in further extracting the most important part of the long answer as the short answer which is typically just a few words 
102,A1.T8.1.1, Question Ground truth LongRAG prediction where does the bob and tom show broadcast from Indianapolis Indiana Indianapolis who has given the theory of unbalanced economic growth Hirschman Albert O Hirschman when does season 6 of the next step start 2018 September 29 2018 what was the precursor to the present day internet the ARPANET project ARPANET ,Table 8 Some examples demonstrate that LongRAG has extracted aliases or different forms of the ground truth ,The most standard metric used in open domain extractive question answering tasks is EM Exact Match since the correct answer must be a substring within the corpus In our framework since the long retrieved context which contains multiple highly related documents to the given query is fed into the reader there is a much higher possibility that an alias of the ground truth exists in the context and can be extracted by the reader As shown in Table 8 although LongRAG s prediction doesn t exactly match the ground truth it s obvious that LongRAG s prediction is correct To better and more fairly evaluate LongRAG s performance we have refined the EM metric slightly We recognize it as an exact match if the prediction is less than five tokens indicating that the short answer is successfully extracted as described in Section A 1 and the ground truth is a substring of the prediction or vice versa We have also manually verified that this refined metric indeed captures aliases or other forms of the ground truth For the fully supervised RAG baselines used in our paper given that they are fine tuned on the training data and the retrieval unit is a small snippet we believe that the difference won t be significant when using the refined EM 
103,A1.T9.1, Method Prompt NQ Question how many episodes are in series 7 game of thrones Answer seven HotpotQA Question What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell Answer Chief of Protocol Qasper Question In the paper 8217 End to End Trainable Non Collaborative Dialog System 8217 How is intent annotated Answer using a role playing task on the Amazon Mechanical Turk platform and collecting typed conversations MultifieldQA en Question What is the name of the most active fan club Answer South West Ultras fan club ,Table 9 Here are some examples from the four datasets used in our experiments ,
104,S2.T1.3, English Arabic Spanish train Task 1 emotion 7102 3376 4544 dev Task 1 emotion 1464 661 793 test Task 1 emotion 71816 1563 2616 train Task 1 valence 1181 932 1566 dev Task 1 valence 449 138 229 test Task 1 valence 17874 730 648 train Task 10 17521 dev Task 10 2722 test Task 10 2340 Task 1 emotion English Arabic Spanish train anger 1701 877 1166 train fear 2252 882 1166 train joy 1616 728 1058 train sadness 1533 889 1154 dev anger 388 150 193 dev fear 389 146 202 dev joy 290 224 202 dev sadness 397 141 196 test anger 17939 373 627 test fear 17923 372 618 test joy 18042 448 730 test sadness 17912 370 641 ,Number of instances in the provided data About of the Task 1 emotion English test sets are not evaluated for the competition rankings and 1002 986 1105 and 975 instances were actually scored ,SemEval 2018 Apidianaki et al 2018 contained two prediction tasks that referential translation machine RTM models Bicici 2017a b Bicici and Way 2015 can be applied to enable new modeling capabilities and provide new results for comparison The trace of the shared information with tweets or social media to the audiences world can be approached by the intensity of the content and structure used in text with Task 1 Mohammad et al 2018 The effectiveness of attributes to semantically separate two words from each other is the goal in Task 10 Paperno et al 2018 which can be used to improve natural language understanding systems Table 1 lists the number of instances in the provided datasets We use RTMs to model both tasks 
105,S2.T1.4, English Arabic Spanish train Task 1 emotion 7102 3376 4544 dev Task 1 emotion 1464 661 793 test Task 1 emotion 71816 1563 2616 train Task 1 valence 1181 932 1566 dev Task 1 valence 449 138 229 test Task 1 valence 17874 730 648 train Task 10 17521 dev Task 10 2722 test Task 10 2340 Task 1 emotion English Arabic Spanish train anger 1701 877 1166 train fear 2252 882 1166 train joy 1616 728 1058 train sadness 1533 889 1154 dev anger 388 150 193 dev fear 389 146 202 dev joy 290 224 202 dev sadness 397 141 196 test anger 17939 373 627 test fear 17923 372 618 test joy 18042 448 730 test sadness 17912 370 641 ,Number of instances in the provided data About of the Task 1 emotion English test sets are not evaluated for the competition rankings and 1002 986 1105 and 975 instances were actually scored ,SemEval 2018 Apidianaki et al 2018 contained two prediction tasks that referential translation machine RTM models Bicici 2017a b Bicici and Way 2015 can be applied to enable new modeling capabilities and provide new results for comparison The trace of the shared information with tweets or social media to the audiences world can be approached by the intensity of the content and structure used in text with Task 1 Mohammad et al 2018 The effectiveness of attributes to semantically separate two words from each other is the goal in Task 10 Paperno et al 2018 which can be used to improve natural language understanding systems Table 1 lists the number of instances in the provided datasets We use RTMs to model both tasks 
106,A1.EGx1,,,
107,S3.T2.2.2, Task r 119903 r italic r MAE RAE MAER MRAER English Task 1 emotion anger 0 104 0 1744 1 121 0 3849 1 106 English Task 1 emotion fear 0 063 0 169 1 156 0 3869 1 155 English Task 1 emotion joy 0 266 0 1859 1 193 0 3849 1 293 English Task 1 emotion sadness 0 233 0 1673 1 114 0 3543 1 165 English Task 1 emotion ALL 0 168 0 1745 1 147 0 3781 1 182 Arabic Task 1 emotion anger 0 196 0 1407 0 988 0 3112 0 886 Arabic Task 1 emotion fear 0 125 0 1445 1 01 0 3152 0 904 Arabic Task 1 emotion joy 0 316 0 1431 0 949 0 4076 0 826 Arabic Task 1 emotion sadness 0 238 0 1452 0 99 0 3238 0 89 Arabic Task 1 emotion ALL 0 229 0 1434 0 981 0 3428 0 871 Spanish Task 1 emotion anger 0 211 0 171 1 007 0 3796 0 957 Spanish Task 1 emotion fear 0 35 0 1612 0 923 0 4543 0 837 Spanish Task 1 emotion joy 0 389 0 1596 0 9 0 471 0 838 Spanish Task 1 emotion sadness 0 415 0 1491 0 896 0 3968 0 833 Spanish Task 1 emotion ALL 0 34 0 1602 0 928 0 4273 0 858 ALL Task 1 emotion ALL 0 216 0 1641 1 041 0 3878 1 021 English Task 1 valence 0 161 0 2642 1 458 0 5155 1 674 Arabic Task 1 valence 0 264 0 1921 0 958 0 5595 0 919 Spanish Task 1 valence 0 272 0 1665 0 962 0 5302 0 85 ALL Task 1 valence 0 117 0 2141 1 154 0 5333 1 185 English Task 10 combined 0 045 0 5034 1 018 1 0954 0 994 F 1 subscript 119865 1 F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT English Task 10 0 47 ,Competition results on the test set ,RTM results in the Task 1 666https competitions codalab org competitions 17751 and Task 10 777https competitions codalab org competitions 17326 competitions are in Table 2 and ranks are in Table 3 Mohammad et al 2018 8 of the results obtain MRAER larger than 1 suggesting more work towards these tasks or subtasks weight models combine top models predictions Bicici 2017b The predictions for Task 10 were transformed to binary classes by thresholding with 0 50 50 50 5 and obtains 0 470 470 470 47 F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 
108,S3.T3.1, T1 EI en T1 EI ar T1 EI es T1 V en T1 V ar T1 V es T10 ranks 44 13 10 35 13 10 9 out of 50 15 17 39 15 15 9 ,RTM ranks at SemEval 2018 ,RTM results in the Task 1 666https competitions codalab org competitions 17751 and Task 10 777https competitions codalab org competitions 17326 competitions are in Table 2 and ranks are in Table 3 Mohammad et al 2018 8 of the results obtain MRAER larger than 1 suggesting more work towards these tasks or subtasks weight models combine top models predictions Bicici 2017b The predictions for Task 10 were transformed to binary classes by thresholding with 0 50 50 50 5 and obtains 0 470 470 470 47 F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 
109,S3.T4.1.1, Task r 119903 r italic r MAE RAE MAER MRAER English Task 1 emotion anger 0 245 0 1689 1 086 0 3619 1 074 English Task 1 emotion fear 0 05 0 1526 1 044 0 413 0 952 English Task 1 emotion joy 0 028 0 1641 1 053 0 438 0 963 English Task 1 emotion sadness 0 004 0 1566 1 043 0 4064 0 944 English Task 1 emotion ALL 0 2245 0 1734 1 14 0 367 1 1693 Arabic Task 1 emotion anger 0 209 0 1413 0 992 0 3093 0 878 Arabic Task 1 emotion fear 0 173 0 1444 1 01 0 3112 0 905 Arabic Task 1 emotion joy 0 377 0 1417 0 94 0 4126 0 805 Arabic Task 1 emotion sadness 0 269 0 1442 0 983 0 3291 0 872 Arabic Task 1 emotion ALL 0 2543 0 1428 0 9771 0 344 0 8545 Spanish Task 1 emotion anger 0 183 0 1706 1 005 0 3807 0 927 Spanish Task 1 emotion fear 0 398 0 1607 0 92 0 4548 0 846 Spanish Task 1 emotion joy 0 298 0 1676 0 945 0 4856 0 843 Spanish Task 1 emotion sadness 0 405 0 1513 0 909 0 3943 0 838 Spanish Task 1 emotion ALL 0 324 0 1627 0 9426 0 4311 0 8568 English Task 1 valence 0 1326 0 1884 1 0399 0 525 0 9791 Arabic Task 1 valence 0 2981 0 1879 0 9366 0 5637 0 8482 Spanish Task 1 valence 0 2152 0 1684 0 973 0 5399 0 8317 ,RTM results on the test set after the competition ,The stacked RTM model with separate prediction steps use separated feature sets with an initial prediction with each Figure 3 Using separate predictors for each word need not achieve better performance than using a single predictor since the rows of words we compare need not belong to different groups due to their ordering in the dataset and the words may be only randomly positioned However both predictions can be useful as coming from two weaker predictors since half of the training instances were used to train each The benefit of using separate feature sets is further specialization of w1 a subscriptw1aw 1 rightarrow aitalic w start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic a and w2 a subscriptw2aw 2 rightarrow aitalic w start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic a models with feature selection and partial least squares from Section 4 As we see in Table 4 separated feature sets with two separate learning and prediction steps improve the performance We recalculated our results using 200 thousand training instances for Task 1 which also helps comparison with previous results Bicici and Way 2015 and an advanced IBM2 alignment model after the challenge Table 4 lists the new results obtained after the challenge The number of results with MRAER larger than 1 decreased to 5 The MRAER obtained by RTMs in STS in 2016 is 0 730 730 730 73 Bicici 2016b and in quality estimation task for English to German in 2017 is 0 80 80 80 8 Bicici 2017a Translation similarity distances alone can be useful to identify the discriminative power of attributes Table 9 The predictions for Task 10 were transformed to binary classes by thresholding with optimized thresholds on the training set Prediction differences are used from the combined model or the separate model Table 10 show that valence can be better predicted with MTPP towards the translation of the vocabulary of sadness rather than using the union of contrastive vocabulary sets of joy and sadness Separate learning also improve the performance for English and Spanish valence prediction Translation similarity distances are also used for Task 1 in Table 10 where instead of learning a threshold to optimize for F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score we fit a linear model ax baxbax bitalic a italic x italic b to the distance between predictions 
110,S3.T5.1.1, Task r 119903 r italic r MAE RAE MAER MRAER English Task 1 emotion anger 0 245 0 1689 1 086 0 3619 1 074 English Task 1 emotion fear 0 05 0 1526 1 044 0 413 0 952 English Task 1 emotion joy 0 028 0 1641 1 053 0 438 0 963 English Task 1 emotion sadness 0 004 0 1566 1 043 0 4064 0 944 English Task 1 emotion ALL 0 2245 0 1734 1 14 0 367 1 1693 Arabic Task 1 emotion anger 0 209 0 1413 0 992 0 3093 0 878 Arabic Task 1 emotion fear 0 177 0 1443 1 009 0 311 0 905 Arabic Task 1 emotion joy 0 377 0 1416 0 939 0 4123 0 805 Arabic Task 1 emotion sadness 0 271 0 144 0 982 0 329 0 871 Arabic Task 1 emotion ALL 0 2562 0 1428 0 9765 0 3439 0 8541 Spanish Task 1 emotion anger 0 183 0 1706 1 004 0 3807 0 926 Spanish Task 1 emotion fear 0 398 0 1607 0 921 0 4551 0 846 Spanish Task 1 emotion joy 0 297 0 1677 0 946 0 4857 0 842 Spanish Task 1 emotion sadness 0 405 0 1514 0 91 0 3944 0 837 Spanish Task 1 emotion ALL 0 3235 0 1627 0 9427 0 4313 0 8565 English Task 1 valence 0 1332 0 1888 1 042 0 5254 0 9842 Arabic Task 1 valence 0 3023 0 1874 0 9344 0 5628 0 8471 Spanish Task 1 valence 0 2149 0 1684 0 973 0 5399 0 8321 ,RTM results on the test set after the competition with symbolic grounding and scaling of predictions ,
111,S3.T6.1.1, Task r 119903 r italic r MAE RAE MAER MRAER rMAER rMRAER without MIX English Task 1 emotion ALL 0 2106 0 1755 1 1536 0 3687 1 1936 0 0795 0 4261 Arabic Task 1 emotion ALL 0 256 0 1428 0 9771 0 3442 0 852 0 0515 0 2447 Spanish Task 1 emotion ALL 0 3125 0 163 0 9445 0 432 0 8605 0 0564 0 2315 English Task 1 valence 0 1203 0 1841 1 0164 0 5569 0 9296 0 0615 0 2367 Arabic Task 1 valence 0 225 0 1927 0 9608 0 5702 0 8984 0 0615 0 1952 Spanish Task 1 valence 0 2216 0 1683 0 9724 0 5416 0 8357 0 0533 0 187 with MIX English Task 1 emotion ALL 0 2351 0 1693 1 1129 0 3648 1 1204 0 0722 0 3857 Arabic Task 1 emotion ALL 0 2543 0 1428 0 9771 0 344 0 8545 0 051 0 2482 Spanish Task 1 emotion ALL 0 324 0 1627 0 9426 0 4311 0 8568 0 0548 0 2298 English Task 1 valence 0 1326 0 1884 1 0399 0 525 0 9791 0 0674 0 2808 Arabic Task 1 valence 0 2981 0 1879 0 9366 0 5637 0 8482 0 0604 0 198 Spanish Task 1 valence 0 2169 0 1683 0 9726 0 5396 0 8313 0 052 0 19 with MIX symbolic grounding English Task 1 emotion ALL 0 2105 0 1738 1 1423 0 3675 1 1705 0 0768 0 4116 Arabic Task 1 emotion ALL 0 2546 0 1429 0 9772 0 3441 0 8542 0 051 0 248 Spanish Task 1 emotion ALL 0 324 0 1627 0 9427 0 4313 0 8563 0 0547 0 2293 English Task 1 valence 0 129 0 1893 1 045 0 5274 0 9891 0 0688 0 2826 Arabic Task 1 valence 0 3023 0 1875 0 9345 0 5632 0 847 0 0619 0 2002 Spanish Task 1 valence 0 2171 0 1683 0 9726 0 5397 0 831 0 0521 0 1901 with new MIX symbolic grounding English Task 1 emotion ALL 0 2355 0 1692 1 112 0 3648 1 1187 0 0719 0 3848 Arabic Task 1 emotion ALL 0 255 0 1428 0 977 0 3441 0 8528 0 0526 0 247 Spanish Task 1 emotion ALL 0 3236 0 1627 0 9427 0 4311 0 8567 0 0543 0 2296 English Task 1 valence 0 1324 0 1881 1 0381 0 5251 0 9751 0 0665 0 2783 Arabic Task 1 valence 0 2983 0 1879 0 9366 0 5637 0 848 0 0602 0 1978 Spanish Task 1 valence 0 2175 0 1683 0 9725 0 5396 0 8308 0 0519 0 1899 with fold MIX symbolic grounding English Task 1 emotion ALL 0 2355 0 1692 1 1119 0 3647 1 1185 0 0717 0 3844 Arabic Task 1 emotion ALL 0 2555 0 1428 0 9771 0 344 0 8533 0 0526 0 249 Spanish Task 1 emotion ALL 0 324 0 1627 0 9426 0 4311 0 8568 0 055 0 2299 English Task 1 valence 0 1321 0 1881 1 0381 0 5249 0 9752 0 0676 0 279 Arabic Task 1 valence 0 2997 0 1877 0 9358 0 5638 0 8475 0 0597 0 1972 Spanish Task 1 valence 0 2169 0 1683 0 9724 0 5397 0 831 0 0521 0 1898 ,RTM results on the test set without symbolic grounding and scaling of predictions ,
112,S4.T7.11, 1 1 1 1 amp 2 2 2 2 gram wrec 1 1 1 1 amp 2 2 2 2 amp 3 3 3 3 gram wGM 1 1 1 1 amp 2 2 2 2 gram wGM 1 1 1 1 amp 2 2 2 2 gram w F 1 subscript 119865 1 F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT 1 1 1 1 gram wGM ,Top features selected for Task 10,The top 5555 features selected for Task 10 are listed in Table 7 1111 2222gram wF1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT is F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score over 1111 gram and 2222 gram features with recall computed according to the sum of the likelihood of observing them among 1111 grams or 2222 grams correspondingly wrec and precision computed according to all corresponding counts in nnnitalic n grams wGM is weighted geometric mean of the arguments of F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The features enable linking w1subscriptw1w 1 italic w start POSTSUBSCRIPT 1 end POSTSUBSCRIPT and aaaitalic a and w2subscriptw2w 2 italic w start POSTSUBSCRIPT 2 end POSTSUBSCRIPT and aaaitalic a and 8888 of the top 10101010 features use nnnitalic n gram features which makes sense for linking words and we observe this even semantically for Task 10 The top 5555 features selected for Task 1 are listed in Table 8 bpw is bits per word WER is word error rate 
113,S4.T8.3, translation logprobability bpw word alignment 1 WER word alignment F 1 subscript 119865 1 F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score 3 3 3 3 gram w F 1 subscript 119865 1 F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT sentence number of characters ,Top features selected for Task 1 Spanish,The top 5555 features selected for Task 10 are listed in Table 7 1111 2222gram wF1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT is F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score over 1111 gram and 2222 gram features with recall computed according to the sum of the likelihood of observing them among 1111 grams or 2222 grams correspondingly wrec and precision computed according to all corresponding counts in nnnitalic n grams wGM is weighted geometric mean of the arguments of F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT The features enable linking w1subscriptw1w 1 italic w start POSTSUBSCRIPT 1 end POSTSUBSCRIPT and aaaitalic a and w2subscriptw2w 2 italic w start POSTSUBSCRIPT 2 end POSTSUBSCRIPT and aaaitalic a and 8888 of the top 10101010 features use nnnitalic n gram features which makes sense for linking words and we observe this even semantically for Task 10 The top 5555 features selected for Task 1 are listed in Table 8 bpw is bits per word WER is word error rate 
114,S4.T9.2.2, Setting F 1 subscript 119865 1 F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT r 119903 r italic r MAE RAE MAER MRAER combined 0 5169 0 035 0 4735 0 958 1 1397 0 95 separate 0 5006 0 014 0 4761 0 963 1 0317 0 939 ,Task 10 test set results with translation similarity distance modeling ,We recalculated our results using 200 thousand training instances for Task 1 which also helps comparison with previous results Bicici and Way 2015 and an advanced IBM2 alignment model after the challenge Table 4 lists the new results obtained after the challenge The number of results with MRAER larger than 1 decreased to 5 The MRAER obtained by RTMs in STS in 2016 is 0 730 730 730 73 Bicici 2016b and in quality estimation task for English to German in 2017 is 0 80 80 80 8 Bicici 2017a Translation similarity distances alone can be useful to identify the discriminative power of attributes Table 9 The predictions for Task 10 were transformed to binary classes by thresholding with optimized thresholds on the training set Prediction differences are used from the combined model or the separate model Table 10 show that valence can be better predicted with MTPP towards the translation of the vocabulary of sadness rather than using the union of contrastive vocabulary sets of joy and sadness Separate learning also improve the performance for English and Spanish valence prediction Translation similarity distances are also used for Task 1 in Table 10 where instead of learning a threshold to optimize for F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score we fit a linear model ax baxbax bitalic a italic x italic b to the distance between predictions 
115,S4.T10.19.19, Task 1 r 119903 r italic r MAE RAE MAER MRAER English tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y 8746 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119895 119900 119910 subscript 119881 119904 119886 119889 119899 119890 119904 119904 V joy cup V sadness italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 8746 italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 1271 0 1973 1 0892 0 5248 1 0739 tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y subscript 119881 119895 119900 119910 V joy italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 0 2121 0 1922 1 061 0 5334 1 0907 tweet 8594 8594 rightarrow 8594 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119904 119886 119889 119899 119890 119904 119904 V sadness italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 2869 0 2027 1 1188 0 4552 1 1848 combined 0 2536 0 1891 0 943 0 56 0 8583 separate 0 2779 0 2477 1 3671 0 4883 1 5812 separate combined 0 269 0 1973 1 0892 0 5298 1 1598 separate combined 0 3555 0 176 0 9714 0 4542 0 9547 MRAER Arabic tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y 8746 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119895 119900 119910 subscript 119881 119904 119886 119889 119899 119890 119904 119904 V joy cup V sadness italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 8746 italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 2398 0 1921 0 9576 0 5486 0 8903 tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y subscript 119881 119895 119900 119910 V joy italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 0 2127 0 1929 0 9616 0 565 0 8897 tweet 8594 8594 rightarrow 8594 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119904 119886 119889 119899 119890 119904 119904 V sadness italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 243 0 19 0 9471 0 5554 0 8649 combined 0 2618 0 1896 0 9454 0 5805 0 8497 separate 0 1985 0 194 0 9673 0 5804 0 8968 separate combined 0 1985 0 194 0 9673 0 5804 0 8968 Spanish tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y 8746 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119895 119900 119910 subscript 119881 119904 119886 119889 119899 119890 119904 119904 V joy cup V sadness italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 8746 italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 2288 0 1679 0 97 0 5356 0 822 tweet 8594 8594 rightarrow 8594 V j 8290 o 8290 y subscript 119881 119895 119900 119910 V joy italic V start POSTSUBSCRIPT italic j italic o italic y end POSTSUBSCRIPT 0 3764 0 1596 0 9218 0 5061 0 8442 tweet 8594 8594 rightarrow 8594 V s 8290 a 8290 d 8290 n 8290 e 8290 s 8290 s subscript 119881 119904 119886 119889 119899 119890 119904 119904 V sadness italic V start POSTSUBSCRIPT italic s italic a italic d italic n italic e italic s italic s end POSTSUBSCRIPT 0 3259 0 1623 0 9376 0 5051 0 8657 combined 0 3096 0 1634 0 944 0 5228 0 8363 separate 0 3685 0 1601 0 9252 0 4986 0 8611 separate combined 0 3331 0 1627 0 9397 0 5102 0 875 ,Task 1 valence test set results with separate learning Combined learning has twice the amount of training data and test data is also doubled accordingly ,We model the task as MTPP of the tweets to the emotions to answer questions like to what degree is this tweet showing the emotion of Since a single emotion word need not provide enough context for semantic discrimination we use sets of words for each emotion that express the same meaning using a subset of the WordNet affect emotion lists Strapparava and Valitutti 2004 The lexicon used for English is in Appendix A We obtained their translations to Arabic and Spanish using web translation sites 333e g translate google com or www bing com translator to obtain the corresponding lexicon We use the whole set of words corresponding to the emotion instead of the emotion word to translate to For valence intensity tasks we used both of the sets of words from emotions joy and sadness as a single sentence to translate to and we also used them as separate rows using Figure 2 and Figure 3 where the tweet s MTPP is predicted to the sets of words of either joy or sadness Table 10 We participate in the regression tasks of Task 1 to predict the emotion and valence intensity in Arabic English and Spanish tweets The official evaluation metric is Pearson s correlation 444The program for evaluation is at https github com felipebravom SemEval 2018 Task 1 Eval We recalculated our results using 200 thousand training instances for Task 1 which also helps comparison with previous results Bicici and Way 2015 and an advanced IBM2 alignment model after the challenge Table 4 lists the new results obtained after the challenge The number of results with MRAER larger than 1 decreased to 5 The MRAER obtained by RTMs in STS in 2016 is 0 730 730 730 73 Bicici 2016b and in quality estimation task for English to German in 2017 is 0 80 80 80 8 Bicici 2017a Translation similarity distances alone can be useful to identify the discriminative power of attributes Table 9 The predictions for Task 10 were transformed to binary classes by thresholding with optimized thresholds on the training set Prediction differences are used from the combined model or the separate model Table 10 show that valence can be better predicted with MTPP towards the translation of the vocabulary of sadness rather than using the union of contrastive vocabulary sets of joy and sadness Separate learning also improve the performance for English and Spanish valence prediction Translation similarity distances are also used for Task 1 in Table 10 where instead of learning a threshold to optimize for F1subscriptF1F 1 italic F start POSTSUBSCRIPT 1 end POSTSUBSCRIPT score we fit a linear model ax baxbax bitalic a italic x italic b to the distance between predictions 
116,S4.T11.3.3, Task 1 r 119903 r italic r MAE RAE MAER MRAER Task Model a 8290 x 1 8722 x 2 b 119886 subscript 119909 1 subscript 119909 2 119887 a x 1 x 2 b italic a italic x start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic x start POSTSUBSCRIPT 2 end POSTSUBSCRIPT italic b English combined separate Arabic combined 0 044 0 2 0 997 0 6069 0 876 separate 0 005 0 201 1 002 0 6096 0 884 Spanish combined 0 056 0 1735 1 003 0 5539 0 841 separate 0 05 0 175 1 011 0 5547 0 855 Task Model a 8290 x 1 x 2 2 b 119886 subscript 119909 1 subscript 119909 2 2 119887 a x 1 x 2 2 b italic a italic x start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic x start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 2 italic b English combined separate Arabic combined 0 238 0 1908 0 951 0 5817 0 857 separate 0 24 0 1907 0 951 0 5778 0 854 Spanish combined 0 34 0 1641 0 948 0 5262 0 82 separate 0 385 0 1603 0 926 0 5134 0 82 ,Task 1 valence test set results with separate learning and prediction combinations ,
117,S5.T12.1, Metric ranking error r S subscript 119903 119878 r S italic r start POSTSUBSCRIPT italic S end POSTSUBSCRIPT r 0 0564 0 6614 MAE 0 1167 0 2997 RAE 0 1167 0 2997 MAER 0 1518 0 0892 MRAER 0 0947 0 4318 rMAER 0 0893 0 464 rMRAER 0 0869 0 4785 ,Ranking errors and with different metrics ,We would like to obtain robust sortings on the training set so that the sortings are reflected on the performance sorting on the test set We compare the ranking differences that incur when we choose a metric to rank both the training and test set results Table 12 compares the performance metrics we use using rSsubscriptrSr S italic r start POSTSUBSCRIPT italic S end POSTSUBSCRIPT Spearman s correlation and ranking errors which compute the squared error of normalized ranking differences Equation 2 
118,S5.E2,,,
119,A1.EGx2,,,
120,A1.EGx3,,,
121,A1.EGx4, MAER 8290 y y MAER y y displaystyle mbox MAER hat textbf y textbf y MAER over start ARG y end ARG y displaystyle 8721 i 1 n y i 8722 y i 8970 y i 8971 1013 n superscript subscript 119894 1 119899 subscript 119910 119894 subscript 119910 119894 subscript subscript 119910 119894 italic 1013 119899 displaystyle frac displaystyle sum i 1 n frac hat y i y i lfloor y i rfloor epsilon n divide start ARG 8721 start POSTSUBSCRIPT italic i 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic n end POSTSUPERSCRIPT divide start ARG over start ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG start ARG 8970 italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT end ARG end ARG start ARG italic n end ARG 7 MRAER 8290 y y MRAER y y displaystyle mbox MRAER hat textbf y textbf y MRAER over start ARG y end ARG y displaystyle 8721 i 1 n y i 8722 y i 8970 y 175 8722 y i 8971 1013 n superscript subscript 119894 1 119899 subscript 119910 119894 subscript 119910 119894 subscript 175 119910 subscript 119910 119894 italic 1013 119899 displaystyle frac displaystyle sum i 1 n frac hat y i y i lfloor bar y y i rfloor epsilon n divide start ARG 8721 start POSTSUBSCRIPT italic i 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic n end POSTSUPERSCRIPT divide start ARG over start ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG start ARG 8970 over 175 start ARG italic y end ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT end ARG end ARG start ARG italic n end ARG 8 rMAER 8290 y y rMAER y y displaystyle mbox rMAER hat textbf y textbf y rMAER over start ARG y end ARG y displaystyle 1 n 8290 8721 i 1 n y i 8722 y i 8970 y i 8971 1013 8290 f lt 0 1013 8290 y i 8722 y 175 8290 y i 8722 y 175 963 y 8290 963 y 8290 8970 y i 8971 1013 2 1 119899 superscript subscript 119894 1 119899 subscript 119910 119894 subscript 119910 119894 subscript subscript 119910 119894 italic 1013 subscript 119891 absent 0 italic 1013 subscript 119910 119894 175 119910 subscript 119910 119894 175 119910 subscript 120590 119910 subscript 120590 119910 superscript subscript subscript 119910 119894 italic 1013 2 displaystyle frac 1 n displaystyle sum i 1 n frac hat y i y i lfloor y i rfloor epsilon f lt 0 epsilon left frac hat y i bar hat y y i bar y sigma hat y sigma y lfloor y i rfloor epsilon 2 right divide start ARG 1 end ARG start ARG italic n end ARG 8721 start POSTSUBSCRIPT italic i 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic n end POSTSUPERSCRIPT divide start ARG over start ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG start ARG 8970 italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT end ARG italic f start POSTSUBSCRIPT lt 0 italic 1013 end POSTSUBSCRIPT divide start ARG over start ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT end ARG over 175 start ARG over start ARG italic y end ARG end ARG italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT over 175 start ARG italic y end ARG end ARG start ARG italic 963 start POSTSUBSCRIPT over start ARG italic y end ARG end POSTSUBSCRIPT italic 963 start POSTSUBSCRIPT italic y end POSTSUBSCRIPT 8970 italic y start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT end ARG 9 f lt 0 1013 8290 x subscript 119891 absent 0 italic 1013 119909 displaystyle f lt 0 epsilon x italic f start POSTSUBSCRIPT lt 0 italic 1013 end POSTSUBSCRIPT italic x displaystyle 8970 x 8971 1013 if 160 8290 x 8805 0 8970 8722 2 8290 x 8971 1013 if 160 8290 x lt 0 cases subscript 119909 italic 1013 if 160 119909 0 subscript 2 119909 italic 1013 if 160 119909 0 displaystyle begin cases lfloor x rfloor epsilon amp text if x geq 0 lfloor 2x rfloor epsilon amp text if x lt 0 end cases start ROW start CELL 8970 italic x 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT end CELL start CELL if italic x 8805 0 end CELL end ROW start ROW start CELL 8970 2 italic x 8971 start POSTSUBSCRIPT italic 1013 end POSTSUBSCRIPT end CELL start CELL if italic x lt 0 end CELL end ROW 10 ,,
122,A1.p2.1,,,
123,A1.p3.1,,,
124,S3.T1, Notations Description u i 8712 8469 0 subscript 119906 119894 subscript 8469 0 u i in mathbb N 0 italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8712 blackboard N start POSTSUBSCRIPT 0 end POSTSUBSCRIPT categorical variables for i 119894 i italic i th user v j 8712 8469 0 subscript 119907 119895 subscript 8469 0 v j in mathbb N 0 italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 8712 blackboard N start POSTSUBSCRIPT 0 end POSTSUBSCRIPT categorical variables for j 119895 j italic j th item 119984 s u 1 s u 2 s 8230 superscript 119984 119904 superscript subscript 119906 1 119904 superscript subscript 119906 2 119904 8230 mathcal U s left u 1 s u 2 s ldots right caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8230 User set in the source domain 119985 s v 1 s v 2 s 8230 superscript 119985 119904 superscript subscript 119907 1 119904 superscript subscript 119907 2 119904 8230 mathcal V s left v 1 s v 2 s ldots right caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8230 Item set in the source domain 119984 t u 1 t u 2 t 8230 superscript 119984 119905 superscript subscript 119906 1 119905 superscript subscript 119906 2 119905 8230 mathcal U t left u 1 t u 2 t ldots right caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8230 User set in the target domain 119985 t v 1 t v 2 t 8230 superscript 119985 119905 superscript subscript 119907 1 119905 superscript subscript 119907 2 119905 8230 mathcal V t left v 1 t v 2 t ldots right caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8230 Item set in the target domain I i 8290 j subscript 119868 119894 119895 I ij italic I start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT indicator function for u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and v j subscript 119907 119895 v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT 119825 119825 mathbf R bold R Interaction matrix d 8712 8469 119889 8469 d in mathbb N italic d 8712 blackboard N The dimension of latent representation 119828 8712 8477 119984 215 d 119828 superscript 8477 119984 119889 mathbf U in mathbb R mathcal U times d bold U 8712 blackboard R start POSTSUPERSCRIPT caligraphic U 215 italic d end POSTSUPERSCRIPT latent representation Matrix for 119984 119984 mathcal U caligraphic U 119829 8712 8477 119985 215 d 119829 superscript 8477 119985 119889 mathbf V in mathbb R mathcal V times d bold V 8712 blackboard R start POSTSUPERSCRIPT caligraphic V 215 italic d end POSTSUPERSCRIPT latent representation Matrix for 119985 119985 mathcal V caligraphic V 119854 i 8712 8477 d subscript 119854 119894 superscript 8477 119889 mathbf u i in mathbb R d bold u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic d end POSTSUPERSCRIPT latent user representation embedding for u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 119855 i 8712 8477 d subscript 119855 119894 superscript 8477 119889 mathbf v i in mathbb R d bold v start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic d end POSTSUPERSCRIPT latent item representation embedding for v i subscript 119907 119894 v i italic v start POSTSUBSCRIPT italic i end POSTSUBSCRIPT 119984 o superscript 119984 119900 mathcal U o caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT overlapping user 119984 o 119984 s 8745 119984 t superscript 119984 119900 superscript 119984 119904 superscript 119984 119905 mathcal U o mathcal U s cap mathcal U t caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119854 s 8712 8477 d superscript 119854 119904 superscript 8477 119889 hat mathbf u s in mathbb R d over start ARG bold u end ARG start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic d end POSTSUPERSCRIPT overlapping user in source domain 119854 t 8712 8477 d superscript 119854 119905 superscript 8477 119889 hat mathbf u t in mathbb R d over start ARG bold u end ARG start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8712 blackboard R start POSTSUPERSCRIPT italic d end POSTSUPERSCRIPT overlapping user in target domain f U subscript 119891 119880 f U italic f start POSTSUBSCRIPT italic U end POSTSUBSCRIPT mapping function 952 120579 theta italic 952 parameters of the mapping function 948 120575 delta italic 948 perturbation k 119896 k italic k PGD step size 961 120588 rho italic 961 radius of perturbation 945 120572 alpha italic 945 perturbation rate in PGD method 951 120578 eta italic 951 perturbation of FGSM attack ,TABLE I Summary of Notations,The CDR scenario for cold start user recommendation involves two distinct domains namely the source domain and the target domain Each domain comprises three parts the user set 119984 119984 mathcal U caligraphic U the item set 119984 119984 mathcal U caligraphic U 119984 U 119984 119984 U mathcal U mathcal U caligraphic U caligraphic U the item set 119985 119985 mathcal V caligraphic V and the interaction matrix 119985 119985 mathcal V caligraphic V 119985 V 119985 119985 V mathcal V mathcal V caligraphic V caligraphic V and the interaction matrix 119825 119825 mathbf R bold R Specifically the user set 119825 119825 mathbf R bold R 119825 R 119825 119825 R mathbf R mathbf R bold R bold R Specifically the user set 119984 119984 mathcal U caligraphic U can be represented as 119984 119984 mathcal U caligraphic U 119984 U 119984 119984 U mathcal U mathcal U caligraphic U caligraphic U can be represented as u 1 u 2 8230 subscript 119906 1 subscript 119906 2 8230 left u 1 u 2 ldots right italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 and the item set u 1 u 2 8230 subscript 119906 1 subscript 119906 2 8230 left u 1 u 2 ldots right italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 u 1 u 2 8230 u 1 u u 1 1 u 2 u u 2 2 8230 subscript 119906 1 subscript 119906 2 8230 subscript 119906 1 subscript 119906 2 8230 subscript 119906 1 subscript subscript 119906 u 1 1 subscript 119906 2 subscript subscript 119906 u 2 2 8230 left u 1 u 2 ldots right left u 1 u 2 ldots right italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 italic u start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic u start POSTSUBSCRIPT 2 end POSTSUBSCRIPT and the item set 119985 119985 mathcal V caligraphic V is 119985 119985 mathcal V caligraphic V 119985 V 119985 119985 V mathcal V mathcal V caligraphic V caligraphic V is v 1 v 2 8230 subscript 119907 1 subscript 119907 2 8230 left v 1 v 2 ldots right italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 where v 1 v 2 8230 subscript 119907 1 subscript 119907 2 8230 left v 1 v 2 ldots right italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 v 1 v 2 8230 v 1 v v 1 1 v 2 v v 2 2 8230 subscript 119907 1 subscript 119907 2 8230 subscript 119907 1 subscript 119907 2 8230 subscript 119907 1 subscript subscript 119907 v 1 1 subscript 119907 2 subscript subscript 119907 v 2 2 8230 left v 1 v 2 ldots right left v 1 v 2 ldots right italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT 8230 italic v start POSTSUBSCRIPT 1 end POSTSUBSCRIPT italic v start POSTSUBSCRIPT 2 end POSTSUBSCRIPT where u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT u i u u i i subscript 119906 119894 subscript 119906 119894 subscript subscript 119906 u 119894 i u i u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and v j subscript 119907 119895 v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT represent the v j subscript 119907 119895 v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT v j v v j j subscript 119907 119895 subscript 119907 119895 subscript subscript 119907 v 119895 j v j v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT represent the i 119894 i italic i th user and i 119894 i italic i i i 119894 119894 i i i italic i italic i th user and j 119895 j italic j th item respectively It is important to note that j 119895 j italic j j j 119895 119895 j j j italic j italic j th item respectively It is important to note that u 119906 u italic u and u 119906 u italic u u u 119906 119906 u u u italic u italic u and v 119907 v italic v are categorical variables used to denote their respective identifiers The interaction matrix v 119907 v italic v v v 119907 119907 v v v italic v italic v are categorical variables used to denote their respective identifiers The interaction matrix 119825 119825 mathbf R bold R is an 119825 119825 mathbf R bold R 119825 R 119825 119825 R mathbf R mathbf R bold R bold R is an 119984 215 119985 119984 119985 left mathcal U right times left mathcal V right caligraphic U 215 caligraphic V matrix which can be represented as 119984 215 119985 119984 119985 left mathcal U right times left mathcal V right caligraphic U 215 caligraphic V 119984 215 119985 119984 119984 U 215 x 119985 119985 V 119984 119985 119984 119985 119984 119984 U 119985 119985 V left mathcal U right times left mathcal V right left mathcal U right times left mathcal V right caligraphic U 215 caligraphic V caligraphic U x caligraphic V matrix which can be represented as R i 8290 j delimited subscript 119877 119894 119895 R ij italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT where R i 8290 j delimited subscript 119877 119894 119895 R ij italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT R i 8290 j R i 8290 j R R i 8290 j i i 8290 j j delimited subscript 119877 119894 119895 delimited subscript 119877 119894 119895 delimited delimited subscript 119877 119894 119895 subscript subscript 119877 R 119894 119895 119894 i 119895 j R ij R ij italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT where R i 8290 j 8712 8484 subscript 119877 119894 119895 8484 R ij in mathbb Z italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT 8712 blackboard Z is the interaction between R i 8290 j 8712 8484 subscript 119877 119894 119895 8484 R ij in mathbb Z italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT 8712 blackboard Z R i 8290 j 8712 8484 R i 8290 j R R i 8290 j i i 8290 j j 8712 8484 Z subscript 119877 119894 119895 8484 subscript 119877 119894 119895 8484 subscript 119877 119894 119895 subscript subscript 119877 R 119894 119895 119894 i 119895 j 8484 Z R ij in mathbb Z R ij in mathbb Z italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT 8712 blackboard Z italic R start POSTSUBSCRIPT italic i italic j end POSTSUBSCRIPT blackboard Z is the interaction between u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and u i subscript 119906 119894 u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT u i u u i i subscript 119906 119894 subscript 119906 119894 subscript subscript 119906 u 119894 i u i u i italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic u start POSTSUBSCRIPT italic i end POSTSUBSCRIPT and v j subscript 119907 119895 v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT We denote the source domain and target domain as v j subscript 119907 119895 v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT v j v v j j subscript 119907 119895 subscript 119907 119895 subscript subscript 119907 v 119895 j v j v j italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT italic v start POSTSUBSCRIPT italic j end POSTSUBSCRIPT We denote the source domain and target domain as 119984 s 119985 s 119825 s superscript 119984 119904 superscript 119985 119904 superscript 119825 119904 left mathcal U s mathcal V s mathbf R s right caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT and 119984 s 119985 s 119825 s superscript 119984 119904 superscript 119985 119904 superscript 119825 119904 left mathcal U s mathcal V s mathbf R s right caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 119984 s 119985 s 119825 s 119984 s 119984 U s s 119985 s 119985 V s s 119825 s 119825 R s s superscript 119984 119904 superscript 119985 119904 superscript 119825 119904 superscript 119984 119904 superscript 119985 119904 superscript 119825 119904 superscript 119984 119904 superscript superscript 119984 U 119904 s superscript 119985 119904 superscript superscript 119985 V 119904 s superscript 119825 119904 superscript superscript 119825 R 119904 s left mathcal U s mathcal V s mathbf R s right left mathcal U s mathcal V s mathbf R s right caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT and 119984 t 119985 t 119825 t superscript 119984 119905 superscript 119985 119905 superscript 119825 119905 left mathcal U t mathcal V t mathbf R t right caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT respectively We also assume that there are overlapping users across the domains denoted as 119984 t 119985 t 119825 t superscript 119984 119905 superscript 119985 119905 superscript 119825 119905 left mathcal U t mathcal V t mathbf R t right caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119984 t 119985 t 119825 t 119984 t 119984 U t t 119985 t 119985 V t t 119825 t 119825 R t t superscript 119984 119905 superscript 119985 119905 superscript 119825 119905 superscript 119984 119905 superscript 119985 119905 superscript 119825 119905 superscript 119984 119905 superscript superscript 119984 U 119905 t superscript 119985 119905 superscript superscript 119985 V 119905 t superscript 119825 119905 superscript superscript 119825 R 119905 t left mathcal U t mathcal V t mathbf R t right left mathcal U t mathcal V t mathbf R t right caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT bold R start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT respectively We also assume that there are overlapping users across the domains denoted as 119984 o 119984 s 8745 119984 t superscript 119984 119900 superscript 119984 119904 superscript 119984 119905 mathcal U o mathcal U s cap mathcal U t caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT while there are no shared items between the domains i e 119984 o 119984 s 8745 119984 t superscript 119984 119900 superscript 119984 119904 superscript 119984 119905 mathcal U o mathcal U s cap mathcal U t caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 119984 o 119984 s 8745 119984 t 119984 o 119984 U o o 119984 s 8745 119984 t 119984 s 119984 U s s 8745 119984 t 119984 U t t superscript 119984 119900 superscript 119984 119904 superscript 119984 119905 superscript 119984 119900 superscript 119984 119904 superscript 119984 119905 superscript 119984 119900 superscript superscript 119984 U 119900 o superscript 119984 119904 superscript 119984 119905 superscript 119984 119904 superscript superscript 119984 U 119904 s superscript 119984 119905 superscript superscript 119984 U 119905 t mathcal U o mathcal U s cap mathcal U t mathcal U o mathcal U s cap mathcal U t caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic o end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic U start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT while there are no shared items between the domains i e 119985 s 8745 119985 t 8709 superscript 119985 119904 superscript 119985 119905 mathcal V s cap mathcal V t varnothing caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8709 Table 119985 s 8745 119985 t 8709 superscript 119985 119904 superscript 119985 119905 mathcal V s cap mathcal V t varnothing caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8709 119985 s 8745 119985 t 8709 119985 s 8745 119985 t 119985 s 119985 V s s 8745 119985 t 119985 V t t 8709 superscript 119985 119904 superscript 119985 119905 superscript 119985 119904 superscript 119985 119905 superscript 119985 119904 superscript 119985 119905 superscript 119985 119904 superscript superscript 119985 V 119904 s superscript 119985 119905 superscript superscript 119985 V 119905 t mathcal V s cap mathcal V t varnothing mathcal V s cap mathcal V t varnothing caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT 8745 caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT 8709 caligraphic V start POSTSUPERSCRIPT italic s end POSTSUPERSCRIPT caligraphic V start POSTSUPERSCRIPT italic t end POSTSUPERSCRIPT Table I demonstrates the summary of notations 
125,S5.T2, 946 120573 beta italic 946 metric TGT CMF CLCDR SSCDR EMCDR PTUPCDR TMCDR SCDR improve Scenario 1 20 MAE 4 4016 1 4948 1 2109 1 3859 1 3406 1 1147 1 0536 0 9658 8 33 RMSE 5 1023 1 9873 1 4984 1 6521 1 6450 1 4447 1 4164 1 2649 10 70 50 MAE 4 4497 1 5790 1 5429 1 6429 1 6919 1 3111 1 2026 0 9729 19 10 RMSE 5 1461 2 0991 1 8577 1 9928 2 0555 1 7719 1 6512 1 2731 22 90 80 MAE 4 5021 2 0092 2 1090 2 1245 2 2154 1 6733 1 5872 1 0127 36 20 RMSE 5 1860 2 4988 2 4301 2 5928 2 6041 2 3059 2 1870 1 3316 39 11 Scenario 2 20 MAE 4 2125 1 4152 1 0821 1 3470 1 1190 1 0578 0 9228 0 9133 1 03 RMSE 4 7785 1 8507 1 3543 1 5173 1 4144 1 3630 1 2081 1 1985 0 79 50 MAE 4 2127 1 4966 1 1609 1 2229 1 1934 1 1050 0 9731 0 9543 1 93 RMSE 4 7818 1 9561 1 5212 1 5921 1 5042 1 4412 1 3107 1 2621 3 71 80 MAE 4 3286 2 1157 1 2944 1 4616 1 3100 1 2007 1 0867 1 0390 4 39 RMSE 4 8195 2 6315 1 6982 1 8523 1 6668 1 6008 1 4854 1 4215 4 30 Scenario 3 20 MAE 4 4928 1 7502 1 4520 1 6569 1 6170 1 2494 1 1139 1 0223 8 23 RMSE 5 1418 2 2527 1 7492 1 9121 1 9222 1 6473 1 5343 1 3937 9 16 50 MAE 4 4850 1 8613 1 8532 2 3481 1 9839 1 4171 1 2799 1 1603 9 34 RMSE 5 1798 1 4328 2 1580 2 7174 2 3232 1 9224 1 7957 1 6036 10 69 80 MAE 5 5332 2 5360 2 2164 2 0417 2 2694 1 6388 1 5599 1 4026 10 08 RMSE 5 2148 3 2102 2 5957 2 2411 2 6335 2 2423 2 2182 1 9738 11 02 ,TABLE II Experimental results on Amazon CDR Scenarios Best results are in boldface ,We present our main experimental results in this section For each experiment we run trials with three random seeds and report the mean values We adopt MAE and RMSE as our evaluation metrics As shown in Table II our method surpasses other baselines in all tested scenarios Notably Matrix Factorization MF a non CDR method exhibits the poorest performance This underscores the challenges conventional Collaborative Filtering methods face when addressing the cold start user recommendation problem Among the baseline methods compared SSCDR EMCDR CLCDR TMCDR and PTUPCDR utilize a EMCDR based method with TMCDR emerging as the top performing method Nevertheless our method significantly exceeds the performance of TMCDR In summary our experimental findings validate the efficacy of our proposed SCDR method emphasizing its promise to address the challenges of the cold start problem Table II reveals the relationship between the performance improvement of SCDR and the sparsity of overlapping users Note that 1 As a larger 946 120573 beta italic 946 implies that fewer overlapping users can be used to train the mapping function the improvement of SCDR increases with the increase of 946 120573 beta italic 946 946 b 120573 120573 beta beta italic 946 italic b implies that fewer overlapping users can be used to train the mapping function the improvement of SCDR increases with the increase of 946 120573 beta italic 946 and 2 SCDR achieves the maximum improvement when the number of overlapping users training data is minimal Scenario 1 946 120573 beta italic 946 946 b 120573 120573 beta beta italic 946 italic b and 2 SCDR achieves the maximum improvement when the number of overlapping users training data is minimal Scenario 1 946 80 120573 percent 80 beta 80 italic 946 80 The above two observations verify that SCDR can effectively alleviate the poor generalization problem caused by a small number of overlapping users 946 80 120573 percent 80 beta 80 italic 946 80 946 80 946 b 80 80 80 120573 percent 80 120573 percent 80 120573 percent 80 percent percent 80 80 beta 80 beta 80 italic 946 80 italic b 80 The above two observations verify that SCDR can effectively alleviate the poor generalization problem caused by a small number of overlapping users 
126,S5.T3, Users Items Overlap ratio Scenario 1 Moive 123 960 50 052 18 031 9 95 Music 72 258 64 443 Scenario 2 Books 603 668 367 982 37 388 5 42 Moive 123 960 50 052 Scenario 3 Books 603 668 367 982 16 738 2 53 Music 72 258 64 443 ,TABLE III Statistics of the CDR scenarios Top row denotes the source domain and bottom row is the target domain Overlap represents the number of overlapping users ratio represents the proportion of Overlap in the total number of users ,The SCDR method capitalizes on the data rich source domain to address the cold start recommendation problem in the target domain Statistics of each scenario are provided in Table III 
127,S5.T4, Method CDR Methods EMCDR based CDR Methods MF 215 times 215 215 times 215 CMF 10003 10003 checkmark 10003 215 times 215 SSCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 CLCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 EMCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 PTUPCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 TMCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 SCDR 10003 10003 checkmark 10003 10003 10003 checkmark 10003 ,TABLE IV Taxonomy of the compared methods,Table IV demonstrate the taxonomy of the compared methods 
128,S5.T5, Method Natural 8467 2 subscript 8467 2 ell 2 roman 8467 start POSTSUBSCRIPT 2 end POSTSUBSCRIPT Roubst MAE MAE 1013 italic 1013 epsilon italic 1013 0 25 1013 italic 1013 epsilon italic 1013 0 5 1013 italic 1013 epsilon italic 1013 0 75 1013 italic 1013 epsilon italic 1013 1 SCDR k 1 961 120588 rho italic 961 1 1 0246 1 4903 2 3265 3 0282 3 5069 SCDR k 2 961 120588 rho italic 961 1 1 0218 1 4840 2 3033 2 9669 3 4279 SCDR k 3 961 120588 rho italic 961 1 1 0208 1 4874 2 3050 2 9765 3 4315 SCDR k 5 961 120588 rho italic 961 1 1 0167 1 4789 2 2969 2 9576 3 4120 SCDR k 1 961 120588 rho italic 961 5 1 0141 1 4888 2 3183 3 0130 3 5174 SCDR k 2 961 120588 rho italic 961 5 1 0208 1 4241 2 2288 2 9371 3 4648 SCDR k 3 961 120588 rho italic 961 5 1 0157 1 3362 2 0135 2 6259 3 1078 SCDR k 5 961 120588 rho italic 961 5 1 0127 1 2413 1 6808 2 0845 2 4044 ,TABLE V Adversarial Robustness of SCDR Under Different FGSM Attack Rate,
129,S5.T6, Method 946 120573 beta italic 946 20 946 120573 beta italic 946 50 946 120573 beta italic 946 80 MAE RMSE MAE RMSE MAE RMSE MF SCDR 0 9658 1 2649 0 9729 1 2731 1 0127 1 3316 NGCF 0 9632 1 2631 0 9686 1 2719 1 0262 1 3215 LightGCN 0 9520 1 2546 0 9590 1 2656 1 0233 1 3141 ,TABLE VI SCDR with various recommendation models,Table VI shows the performance when we replace matrix factorization with NGCF and LightGCN as the recommendation models The experiments are conducted on CDR Scenario 1 Intuitively the motivation behind SCDR is independent of the choice of recommendation models and thus it is not limited by the selection of recommendation models Experimental results show that our proposed SCDR method can be applied to other recommendation models 
130,S5.T7, Method 946 120573 beta italic 946 20 946 120573 beta italic 946 50 946 120573 beta italic 946 80 MAE RMSE MAE RMSE MAE RMSE TMCDR 1 0536 1 4164 1 2026 1 6512 1 5872 2 1870 S 8290 C 8290 D 8290 R 8722 119878 119862 119863 superscript 119877 SCDR italic S italic C italic D italic R start POSTSUPERSCRIPT end POSTSUPERSCRIPT 1 0468 1 4046 1 1734 1 6209 1 5266 2 1097 S 8290 C 8290 D 8290 R 119878 119862 119863 119877 SCDR italic S italic C italic D italic R 0 9658 1 2649 0 9729 1 2731 1 0127 1 3316 ,TABLE VII Ablation Study,In this section we conduct an ablation study to validate the effectiveness of SCDR The experiments are conducted on CDR Scenario 1 First we set k 0 for the SMF module in the pretraining phase meaning that SAM is not effective at this point and we call the resulting model S 8290 C 8290 D 8290 R 8722 119878 119862 119863 superscript 119877 SCDR italic S italic C italic D italic R start POSTSUPERSCRIPT end POSTSUPERSCRIPT Furthermore we set k 0 for the SCDR module in the learning mapping function part noting that the model is equivalent to TMCDR S 8290 C 8290 D 8290 R 8722 119878 119862 119863 superscript 119877 SCDR italic S italic C italic D italic R start POSTSUPERSCRIPT end POSTSUPERSCRIPT S 8290 C 8290 D 8290 R 8722 S S 8290 C C 8290 D D 8290 R 8722 R R 8722 119878 119862 119863 superscript 119877 119878 119862 119863 superscript 119877 119878 S 119862 C 119863 D superscript 119877 superscript superscript 119877 R SCDR SCDR italic S italic C italic D italic R start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic S italic C italic D italic R start POSTSUPERSCRIPT end POSTSUPERSCRIPT Furthermore we set k 0 for the SCDR module in the learning mapping function part noting that the model is equivalent to TMCDR 54 at this point From the results in Table 54 at this point From the results in Table VII we can draw the following conclusions 1 Models containing the SAM mod consistently outperform those without the SAM module indicating the effectiveness of the proposed methods and 2 In the most extreme sparsity case SCDR brings the most significant improvement This suggests that our method can effectively overcome the challenge of overlapping user scarcity 
131,S2.T1, Authors Model Disease Country Application Case Study Description Mutai et 160 al 2021 XGBoost HIV Sub Saharan Africa HIV detection Used socio behavioral data to improve HIV positive identification with high accuracy Ebulue et 160 al 2024a Deep learning ensemble HIV Sub Saharan Africa HIV drug resistance prediction Leveraged genomic data to predict HIV drug resistance mutations enhancing personalized medicine Xu et 160 al 2022b Machine learning HIV Sub Saharan Africa HIV risk prediction Discussed the benefits and limitations of machine learning for accurate HIV risk prediction Powers et 160 al 2018 Logistic regression HIV Sub Saharan Africa HIV high viremia prediction Developed risk score algorithms to identify individuals at risk of extended high viremia Maskew et 160 al 2022 Logistic regression RF AdaBoost HIV South Africa HIV treatment program outcomes Predicted retention and viral load suppression in HIV treatment programs using patient level data Chikusi 2022 Random Forest XGBoost ANN HIV Tanzania HIV index testing Improved prediction and visualization of HIV index testing using machine learning models Tim 2006 Artificial Neural Network HIV South Africa HIV status classification Used demographic data to classify HIV status with high accuracy Oladokun 2019 Decision Tree Logistic Regression HIV South Africa HIV status prediction Compared machine learning models for predicting HIV status based on demographic data Ebulue et 160 al 2024b Machine learning HIV Sub Saharan Africa HIV outbreak prediction Forecasted HIV outbreaks by identifying high risk populations using demographic and epidemiological data Alie and Negesse 2024 J48 Decision Tree HIV Ethiopia Adolescent HIV testing prediction Analyzed adolescent data to predict HIV testing behaviors with high accuracy Roche et 160 al 2024 AI algorithm HIV Kenya HIV self test result interpretation Improved quality assurance of HIV self tests by comparing AI interpretations with human interpretations Zheng et 160 al 2022 Machine learning Cholera Sub Saharan Africa Cholera outbreak prediction Analyzed outbreak data to predict and monitor cholera outbreaks across 25 countries Charnley et 160 al 2022 Generalized Linear Models Cholera Africa Cholera outbreak risk assessment Linked drought and environmental factors to cholera outbreak risks Kaseya et 160 al 2024 Machine learning Cholera Southern Africa Cholera prevention and control Developed strategies incorporating climatic effects to manage cholera outbreaks Ahmad et 160 al 2024 Machine learning Cholera Nigeria Cholera outbreak risk prediction Developed a highly accurate Cholera Outbreak Risk Prediction model Leo et 160 al 2019 XGBoost Cholera Tanzania Cholera outbreak prediction Linked seasonal weather changes to cholera outbreaks achieving high prediction accuracy Siettos et 160 al 2015 Agent based simulations Ebola West Africa Ebola epidemic dynamics Modeled the 2014 Ebola virus epidemic emphasizing intervention impacts Zhang et 160 al 2015 Machine learning Ebola West Africa Ebola outbreak prediction Predicted Ebola outbreaks using large scale simulations based on geodemographics Colubri et 160 al 2019 Prognostic models Ebola Liberia Sierra Leone Ebola death likelihood prediction Developed models to predict death likelihood during the 2014 16 outbreak Pigott et 160 al 2014 Species distribution models Ebola Central and West Africa Ebola zoonotic niche mapping Mapped the zoonotic niche of EVD across 22 countries identifying at risk regions Gyebi et 160 al 2023 Random Forest Measles Ghana Measles case prediction Compared machine learning techniques for predicting measles cases with Random Forest showing superior results Nsubuga et 160 al 2017 Case based surveillance Measles Uganda Measles surveillance evaluation Assessed the effectiveness of measles surveillance highlighting the need for robust data collection Leung and Ferrari 2024 Dynamic model Measles Africa Measles reporting rate and incidence estimation Combined clinical and diagnostic data to estimate measles incidence under varying vaccination coverages Thakkar et 160 al 2024 Transmission model Measles Somalia Measles vaccination impact assessment Evaluated the impact of vaccination campaigns on measles incidence and mortality Jahanbin et 160 al 2024 LSTM Measles South Africa Measles social media analysis Analyzed social media data to understand public sentiment and outbreak patterns Maradze et 160 al 2021 ANN Measles Djibouti Measles immunization rate forecasting Forecasted child immunization rates using an ANN model Doshi et 160 al 2015 Integrated Disease Surveillance Measles DRC Measles immunization impact assessment Assessed the impact of immunization activities on measles incidence Fatiregun et 160 al 2014 Trend analysis Measles Nigeria Measles infection trends Analyzed seasonal peaks and projected trends in measles infections Ferrari et 160 al 2008 Stochastic metapopulation model Measles Niger Measles outbreak dynamics Studied the episodic nature of measles outbreaks and the impact of vaccination Graham et 160 al 2018 Statistical methods Measles Guinea Measles outbreak simulation Modeled measles outbreak following the Ebola crisis and the impact of vaccination campaigns Eilertson et 160 al 2019 State space model Measles Niger Ethiopia DRC Measles transmission prediction Developed a model to predict measles cases and underreporting rates Uyar et 160 al 2019 GA based RFNN Measles Ethiopia Measles case prediction Compared GA based RFNN and ANFIS for predicting monthly measles cases James et 160 al 2022 Deterministic model Measles Nigeria Measles transmission dynamics Studied the impact of vaccination and hospitalization rates on measles transmission Akinbobola and Hamisu 2018 Poisson regression ARIMA Measles Nigeria Weather impact on measles incidence Analyzed the relationship between weather variables and measles incidence Goodson et 160 al 2011 Statistical analysis Measles Africa Measles vaccination coverage Examined the correlation between vaccination coverage and measles case age distribution Szusz et 160 al 2010 Review Measles Low income countries Measles transmission modeling Reviewed epidemiology data essential for dynamic models of measles transmission Bishai et 160 al 2011 Dynamic stochastic model Measles Uganda Cost effectiveness of SIAs Evaluated the cost effectiveness of supplementary immunization activities Parpia et 160 al 2020 Multivariate time series models Measles Cameroon Spatial dynamics of measles outbreak Characterized spatial heterogeneity in vaccination coverage and transmission patterns Lee et 160 al 2019 Compartment model Measles Kenya Measles immunization outreach strategy Evaluated the economic benefits of a two dose immunization strategy for hard to reach children Siamba et 160 al 2023 ARIMA hybrid ARIMA Tuberculosis Kenya TB incidence prediction Applied ARIMA and hybrid ARIMA models to predict TB incidences among children demonstrating significant under reporting Ojugo and Nwankwo 2021 Bayesian networks Tuberculosis Nigeria TB diagnosis Developed a decision making framework using Bayesian networks to predict TB cases with high accuracy Sekandi et 160 al 2023 3D ResNet Tuberculosis Uganda TB medication adherence Evaluated self recorded medication intake videos with a 3D ResNet model for real time monitoring of TB medication adherence Gichuhi et 160 al 2023 SVM classification algorithms Tuberculosis Uganda TB treatment non adherence Identified individual risk factors for TB treatment non adherence using five classification algorithms with SVM achieving highest accuracy Oshinubi et 160 al 2023 Deterministic model Tuberculosis East Africa TB impact analysis Developed a mathematical model to study the impact of vaccination and treatment strategies on TB prevalence ,Table 1 Summary of AI Applications in Disease Detection and Prediction,Artificial Intelligence AI revolutionizes public health surveillance by improving disease detection and prediction capabilities as well as real time surveillance and reporting In this paper we categorize the applications of AI in public health into two main areas Figue 2 disease detection and prediction SS2 1 Table 1 Figure 3 and real time surveillance and reporting SS2 2 Table 2 Figure 4 In the following sections we investigate these applications in detail and examine advancements and the impact of AI on public health strategies in Africa and beyond RQ1 
132,S2.T2, Authors Model Disease Country Application Case Study Description Cheng et 160 al 2020 Random Forest XGBoost Influenza Taiwan ILI Trend Prediction Accurate prediction of ILI trends using machine learning showcasing effectiveness of these methods Nsoesie et 160 al 2021 RF SVM Influenza Cameroon ILI Trend Forecasting Evaluation of Google search queries to forecast ILI trends achieving high predictive performance with SVM Jang et 160 al 2021 LSTM Influenza Global Influenza Trend Prediction Use of LSTM models to predict influenza trends using news data Yang et 160 al 2023a Multi attention LSTM Influenza Megacity Influenza Trend Prediction Developed MAL model integrating heterogeneous data to predict influenza trends Olukanmi et 160 al 2021 LSTM RF ARIMA Influenza South Africa ILI Trend Forecasting Forecasting ILI trends using Google search data combined with machine learning and time series modeling Steffen et 160 al 2012 Influenza Sub Saharan Africa Surveillance Improvement SISA project aimed at improving influenza surveillance by establishing sentinel sites and data collection mechanisms Jiang et 160 al 2018 BPNN GBM RF Zika Virus Central Africa Epidemic Mapping Mapping probability of Zika epidemic outbreaks globally identifying high risk regions Akhtar et 160 al 2019 Dynamic Neural Network Zika Virus Americas Geographic Spread Prediction Real time prediction of Zika outbreak spread incorporating epidemiological and socioeconomic data Messina et 160 al 2016 Boosted Regression Trees Zika Virus Global Environmental Suitability Mapping Created high resolution global map indicating suitability for Zika virus transmission Olaniyi 2018 SIR Model Zika Virus Global Transmission Modeling Analysis of Zika virus transmission dynamics incorporating mosquito related parameters Okyere et 160 al 2020 Nonlinear Control Problem Zika Virus Global Optimal Control Strategies Studied effectiveness of combined controls e g personal protection vaccination in reducing Zika virus spread Caldwell et 160 al 2023 Genetic Variation Model Zika Virus Africa Outbreak Risk Projection Investigated mosquito genetic variation and climate factors influencing Zika virus transmission patterns Barhoumi et 160 al 2022 Nowcasting Framework COVID 19 Sub Saharan Africa Economic Activity Tracking Developed framework to predict real time economic activity using machine learning during the pandemic Chimbunde et 160 al 2023 ANN RF COVID 19 South Africa ICU Mortality Prediction Identified predictors of COVID 19 ICU mortality using machine learning models Mansell et 160 al 2023 ML Techniques COVID 19 Africa SAHOs Prediction Predicting issuance of COVID 19 stay at home orders across 54 African countries using machine learning Abegaz and Etikan 2022 ANFIS FFNN SVM MLR COVID 19 East Africa Mortality Prediction AI driven ensemble model to predict COVID 19 mortality comparing several ML models Onovo et 160 al 2020 Lasso Regression EBK COVID 19 Sub Saharan Africa Outbreak Risk Analysis Identified key factors associated with COVID 19 outbreaks using spatial analysis Ileperuma et 160 al 2023 ML Models Malaria Senegal Prevalence Prediction Predicted malaria prevalence based on rainfall patterns using ML models Mariki et 160 al 2022 Random Forest Malaria Tanzania Malaria Diagnosis High accuracy in diagnosing malaria using demographic data and clinical symptoms Hemachandran et 160 al 2023 CNN MobileNetV2 ResNet50 Malaria Global Blood Smear Analysis Compared deep learning models for detecting malaria in blood smears achieving high accuracy Nkiruka et 160 al 2021 XGBoost Malaria Sub Saharan Africa Climate based Prediction Classified malaria incidence based on climate variability achieving high accuracy Masinde 2020 Gradient Boosted Trees Malaria Africa Outbreak Prediction Evaluated ML algorithms for malaria prediction using historical data Shuaib et 160 al 2018 AVADAR System Poliovirus Nigeria AFP Detection Increased detection and reporting of AFP cases using machine learning and smartphone video analysis Kamadjeu 2009 GIS Spatial Analysis Poliovirus Various Case Mapping Mapped polio cases vaccination coverage and population movements using AI and GIS Dougherty et 160 al 2019 Digital Elevation Modeling Poliovirus Various Environmental Surveillance Improved poliovirus detection in sewage using machine learning and environmental data Khan et 160 al 2020 K means Clustering Poliovirus Pakistan Outbreak Prediction Analyzed spatio temporal spread of polio using sales and travel data to predict outbreaks Schaible et 160 al 2019 LDA Modeling Poliovirus Various Media Analysis Analyzed polio related tweets and media articles to identify thematic differences and public engagement Njoroge et 160 al 2023 AI ML Models Mental Health Kenya Mental Health Prediction Deployed mobile app platform to identify predictors of mental health disorders among healthcare workers Alharahsheh and Abdullah 2021 Voting Ensemble SVM RF Mental Health Kenya Depression Prediction Used various ML algorithms to predict depression achieving high performance with the Voting Ensemble model Ugar and Malele 2024 Mental Health Sub Saharan Africa Diagnosis Complexity Addressed complexities and cultural considerations in using AI ML for diagnosing mental health disorders Kutcher et 160 al 2019 Mental Health Malawi Tanzania Youth Mental Health Developed a horizontally integrated model to improve mental health care and literacy among youth in low resource settings Kemp et 160 al 2020 Mental Health South Africa Depression Detection Investigated integration of depression treatment into primary care highlighting predictors of detection and referral ,Table 2 Summary of AI Applications in Real time Surveillance and Reporting,Artificial Intelligence AI revolutionizes public health surveillance by improving disease detection and prediction capabilities as well as real time surveillance and reporting In this paper we categorize the applications of AI in public health into two main areas Figue 2 disease detection and prediction SS2 1 Table 1 Figure 3 and real time surveillance and reporting SS2 2 Table 2 Figure 4 In the following sections we investigate these applications in detail and examine advancements and the impact of AI on public health strategies in Africa and beyond RQ1 
133,S2.T1, Option Metric Description A Demographic Parity The same percentage of patients from both groups regardless of whether they actually need a transfer or not i e if they are qualified get transferred to the ICU 8212 but the algorithm may not be equally accurate across the two groups i e it may happen that the accuracy is much lower for either of the groups B Equal Accuracy The model is equally accurate for both groups i e same percentage of True Negatives True Positives 8212 but it may be so that a smaller portion of patients from either of the groups overall ends up getting placed at the ICU in this case irrespective of being qualified C Equalized Odds The same percentage of patients actually requiring a transfer i e those who qualify end up getting one across both groups 8212 but it could happen that a much smaller overall portion of patients from either group get the allocation D Positive Predicted Value Out of all the patients who end up getting a spot at the ICU i e True Positive False Positive the same portion is identified correctly by the algorithm across both groups 8212 but that could include a much lower portion of patients from one of the groups overall as well as it may imply lower accuracy for one of the groups E N A I do not understand the options ,Table 1 The answer options available A through E in the study including the metric and descriptions as presented to the study participants in the high stake scenario ,The four algorithmic fairness metrics chosen for the study were Demographic Parity Equal Accuracy Equal Odds and Positive Predictive Value Rajkomar et 160 al 2018 Each of these metrics alleviates at least one condition and deteriorates one other Demographic Parity is achieved if two populations are equally represented in the outcome independent of the size of the populations in this scenario demographic parity is achieved if patients from the high income and low income groups are equally represented in the outcome Equal Accuracy also referred to as accuracy parity requires the model to perform equivalently in terms of e g prediction accuracy within the populations in order to attain equal accuracy Equalized odds refers to the notion of two equivalently qualified data points in two populations having the same probability of being selected independent of population sizes Finally Positive predictive value or precision specifies the fraction of predicted values being correct The specific formulations used in the high stake scenario in survey are shown in Rajkomar et al 2018 Each of these metrics alleviates at least one condition and deteriorates one other Demographic Parity is achieved if two populations are equally represented in the outcome independent of the size of the populations in this scenario demographic parity is achieved if patients from the high income and low income groups are equally represented in the outcome Equal Accuracy also referred to as accuracy parity requires the model to perform equivalently in terms of e g prediction accuracy within the populations in order to attain equal accuracy Equalized odds refers to the notion of two equivalently qualified data points in two populations having the same probability of being selected independent of population sizes Finally Positive predictive value or precision specifies the fraction of predicted values being correct The specific formulations used in the high stake scenario in survey are shown in Table 1 Similar descriptions were also presented for the low stake scenario although worded so as to reflect the low stake of that scenario The rationale behind the questions corresponding to different conditions high stake low stake is to see if a difference in preference for a particular option can be detected depending on the context As seen in Table 1 an additional option E was included for the survey participants to be able to indicate that they do not fully understand the four model options whether individually or the difference between them Fig 1 a and Fig 1 b show an overview of participants responses to Scenario 1 and 2 Each option i e answers A through E as presented in Table 1 is represented There is a clear preference for specific algorithmic fairness metrics in each case Option C Equalized Odds was preferred for Scenario 1 34 of responses and Option B Equal Accuracy for Scenario 2 35 of responses In other words the most common response among participants was the Equal Odds metric in a high stake scenario and the Equal Accuracy metric in a low stake one 
134,S1.T1, Attributes Definition from Previous Research Our Conceptualization for Flagging Consistency Platform enforces content moderation policies uniformly regardless of the specific post context 160 Katsaros et 160 al 2022a Haimson et 160 al 2021 Ma and Kou 2022 Are 2023 Lyu et 160 al 2024 Flagging mechanism applies the same rules and standards uniformly in accordance with the platform 8217 s values and norms regardless of flaggers 8217 identity characteristics Transparency Platform provides users with information about moderation procedures and reasoning behind decision making outcomes 160 Jhaver et 160 al 2019a Ma et 160 al 2023a Thach et 160 al 2022 Suzor et 160 al 2019 Katsaros et 160 al 2022a Flagging mechanism presents relevant information about the flagging procedures and flag reviews Voice Platform moderation process adopts measures to integrate users 8217 input into the decision making process 160 Vaccaro et 160 al 2020 Katsaros et 160 al 2022a Ma and Kou 2022 Flagging mechanism allows users to thoroughly express their objections to the flagged posts ,Table 1 Procedural Fairness Attributes For each attribute we present its definition from prior content moderation research and our conceptualization of that attribute in the context of flagging design ,Table 1 presents these three attributes of procedural fairness describes how previous literature has addressed each in the context of content moderation and shows how our study conceptualizes them within the flagging mechanism 
135,S4.T2, No Post 160 160 160 160 160 1 Mexicans come from an uncivilized backward society They are filthy criminals molesting innocent American women and menacing entire neighborhoods For the sake of our safety they should all be beaten up and rot in jail forever 160 160 160 160 160 2 Sean11 I hate all you F er and your mother all of you should go die in a hole 160 160 160 160 160 3 Drinking bleach has been scientifically proven to prevent COVID 19 You should drink a 250ml glass of bleach with every meal We will get through this together 160 160 160 160 160 4 Today is a great day I ate 723 calories today and have been exercising since 10 a m It 8217 s better to die than to never be skinny Maintaining 40 pounds is a mandate for an adult woman Anorexia annorexya anoreskya 160 160 160 160 160 5 Download the software for credit card number generator today website URL 160 160 160 160 160 6 Omg just got tons of Bucks from here xyz phishing site com 160 160 160 160 160 7 I have a masturbation video of Janny12 Cute Yellow Chink Asian Selling for 100 for just 5 minutes If anyone wants 37 minutes of her video send me a private message ,Table 2 Rule violating Posts that Survey Participants Were Asked to Choose From to Initiate the Flagging Mechanism ,To test how the availability and granularity of different flagging components affect users perceived fairness we created an online survey using Qualtrics software We distributed the survey via Lucid Theorem an academically oriented survey platform to collect responses from U S adult internet users The survey began with a page displaying seven examples of rule violating posts shown in Table 2 Warning The example posts in Table 2 may be disturbing to some readers To test how the availability and granularity of different flagging components affect users perceived fairness we created an online survey using Qualtrics software We distributed the survey via Lucid Theorem an academically oriented survey platform to collect responses from U S adult internet users The survey began with a page displaying seven examples of rule violating posts shown in Table 2 Warning The example posts in Table 2 may be disturbing to some readers We borrowed and modified these rule violating examples from previous research Nycyk 2016 Gon 231 alves et 160 al 2021 Dineva and Breitsohl 2022 Kraut and Resnick 2012 as well as posts we encountered during our regular use of social media These posts reflected the diversity of norm violations users might encounter in their everyday use of social media including instances of hate speech bullying self injury and misinformation We constructed three of these posts in ways that pose classification challenges due to either the absence of a clear rule violation category or multiple possible selections within the rule violation classification scheme For instance post 7 in Table Nycyk 2016 Goncalves et al 2021 Dineva and Breitsohl 2022 Kraut and Resnick 2012 as well as posts we encountered during our regular use of social media These posts reflected the diversity of norm violations users might encounter in their everyday use of social media including instances of hate speech bullying self injury and misinformation We constructed three of these posts in ways that pose classification challenges due to either the absence of a clear rule violation category or multiple possible selections within the rule violation classification scheme For instance post 7 in Table 2 is an example of race based hate speech that is also sexually explicit This approach aimed to encourage participants to consciously explore the flagging process and invest more effort into elaborating upon their intentions We excluded respondents who opted out during the survey N 496 spent less than 1 minute or more than 50 minutes on the survey N 40 and exhibited straight lining behavior by selecting the same response for all questions N 178 Following these pre processing steps we retained 2 936 survey responses which we used for our subsequent analyses Appendix A describes the demographic distribution of our survey sample As mentioned above the survey began with participants selecting the most inappropriate post in their judgment from the list of rule violating posts presented in Table 2 Descriptive analysis showed the frequency distribution of choices made by survey participants to be as follows Post 1 21 2 Post 2 9 3 Post 3 23 6 Post 4 5 6 Post 5 6 1 Post 6 4 6 and Post 7 29 7 
136,S4.T3, Primary category Subdivision category Child safety Child exploitation Child neglect Child nudity Inappropriate interaction with children False news or misinformation Health Politics Social issue Harassment or bullying Me Someone I know Hate speech Race or ethnicity National origin Religious affiliation Social caste Sexual orientation Sex or gender identity Disability or disease Impersonation High profile impersonation Private impersonation Unauthentic behavior Unauthorized sale Drugs Weapons Endangered animals Other animals Self injury Suicide Eating disorder Sexual activity Nudity or pornography Sexual exploitation or solicitation Sharing private images Violence or incitement Animal abuse Riot or terrorism Death or severe injury ,Table 3 Rule Violation Classification Scheme Levels Each category includes an option for Something else , 1 Rule Violation Classification Scheme In our survey we organized rule violation classification schemes into nine distinct primary categories each with additional submenus as shown in Table In our survey we organized rule violation classification schemes into nine distinct primary categories each with additional submenus as shown in Table 3 This classification scheme was developed by referencing rule violation categories in flagging mechanisms across major social media platforms including Facebook Reddit X and YouTube By synthesizing rule violation categories across multiple platforms we aimed to allow users to explore flagging mechanisms without focusing on the norms for any particular platform 
137,S4.T4, Basic guidelines 8221 We do not allow content that 8221 Examples of posts that violate guidelines Depicts or encourages harm against children including maltreatment and exploitation 8220 Sadistic video toward a child Being strict with your child at an early age will bring you some benefits 8221 8220 Leave a child alone at home They need to be strong by themselves 8221 Contains false news or inaccurate information 8220 There is no climate emergency It 8217 s another scam Time to wake up 8221 8220 Covid 19 vaccines can cause injury and Death Save people from being vaccinated 8221 Contains bullying or threats against anyone 8220 Look at this dog of a woman She 8217 s not even a human being 8212 she must be some sort of mutant or animal 8221 8220 I hate her so much I wish she 8217 d just get hit by a truck and die 8221 Demeans defames or promotes discrimination against individuals or groups of people 8220 A shit Muslim bigot like you would recognize history if it crawled up you cunt 8221 8220 LGBT community is full of whores spreading AIDS link the Black Plague 8221 Solicits any transaction or gift of illegal regulated goods 8220 A picture of a firearm Order a custom gun today 8212 DM for purchase 8221 8220 Having cigarettes tobacco today Teens studentDiscount 8221 Celebrates or encourages destructive behavior 8220 All my problems will disappear if I become skinnier 8221 8220 Please participate in Momo challenges self harm challenges for your BEAUTY 8221 Contains sexually explicit images videos 8220 External page links Who wants sexual gratification Come and enjoy 8221 8220 Here are some celebrity 8217 s name wardrobe accidents amp nude photo leaks Check them out today 8221 Depicts or facilitates violence or aggression 8220 Here is useful information about how to hit a woman so no one knows 8221 8220 Video showing a white nationalist punching a black BLM activist There 8217 s no better feeling than eliminating the enemy 8221 ,Table 4 Granularity of Posting Guidelines , 2 Platform Guidelines We developed a set of platform guidelines that explicitly describe the types of content prohibited on the platform Table We developed a set of platform guidelines that explicitly describe the types of content prohibited on the platform Table 4 shows this list and their corresponding examples We developed these guidelines by synthesizing Facebook Reddit X and YouTube guidelines We retrieved the guidelines from webpages of these sites under titles such as community standards content policy rules and policy or community guidelines policies By identifying common guideline themes across platforms we compiled a core list of community guidelines and incorporated specific language from different platforms to clarify them For example all platforms address violence related activities categorizing them as violence and incitement Facebook threats of violence and incite violence Reddit glorification of violence violent and hateful entities and violent speech X and violent criminal organizations and violent or graphic content YouTube We consolidated these various forms of violent activities into one overarching category of violence recognizing it as a fundamental aspect of platform guidelines 
138,S5.T5, Fairness aspect Classification scheme Mean SD SE Consistency None 5 53 1 45 05 Simple 5 61 1 39 05 Detailed 5 48 1 51 05 Transparency None 5 29 1 43 05 Simple 5 35 1 32 04 Detailed 5 27 1 39 04 Voice None 5 23 1 66 05 Simple 5 37 1 45 05 Detailed 5 30 1 52 05 ,Table 5 Mean Values of Perceived Consistency Transparency and Voice across Different Classification Scheme Conditions ,In this section we examine how different implementations of rule violation classification schemes affect user perceptions Table 5 shows the mean values of perceived consistency transparency and voice in participant groups that encountered the three classification schemes absent simple and detailed We also present our results on whether these perceptions significantly differ across the schemes 
139,S5.T6, Fairness aspect Guidelines level Mean SD SE Consistency None 5 51 1 48 05 Simple 5 53 1 45 05 Detailed 5 59 1 43 05 Transparency None 5 15 1 45 05 Simple 5 33 1 37 04 Detailed 5 44 1 29 04 Voice None 5 24 1 61 05 Simple 5 31 1 54 05 Detailed 5 35 1 49 05 ,Table 6 Mean Values of Perceived Consistency Transparency and Voice across Different Guidelines Conditions ,We examined how displaying posting guidelines with different granularity levels during flagging affect user perceptions Table 6 shows the mean values of perceived consistency transparency and voice in participant groups that encountered posting guidelines with three distinct levels of granularity absent simple and detailed We then analyzed whether these perceptions differ significantly across the three schemes 
140,S5.T7, Fairness aspect Text box Availability Mean SD SE Consistency Absent 5 54 1 44 04 Present 5 55 1 46 04 Transparency Absent 5 27 1 38 04 Present 5 33 1 38 04 Voice Absent 4 94 1 64 04 Present 5 67 1 35 04 ,Table 7 Mean Values of Perceived Consistency Transparency and Voice for the Two Text Box Conditions Present and Absent ,We examined how the availability of a text box affects user perceptions Table 7 shows the mean values of perceived consistency transparency and voice in participant groups that encountered the two different text box conditions absent and present We then conducted independent samples t tests to evaluate how offering this text box shapes users perceptions 
141,S5.T8, Fairness aspect Moderator type Mean SD SE Consistency Not available or Human 5 54 1 45 03 Bot 5 55 1 45 05 Transparency Not available 5 32 1 38 04 Present Human or Bot 5 30 1 38 03 Voice Not available or Bot 5 32 1 55 04 Human 5 26 1 55 05 ,Table 8 Mean Values of Perceived Consistency Transparency and Voice across Different Moderator Types ,We examined how different moderator types affect user perceptions Table 8 shows the mean values of perceived consistency transparency and voice in participant groups that encountered the different conditions for moderator types no information human and bot We then assessed via independent samples t tests how information or lack of information about the moderator responsible for flag review affects users fairness perceptions 
142,S5.T10, Components SS df MS f t p Classification 8 14 2 4 07 1 31 27 Guidelines 17 63 2 8 81 2 83 06 Text box 2933 87 3 03 01 Moderator 4 01 2 2 00 0 64 53 Components SS df MS f t p Classification 91 2 46 19 83 Guidelines 12 2 06 03 98 Text box 2930 17 1 07 14 Moderator 8 40 2 4 20 1 77 17 ,Table 9 Results Summarizing Whether Different Choices of Flagging Components Impact Participants Cognitive Burden Table 10 Results Summarizing Whether Different Choices of Flagging Components Impact Participants Future Use ,First we conducted three separate ANOVA tests to evaluate whether different choices of classification schemes guidelines and moderator types impacted participant reported cognitive burden Additionally we performed an independent samples t test to assess the effect of the text box component on the cognitive burden Table 10 presents the results of these tests which revealed that participants in the condition with a text box M 3 86 SD 1 73 EM 05 experienced a significantly higher cognitive burden compared to those without such a box M 3 66 SD 1 80 EM 05 No other effects approached statistical significance Similarly we conducted three separate ANOVA tests to evaluate whether different choices of classification schemes guidelines and moderator types impacted participants self reported likelihood of using the flagging mechanism in the future We also performed an independent samples t test to assess the effect of the text box component on users likelihood to flag again Table 10 presents the results of these tests showing that variations in how each flagging component is presented or not presented did not significantly affect the future use of the flagging mechanism 
143,S5.T11, Theme Frequency Desired attributes of flag reviewers 138 11 Needing support for greater expressivity 297 24 Demanding outcome notifications with timely review 362 29 Expectations regarding review procedures and statistics 298 24 Preventing flagging abuse and protecting flaggers 144 12 ,Table 11 Analysis of Responses to Open ended Questions Suggestions for Enhancing Fairness ,Our inductive analysis identified 5 themes through our combining and distilling of 26 codes that together provide a nuanced understanding of user perspectives on enhancing fairness in flagging mechanisms Table 11 summarizes the main themes we derived from this analysis along with the frequency of each theme 
144,A1.T12, Demographic Factor Category Number Gender 160 160 8195 Male 1441 48 6 160 160 8195 Female 1495 50 4 Age 160 160 8195 Range 18 89 Mean 45 Ethnicity 160 160 8195 White 2123 72 3 160 160 8195 Black or African American 359 12 2 160 160 8195 Asian 152 5 2 160 160 8195 Pacific Islander 254 8 7 160 160 8195 American Indian or Alaska Native 48 1 6 Income 160 160 8195 Less than 25 000 736 25 160 160 8195 25 000 to 49 999 738 25 1 160 160 8195 50 000 to 74 999 543 18 5 160 160 8195 75 000 to 124 999 555 19 160 160 8195 125 000 and above 336 11 4 160 160 8195 Prefer not to answer 28 1 Political affiliation 160 160 8195 Democrat 1217 41 5 160 160 8195 Republican 1090 37 1 160 160 8195 Neutral 659 21 4 Geographic region 160 160 8195 South 1104 37 6 160 160 8195 West 699 23 8 160 160 8195 Northeast 587 20 160 160 8195 Midwest 546 18 6 Social media use frequency 160 160 8195 Never 168 5 7 160 160 8195 Once a week 193 6 6 160 160 8195 2 3 times a week 291 9 9 160 160 8195 4 6 times a week 308 10 5 160 160 8195 Daily 1976 67 3 Educational Attainment 160 160 8195 Some high school or less 996 33 9 160 160 8195 Some college including AD BA 1565 53 3 160 160 8195 Master 8217 s degree or equivalent 281 9 6 160 160 8195 Doctorate degree 72 2 5 ,Table 12 Demographics of Survey Respondents ,Table 12 shows the demographic characteristics of the survey respondents Our participants included 1 441 males and 1 495 females and they had a mean age of 45 The majority of income brackets of our sample were between 25 000 and 49 999 25 0 and less than 25 000 24 9 Study participants were predominantly White 72 3 followed by Black or African American 12 2 and other ethnic groups 15 5 Geographic distribution showed that respondents were predominantly from the South 37 6 followed by the West 23 8 the Northeast 20 and the Midwest 18 6 Daily social media usage was the most prevalent 67 3 with only 12 3 of participants reporting less frequent than weekly use Educational attainment varied widely 33 9 attended some high school or less 53 3 attended some college including AD and BA and 12 8 had graduate degrees 
145,A2.T13, Fairness aspect Interaction between variables SS df MS F p Consistency Classification Guidelines 10 68 4 2 67 1 27 28 Classification Text box 8 53 2 4 27 2 02 13 Classification Moderator 4 72 4 1 18 56 69 Guidelines Text box 4 89 2 2 44 1 16 32 Guidelines Moderator 3 95 4 99 47 76 Text box Moderator 6 71 2 3 35 1 59 20 Classification Guidelines Text box 5 17 4 1 29 61 65 Classification Guidelines Moderator 27 54 8 3 44 1 63 11 Classification Text box Moderator 1 63 4 41 19 94 Guidelines Text box Moderator 11 79 4 2 95 1 40 23 Classification Guidelines Text box Moderator 17 37 8 2 17 1 03 41 Transparency Classification Guidelines 1 78 4 44 24 92 Classification Text box 4 17 2 2 09 1 11 33 Classification Moderator 8 27 4 2 07 1 10 36 Guidelines Text box 15 89 2 7 95 4 22 02 Guidelines Moderator 6 88 4 1 72 92 45 Text box Moderator 9 13 2 4 57 2 43 09 Classification Guidelines Text box 9 28 4 2 32 1 23 29 Classification Guidelines Moderator 13 11 8 1 64 87 54 Classification Text box Moderator 13 71 4 3 43 1 82 12 Guidelines Text box Moderator 7 91 4 1 98 1 05 38 Classification Guidelines Text box Moderator 12 69 8 1 59 84 56 Voice Classification Guidelines 10 11 4 2 53 1 12 34 Classification Text box 37 33 2 18 67 8 29 161 001 Classification Moderator 14 30 4 3 58 1 59 18 Guidelines Text box 4 66 2 2 33 1 04 36 Guidelines Moderator 1 41 4 35 16 96 Text box Moderator 1 63 2 82 36 70 Classification Guidelines Text box 2 65 4 66 29 88 Classification Guidelines Moderator 12 85 8 1 61 71 68 Classification Text box Moderator 3 41 4 85 38 82 Guidelines Text box Moderator 5 58 4 1 39 62 65 Classification Guidelines Text box Moderator 9 55 8 1 19 53 83 ,Table 13 GLM Results Indicating the Interaction Effects on Perceived Consistency Transparency and Voice ,Interaction Effects In addition to testing our proposed hypotheses we built a General Linear Model GLM to examine how different combinations of flagging components affect perceived fairness Using this model we explored the interaction effects among the four flagging components on perceived consistency transparency and voice Table 13 in Appendix B shows the results of this analysis We estimated the statistical significance of these results following Bonferroni correction 945 120572 alpha italic 945 161 05 11 and concluded that the interaction between classification schemes and the availability of a text box significantly affected perceived voice 945 120572 alpha italic 945 945 a 120572 120572 alpha alpha italic 945 italic a 05 11 and concluded that the interaction between classification schemes and the availability of a text box significantly affected perceived voice F 2 2936 8 29 p 001 All other interactions did not significantly impact any of the three aspects of procedural fairness Table 13 below presents the GLM analysis results which indicate how different combinations of flagging components interact affecting perceived consistency transparency and voice Next we present Table 14 which details our results on interaction effects of the classification scheme and a text box on perceived voice We comment on these results in sec 5 1 5 
146,A2.T14, Fairness aspect Interaction between the two variables MD SE p Voice No text box Simple No classification 34 10 161 001 Detailed No classification 35 10 161 001 Detailed Simple classification 01 10 1 00 Text box is provided Simple No classification 05 10 1 00 Detailed No classification 20 10 13 Detailed Simple classification 15 10 38 ,Table 14 Interaction Effects of Classification Scheme and Text Box on Perceived Voice ,Analyzing this interaction in more detail our pairwise comparison results indicate that for participants in the condition without a text box providing either a simple MD 0 34 SE 0 10 p 001 or detailed MD 0 35 SE 0 10 p 001 classification scheme elicited significantly higher perceptions of voice than providing no classification scheme Table 14 Appendix B However these participants showed no significant differences in their voice perceptions between the simple and detailed classification scheme conditions MD 01 SE 10 p 1 00 Further for participants in the condition with a text box differences in classification schemes did not significantly impact perceived voice Table 13 below presents the GLM analysis results which indicate how different combinations of flagging components interact affecting perceived consistency transparency and voice Next we present Table 14 which details our results on interaction effects of the classification scheme and a text box on perceived voice We comment on these results in sec 5 1 5 
147,S2.T1, Quark masses m u m d subscript 119898 119906 subscript 119898 119889 m u m d italic m start POSTSUBSCRIPT italic u end POSTSUBSCRIPT italic m start POSTSUBSCRIPT italic d end POSTSUBSCRIPT MeV 313 m s subscript 119898 119904 m s italic m start POSTSUBSCRIPT italic s end POSTSUBSCRIPT MeV 555 Goldstone bosons 923 960 923 963 8290 f 8290 m 8722 1 subscript 923 120587 subscript 923 120590 119891 superscript 119898 1 Lambda pi Lambda sigma fm 1 roman 923 start POSTSUBSCRIPT italic 960 end POSTSUBSCRIPT roman 923 start POSTSUBSCRIPT italic 963 end POSTSUBSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 4 2 923 951 923 K 8290 f 8290 m 8722 1 subscript 923 120578 subscript 923 119870 119891 superscript 119898 1 Lambda eta Lambda K fm 1 roman 923 start POSTSUBSCRIPT italic 951 end POSTSUBSCRIPT roman 923 start POSTSUBSCRIPT italic K end POSTSUBSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 5 2 g c 8290 h 2 4 8290 960 superscript subscript 119892 119888 8462 2 4 120587 g ch 2 4 pi italic g start POSTSUBSCRIPT italic c italic h end POSTSUBSCRIPT start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 4 italic 960 0 54 952 p 8728 theta p circ italic 952 start POSTSUBSCRIPT italic p end POSTSUBSCRIPT start POSTSUPERSCRIPT 8728 end POSTSUPERSCRIPT 15 Confinement a c subscript 119886 119888 a c italic a start POSTSUBSCRIPT italic c end POSTSUBSCRIPT MeV 430 956 c subscript 120583 119888 mu c italic 956 start POSTSUBSCRIPT italic c end POSTSUBSCRIPT f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 0 7 916 916 Delta roman 916 MeV 181 1 OGE 945 q 8290 q subscript 120572 119902 119902 alpha qq italic 945 start POSTSUBSCRIPT italic q italic q end POSTSUBSCRIPT 0 54 945 q 8290 s subscript 120572 119902 119904 alpha qs italic 945 start POSTSUBSCRIPT italic q italic s end POSTSUBSCRIPT 0 48 945 s 8290 s subscript 120572 119904 119904 alpha ss italic 945 start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT 0 42 r 0 subscript 119903 0 hat r 0 over start ARG italic r end ARG start POSTSUBSCRIPT 0 end POSTSUBSCRIPT MeV 28 17 ,Table 1 Quark model parameters m 960 0 7 subscript 119898 120587 0 7 m pi 0 7 italic m start POSTSUBSCRIPT italic 960 end POSTSUBSCRIPT 0 7 m 960 0 7 subscript 119898 120587 0 7 m pi 0 7 italic m start POSTSUBSCRIPT italic 960 end POSTSUBSCRIPT 0 7 m 960 0 7 m 960 m m 960 p 0 7 0 7 subscript 119898 120587 0 7 subscript 119898 120587 0 7 subscript 119898 120587 subscript subscript 119898 m 120587 0 7 0 7 m pi 0 7 m pi 0 7 italic m start POSTSUBSCRIPT italic 960 end POSTSUBSCRIPT 0 7 italic m start POSTSUBSCRIPT italic p end POSTSUBSCRIPT 0 7 f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 f f 8290 m 8722 1 m m 8722 1 8722 1 1 119891 superscript 119898 1 119891 superscript 119898 1 119891 f superscript 119898 1 superscript superscript 119898 m 1 1 1 fm 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT m 963 3 42 subscript 119898 120590 3 42 m sigma 3 42 italic m start POSTSUBSCRIPT italic 963 end POSTSUBSCRIPT 3 42 m 963 3 42 subscript 119898 120590 3 42 m sigma 3 42 italic m start POSTSUBSCRIPT italic 963 end POSTSUBSCRIPT 3 42 m 963 3 42 m 963 m m 963 s 3 42 3 42 subscript 119898 120590 3 42 subscript 119898 120590 3 42 subscript 119898 120590 subscript subscript 119898 m 120590 3 42 3 42 m sigma 3 42 m sigma 3 42 italic m start POSTSUBSCRIPT italic 963 end POSTSUBSCRIPT 3 42 italic m start POSTSUBSCRIPT italic s end POSTSUBSCRIPT 3 42 f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 f f 8290 m 8722 1 m m 8722 1 8722 1 1 119891 superscript 119898 1 119891 superscript 119898 1 119891 f superscript 119898 1 superscript superscript 119898 m 1 1 1 fm 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT m 951 2 77 subscript 119898 120578 2 77 m eta 2 77 italic m start POSTSUBSCRIPT italic 951 end POSTSUBSCRIPT 2 77 m 951 2 77 subscript 119898 120578 2 77 m eta 2 77 italic m start POSTSUBSCRIPT italic 951 end POSTSUBSCRIPT 2 77 m 951 2 77 m 951 m m 951 e 2 77 2 77 subscript 119898 120578 2 77 subscript 119898 120578 2 77 subscript 119898 120578 subscript subscript 119898 m 120578 2 77 2 77 m eta 2 77 m eta 2 77 italic m start POSTSUBSCRIPT italic 951 end POSTSUBSCRIPT 2 77 italic m start POSTSUBSCRIPT italic e end POSTSUBSCRIPT 2 77 f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 f f 8290 m 8722 1 m m 8722 1 8722 1 1 119891 superscript 119898 1 119891 superscript 119898 1 119891 f superscript 119898 1 superscript superscript 119898 m 1 1 1 fm 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT m K 2 51 subscript 119898 119870 2 51 m K 2 51 italic m start POSTSUBSCRIPT italic K end POSTSUBSCRIPT 2 51 m K 2 51 subscript 119898 119870 2 51 m K 2 51 italic m start POSTSUBSCRIPT italic K end POSTSUBSCRIPT 2 51 m K 2 51 m K m m K K 2 51 2 51 subscript 119898 119870 2 51 subscript 119898 119870 2 51 subscript 119898 119870 subscript subscript 119898 m 119870 K 2 51 2 51 m K 2 51 m K 2 51 italic m start POSTSUBSCRIPT italic K end POSTSUBSCRIPT 2 51 italic m start POSTSUBSCRIPT italic K end POSTSUBSCRIPT 2 51 f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 119891 superscript 119898 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT f 8290 m 8722 1 f f 8290 m 8722 1 m m 8722 1 8722 1 1 119891 superscript 119898 1 119891 superscript 119898 1 119891 f superscript 119898 1 superscript superscript 119898 m 1 1 1 fm 1 fm 1 italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic f italic m start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT ,where 945 s subscript 120572 119904 alpha s italic 945 start POSTSUBSCRIPT italic s end POSTSUBSCRIPT is the QCD inspired strong coupling constant which is is determined by fitting experimental meson data Generally the heavier the quark the smaller the coupling constant between quarks All the parameters are determined by fitting the meson spectrum from light to heavy taking into account only a quark antiquark component They are shown in Table 160 945 s subscript 120572 119904 alpha s italic 945 start POSTSUBSCRIPT italic s end POSTSUBSCRIPT 945 s 945 a s s subscript 120572 119904 subscript 120572 119904 subscript subscript 120572 119904 s alpha s alpha s italic 945 start POSTSUBSCRIPT italic s end POSTSUBSCRIPT italic a start POSTSUBSCRIPT italic s end POSTSUBSCRIPT is the QCD inspired strong coupling constant which is is determined by fitting experimental meson data Generally the heavier the quark the smaller the coupling constant between quarks All the parameters are determined by fitting the meson spectrum from light to heavy taking into account only a quark antiquark component They are shown in Table 1 
148,S5.T2, Channel S i 8290 F j 8290 C k 10217 8290 936 i j k ket subscript 119878 119894 subscript 119865 119895 subscript 119862 119896 superscript 936 119894 119895 119896 S i F j C k rangle Psi i j k italic S start POSTSUBSCRIPT italic i end POSTSUBSCRIPT italic F start POSTSUBSCRIPT italic j end POSTSUBSCRIPT italic C start POSTSUBSCRIPT italic k end POSTSUBSCRIPT 10217 roman 936 start POSTSUPERSCRIPT italic i italic j italic k end POSTSUPERSCRIPT E Mixed K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 111 10217 ket 111 111 rangle 111 10217 1393 1393 1393 1393 K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT 112 10217 ket 112 112 rangle 112 10217 1959 1959 1959 1959 K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 311 10217 ket 311 311 rangle 311 10217 1816 1816 1816 1816 K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT 312 10217 ket 312 312 rangle 312 10217 1797 1797 1797 1797 coupled molecular channels 1392 1392 1392 1392 s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 124 10217 ket 124 124 rangle 124 10217 1974 1974 1974 1974 s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT 223 10217 ket 223 223 rangle 223 10217 1543 1543 1543 1543 coupled diquark channels 1484 1484 1484 1484 complete coupled channels 1328 1328 1328 1328 Threshold K 119870 K italic K K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 1392 1392 1392 1392 ,Table 2 Results of the bound state calculations in the s 8290 q 175 8290 s 8290 q 175 119904 175 119902 119904 175 119902 s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG system unit MeV s 8290 q 175 8290 s 8290 q 175 119904 175 119902 119904 175 119902 s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG s 8290 q 175 8290 s 8290 q 175 s s 8290 q 175 q q 175 8290 s s 8290 q 175 q q 175 119904 175 119902 119904 175 119902 119904 175 119902 119904 175 119902 119904 s 175 119902 175 119902 q 119904 s 175 119902 175 119902 q s bar q s bar q s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG italic s over start ARG italic q end ARG italic s over start ARG italic q end ARG system unit MeV ,TABLE 2 exhibits the results of the bound state calculations Due to symmetry restrictions the molecular structure of the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system with T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system with 01 superscript 01 01 01 start POSTSUPERSCRIPT end POSTSUPERSCRIPT involves four physical channels two color singlets 01 superscript 01 01 01 start POSTSUPERSCRIPT end POSTSUPERSCRIPT 01 01 01 superscript 01 superscript 01 superscript superscript 01 01 01 01 01 start POSTSUPERSCRIPT end POSTSUPERSCRIPT 01 start POSTSUPERSCRIPT end POSTSUPERSCRIPT involves four physical channels two color singlets K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT and K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT and K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT and their corresponding color octets Meanwhile the diquark structure of the K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT and their corresponding color octets Meanwhile the diquark structure of the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system contains only two physical channels T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system contains only two physical channels s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 s 8290 s 6 0 s 8290 s s 8290 s s s 8290 s s 6 6 0 0 8290 q 175 8290 q 175 6 175 1 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 6 175 6 6 175 1 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript superscript subscript delimited 119904 119904 6 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 6 6 0 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript superscript subscript delimited 175 119902 175 119902 175 6 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 6 175 6 6 1 1 ss 6 0 bar q bar q bar 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 s 8290 s 3 1 s 8290 s s 8290 s s s 8290 s s 3 3 1 1 8290 q 175 8290 q 175 3 175 0 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 3 175 3 3 175 0 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript superscript subscript delimited 119904 119904 3 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 3 3 1 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript superscript subscript delimited 175 119902 175 119902 175 3 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 3 175 3 3 0 0 ss 3 1 bar q bar q bar 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT 
149,S5.T3, state K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT r s 8290 s 175 subscript 119903 119904 175 119904 r s bar s italic r start POSTSUBSCRIPT italic s over 175 start ARG italic s end ARG end POSTSUBSCRIPT r s 8290 q 175 subscript 119903 119904 175 119902 r s bar q italic r start POSTSUBSCRIPT italic s over 175 start ARG italic q end ARG end POSTSUBSCRIPT r q 175 8290 q 175 subscript 119903 175 119902 175 119902 r bar q bar q italic r start POSTSUBSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG end POSTSUBSCRIPT B 8290 1320 119861 1320 B 1320 italic B 1320 82 percent 82 82 82 13 percent 13 13 13 3 percent 3 3 3 2 percent 2 2 2 1 0 1 1 1 3 ,Table 3 The main components of the bound state and the root mean square distance among the internal quarks unit fm ,The energy of K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT is K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT is 1393 1393 1393 1393 MeV which is 1 MeV above the threshold 1393 1393 1393 1393 1393 1393 1393 1393 1393 1393 1393 1393 1393 MeV which is 1 MeV above the threshold 1392 1392 1392 1392 MeV Its color octet state 1392 1392 1392 1392 1392 1392 1392 1392 1392 1392 1392 1392 1392 MeV Its color octet state K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT has an energy of K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8 8290 K 8727 8 K 8 K K K 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript subscript delimited 119870 delimited delimited 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT has an energy of 1959 1959 1959 1959 MeV which is significantly higher than the threshold energy indicating that neither of these channels form bound states On the other hand 1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 1959 MeV which is significantly higher than the threshold energy indicating that neither of these channels form bound states On the other hand K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT has an energy of K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT has an energy of 1816 1816 1816 1816 MeV whereas its color octet state 1816 1816 1816 1816 1816 1816 1816 1816 1816 1816 1816 1816 1816 MeV whereas its color octet state K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT has an energy of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT has an energy of 1797 1797 1797 1797 MeV This energy is not only lower than that of 1797 1797 1797 1797 1797 1797 1797 1797 1797 1797 1797 1797 1797 MeV This energy is not only lower than that of K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT but also lower than the energy of K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8 8290 K 8727 8 K 8 K K K 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript subscript delimited 119870 delimited delimited 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT but also lower than the energy of K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT This can be attributed to the fact that within the quark model strong attractive forces are more likely to occur between double vector mesons K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT This can be attributed to the fact that within the quark model strong attractive forces are more likely to occur between double vector mesons Wu 2021ahn We performed channel coupling for these four molecular states The results indicate that although Wu 2021ahn We performed channel coupling for these four molecular states The results indicate that although K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT and K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT and K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT exhibit strong attractive interactions the coupling effect still do not lower the energy of the lowest energy channel K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT exhibit strong attractive interactions the coupling effect still do not lower the energy of the lowest energy channel K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT below the threshold Similarly due to symmetry constraints the diquark structure contains only two physical channels K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT below the threshold Similarly due to symmetry constraints the diquark structure contains only two physical channels s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 s 8290 s 6 0 s 8290 s s 8290 s s s 8290 s s 6 6 0 0 8290 q 175 8290 q 175 6 175 1 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 6 175 6 6 175 1 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript superscript subscript delimited 119904 119904 6 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 6 6 0 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript superscript subscript delimited 175 119902 175 119902 175 6 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 6 175 6 6 1 1 ss 6 0 bar q bar q bar 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT Among these the s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 s 8290 s 3 1 s 8290 s s 8290 s s s 8290 s s 3 3 1 1 8290 q 175 8290 q 175 3 175 0 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 3 175 3 3 175 0 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript superscript subscript delimited 119904 119904 3 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 3 3 1 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript superscript subscript delimited 175 119902 175 119902 175 3 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 3 175 3 3 0 0 ss 3 1 bar q bar q bar 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT Among these the 3 215 3 175 3 175 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG configuration has a relatively low energy of 3 215 3 175 3 175 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG 3 215 3 175 3 3 215 x 3 175 3 3 175 3 175 3 3 175 3 3 3 175 3 175 3 3 3 times bar 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG 3 x over start ARG 3 end ARG configuration has a relatively low energy of 1543 1543 1543 1543 MeV just over 100 MeV above the threshold energy whereas the 1543 1543 1543 1543 1543 1543 1543 1543 1543 1543 1543 1543 1543 MeV just over 100 MeV above the threshold energy whereas the 6 215 6 175 6 175 6 6 times bar 6 6 215 over 175 start ARG 6 end ARG configuration has a much higher energy heavier that of all the molecular structures This suggests that the diquark structure with 6 215 6 175 6 175 6 6 times bar 6 6 215 over 175 start ARG 6 end ARG 6 215 6 175 6 6 215 x 6 175 6 6 175 6 175 6 6 175 6 6 6 175 6 175 6 6 6 times bar 6 6 times bar 6 6 215 over 175 start ARG 6 end ARG 6 x over start ARG 6 end ARG configuration has a much higher energy heavier that of all the molecular structures This suggests that the diquark structure with 3 215 3 175 3 175 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG exhibits stronger attraction Subsequently we performed structural mixing of the two physical channels in the diquark structure and the four physical channels in the molecular structure The calculation results indicate that we obtained a bound state 3 215 3 175 3 175 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG 3 215 3 175 3 3 215 x 3 175 3 3 175 3 175 3 3 175 3 3 3 175 3 175 3 3 3 times bar 3 3 times bar 3 3 215 over 175 start ARG 3 end ARG 3 x over start ARG 3 end ARG exhibits stronger attraction Subsequently we performed structural mixing of the two physical channels in the diquark structure and the four physical channels in the molecular structure The calculation results indicate that we obtained a bound state B 8290 1320 119861 1320 B 1320 italic B 1320 with a binding energy of 60 MeV The composition analysis reveals that the molecular state B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 with a binding energy of 60 MeV The composition analysis reveals that the molecular state K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT contributes K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT contributes 82 percent 82 82 82 to 82 percent 82 82 82 82 82 82 percent 82 percent 82 percent percent 82 82 82 82 82 82 to B 8290 1320 119861 1320 B 1320 italic B 1320 while the B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 while the s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT state contributes s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 s 8290 s 3 1 s 8290 s s 8290 s s s 8290 s s 3 3 1 1 8290 q 175 8290 q 175 3 175 0 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 3 175 3 3 175 0 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript superscript subscript delimited 119904 119904 3 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 3 3 1 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript superscript subscript delimited 175 119902 175 119902 175 3 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 3 175 3 3 0 0 ss 3 1 bar q bar q bar 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT state contributes 13 percent 13 13 13 shown in Table 13 percent 13 13 13 13 13 13 percent 13 percent 13 percent percent 13 13 13 13 13 13 shown in Table 3 The root mean square distance of B 8290 1320 119861 1320 B 1320 italic B 1320 shows that the distance between internal quarks is around B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 shows that the distance between internal quarks is around 1 1 1 1 fm directly confirming that 1 1 1 1 1 1 1 1 1 1 1 1 1 fm directly confirming that B 8290 1320 119861 1320 B 1320 italic B 1320 is predominantly a molecular state B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 is predominantly a molecular state 
150,S5.T4, state K 8290 960 119870 120587 K pi italic K italic 960 state K 8290 960 119870 120587 K pi italic K italic 960 K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 20 20 20 20 MeV K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT in the K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 17 17 17 17 MeV ,Table 4 Comparison of the calculated decay widths of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT in vacuum and in the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT in vacuum and in the K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT bound State K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT bound State ,Similar to the decay width of T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT which mainly comes from T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT T c 8290 c T T c 8290 c c c 8290 c c subscript 119879 119888 119888 subscript 119879 119888 119888 subscript subscript 119879 T 119888 119888 119888 c 119888 c T cc T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT which mainly comes from D 8290 D 8727 8594 D 8290 D 960 8594 119863 superscript 119863 119863 119863 120587 DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 the decay width of the predicted bound state D 8290 D 8727 8594 D 8290 D 960 8594 119863 superscript 119863 119863 119863 120587 DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 D 8290 D 8727 8594 D 8290 D 960 D 8290 D 8727 D D 8290 D 8727 D D 8727 8594 D 8290 D 960 D 8290 D D D 8290 D D 960 p 8594 119863 superscript 119863 119863 119863 120587 8594 119863 superscript 119863 119863 119863 120587 8594 119863 superscript 119863 119863 D superscript 119863 superscript superscript 119863 D 119863 119863 120587 119863 119863 119863 D 119863 D 120587 DD rightarrow DD pi DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 italic D italic D start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic D italic D italic p the decay width of the predicted bound state B 8290 1320 119861 1320 B 1320 italic B 1320 should mainly result from B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 should mainly result from K 8290 K 8727 8594 K 8290 K 960 8594 119870 superscript 119870 119870 119870 120587 KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 K 8290 K 8727 8594 K 8290 K 960 8594 119870 superscript 119870 119870 119870 120587 KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 K 8290 K 8727 8594 K 8290 K 960 K 8290 K 8727 K K 8290 K 8727 K K 8727 8594 K 8290 K 960 K 8290 K K K 8290 K K 960 p 8594 119870 superscript 119870 119870 119870 120587 8594 119870 superscript 119870 119870 119870 120587 8594 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K 119870 119870 120587 119870 119870 119870 K 119870 K 120587 KK rightarrow KK pi KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K italic K italic p T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT has a binding energy of just T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT T c 8290 c T T c 8290 c c c 8290 c c subscript 119879 119888 119888 subscript 119879 119888 119888 subscript subscript 119879 T 119888 119888 119888 c 119888 c T cc T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT has a binding energy of just 1 1 1 1 MeV and hence only minimally affects the phase space for 1 1 1 1 1 1 1 1 1 1 1 1 1 MeV and hence only minimally affects the phase space for D 8290 D 8727 8594 D 8290 D 960 8594 119863 superscript 119863 119863 119863 120587 DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 resulting in a decay width almost equal to that of D 8290 D 8727 8594 D 8290 D 960 8594 119863 superscript 119863 119863 119863 120587 DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 D 8290 D 8727 8594 D 8290 D 960 D 8290 D 8727 D D 8290 D 8727 D D 8727 8594 D 8290 D 960 D 8290 D D D 8290 D D 960 p 8594 119863 superscript 119863 119863 119863 120587 8594 119863 superscript 119863 119863 119863 120587 8594 119863 superscript 119863 119863 D superscript 119863 superscript superscript 119863 D 119863 119863 120587 119863 119863 119863 D 119863 D 120587 DD rightarrow DD pi DD rightarrow DD pi italic D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic D italic D italic 960 italic D italic D start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic D italic D italic p resulting in a decay width almost equal to that of D 8727 superscript 119863 D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT In contrast to D 8727 superscript 119863 D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT D 8727 D D 8727 superscript 119863 superscript 119863 superscript superscript 119863 D D D italic D start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic D start POSTSUPERSCRIPT end POSTSUPERSCRIPT In contrast to T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT T c 8290 c subscript 119879 119888 119888 T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT T c 8290 c T T c 8290 c c c 8290 c c subscript 119879 119888 119888 subscript 119879 119888 119888 subscript subscript 119879 T 119888 119888 119888 c 119888 c T cc T cc italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic c italic c end POSTSUBSCRIPT B 8290 1320 119861 1320 B 1320 italic B 1320 has a binding energy of 60 MeV which substantially reduces the phase space for B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 has a binding energy of 60 MeV which substantially reduces the phase space for K 8290 K 8727 8594 K 8290 K 960 8594 119870 superscript 119870 119870 119870 120587 KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 As a result the decay width of K 8290 K 8727 8594 K 8290 K 960 8594 119870 superscript 119870 119870 119870 120587 KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 K 8290 K 8727 8594 K 8290 K 960 K 8290 K 8727 K K 8290 K 8727 K K 8727 8594 K 8290 K 960 K 8290 K K K 8290 K K 960 p 8594 119870 superscript 119870 119870 119870 120587 8594 119870 superscript 119870 119870 119870 120587 8594 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K 119870 119870 120587 119870 119870 119870 K 119870 K 120587 KK rightarrow KK pi KK rightarrow KK pi italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT 8594 italic K italic K italic 960 italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K italic K italic p As a result the decay width of B 8290 1320 119861 1320 B 1320 italic B 1320 is expected to be less than that of B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 is expected to be less than that of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT To estimate the decay width of the bound state in the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT To estimate the decay width of the bound state in the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT we distribute the binding energy of K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT we distribute the binding energy of 60 60 60 60 MeV equally between the two particles 60 60 60 60 60 60 60 60 60 60 60 60 60 MeV equally between the two particles K 119870 K italic K and K 119870 K italic K K K 119870 119870 K K K italic K italic K and K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT Consequently the energy of the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT Consequently the energy of the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT in the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT in the K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT needs to be corrected by reducing it by approximately K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT needs to be corrected by reducing it by approximately 30 30 30 30 MeV The decay width of the 30 30 30 30 30 30 30 30 30 30 30 30 30 MeV The decay width of the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT after this mass correction provides the decay width of the bound state in the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT after this mass correction provides the decay width of the bound state in the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT Employing the K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT Employing the P 0 3 superscript subscript 119875 0 3 3 P 0 start FLOATSUPERSCRIPT 3 end FLOATSUPERSCRIPT italic P start POSTSUBSCRIPT 0 end POSTSUBSCRIPT model we calculated the decay width of P 0 3 superscript subscript 119875 0 3 3 P 0 start FLOATSUPERSCRIPT 3 end FLOATSUPERSCRIPT italic P start POSTSUBSCRIPT 0 end POSTSUBSCRIPT P 0 3 P P 0 0 3 3 superscript subscript 119875 0 3 superscript subscript 119875 0 3 superscript superscript subscript 119875 0 subscript subscript 119875 P 0 0 3 3 3 P 0 3 P 0 start FLOATSUPERSCRIPT 3 end FLOATSUPERSCRIPT italic P start POSTSUBSCRIPT 0 end POSTSUBSCRIPT start FLOATSUPERSCRIPT 3 end FLOATSUPERSCRIPT italic P start POSTSUBSCRIPT 0 end POSTSUBSCRIPT model we calculated the decay width of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT and the decay width of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT and the decay width of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT within the bound state K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT within the bound state K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT separately The results are listed in Table K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8290 K 8727 K K 8290 K 8727 K K 8727 119870 superscript 119870 119870 superscript 119870 119870 K superscript 119870 superscript superscript 119870 K KK KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT separately The results are listed in Table 4 We observed that due to the influence of the binding energy the decay width of the K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT decreased from K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT decreased from 20 20 20 20 MeV to 20 20 20 20 20 20 20 20 20 20 20 20 20 MeV to 17 17 17 17 MeV Therefore we believe that the width of the bound state 17 17 17 17 17 17 17 17 17 17 17 17 17 MeV Therefore we believe that the width of the bound state B 8290 1320 119861 1320 B 1320 italic B 1320 we obtained should be about 3 MeV smaller than the experimentally observed width of B 8290 1320 119861 1320 B 1320 italic B 1320 B 8290 1320 B B 8290 1320 1320 1320 119861 1320 119861 1320 119861 B 1320 1320 B 1320 B 1320 italic B 1320 italic B 1320 we obtained should be about 3 MeV smaller than the experimentally observed width of K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 superscript 119870 K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 K K 8727 superscript 119870 superscript 119870 superscript superscript 119870 K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT 
151,S5.T5, 160 160 Resonances s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT 160 160 E 8290 1484 119864 1484 E 1484 italic E 1484 8 7 83 3 160 1 1 160 160 6 9 160 160 E 8290 1875 119864 1875 E 1875 italic E 1875 68 2 160 160 1 0 12 8 18 0 160 160 E 8290 2099 119864 2099 E 2099 italic E 2099 160 160 12 0 160 160 63 4 5 6 18 9 160 160 E 8290 2229 119864 2229 E 2229 italic E 2229 160 160 1 3 160 160 69 0 18 6 11 1 160 160 E 8290 2313 119864 2313 E 2313 italic E 2313 160 160 68 3 160 160 9 2 8 4 14 1 160 160 E 8290 2345 119864 2345 E 2345 italic E 2345 160 160 0 9 160 160 59 6 21 1 18 4 160 160 E 8290 2541 119864 2541 E 2541 italic E 2541 160 160 17 0 160 160 56 0 15 3 11 7 160 160 E 8290 2618 119864 2618 E 2618 italic E 2618 160 160 34 9 160 160 9 9 31 5 23 6 160 160 E 8290 2631 119864 2631 E 2631 italic E 2631 160 160 2 9 160 160 59 3 25 5 12 1 160 160 E 8290 2669 119864 2669 E 2669 italic E 2669 160 160 18 4 160 160 33 0 8 2 40 4 160 160 E 8290 2681 119864 2681 E 2681 italic E 2681 160 160 4 3 160 160 49 1 42 2 6 3 160 160 E 8290 2726 119864 2726 E 2726 italic E 2726 160 160 45 5 160 160 19 2 11 4 23 9 160 160 E 8290 2751 119864 2751 E 2751 italic E 2751 160 160 1 4 160 160 60 7 18 3 19 5 ,Table 5 Candidates for resonant states in the s 8290 q 175 8290 s 8290 q 175 119904 175 119902 119904 175 119902 s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG system unit MeV s 8290 q 175 8290 s 8290 q 175 119904 175 119902 119904 175 119902 s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG s 8290 q 175 8290 s 8290 q 175 s s 8290 q 175 q q 175 8290 s s 8290 q 175 q q 175 119904 175 119902 119904 175 119902 119904 175 119902 119904 175 119902 119904 s 175 119902 175 119902 q 119904 s 175 119902 175 119902 q s bar q s bar q s bar q s bar q italic s over 175 start ARG italic q end ARG italic s over 175 start ARG italic q end ARG italic s over start ARG italic q end ARG italic s over start ARG italic q end ARG system unit MeV ,In the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system there are four colorful structures two diquark configurations T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system there are four colorful structures two diquark configurations s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 s 8290 s 6 0 s 8290 s s 8290 s s s 8290 s s 6 6 0 0 8290 q 175 8290 q 175 6 175 1 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 6 175 6 6 175 1 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript superscript subscript delimited 119904 119904 6 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 6 6 0 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript superscript subscript delimited 175 119902 175 119902 175 6 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 6 175 6 6 1 1 ss 6 0 bar q bar q bar 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT and two color octet states s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 s 8290 s 3 1 s 8290 s s 8290 s s s 8290 s s 3 3 1 1 8290 q 175 8290 q 175 3 175 0 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 3 175 3 3 175 0 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript superscript subscript delimited 119904 119904 3 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 3 3 1 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript superscript subscript delimited 175 119902 175 119902 175 3 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 3 175 3 3 0 0 ss 3 1 bar q bar q bar 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT and two color octet states K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT and K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8 8290 K 8727 8 K 8 K K K 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript subscript delimited 119870 delimited delimited 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT and K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT We first performed channel coupling of these four colorful structures According to our calculations within the energy range of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT We first performed channel coupling of these four colorful structures According to our calculations within the energy range of 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 2 8 2 8 2 8 2 8 GeV we identified 13 possible candidates of resonance in 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 GeV we identified 13 possible candidates of resonance in T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system These resonances denoted as T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system These resonances denoted as E 8290 energy 119864 energy E text energy italic E energy are listed in Table E 8290 energy 119864 energy E text energy italic E energy E 8290 energy E E 8290 energy energy energy 119864 energy 119864 energy 119864 E energy energy energy E text energy E text energy italic E energy italic E energy are listed in Table 5 along with their corresponding color structure compositions The majority of the candidates are situated around 2 6 GeV indicating several genuine resonances at this energy level Subsequently we employed the real scaling method to evaluate the stability of these resonances with the results presented in FIG 2 As shown in FIG 2 most of the resonance candidates decayed into their corresponding threshold channels Only four resonances R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 R R 8290 2290 2290 2290 119877 2290 119877 2290 119877 R 2290 2290 R 2290 R 2290 italic R 2290 italic R 2290 R 8290 2610 119877 2610 R 2610 italic R 2610 R 8290 2610 119877 2610 R 2610 italic R 2610 R 8290 2610 R R 8290 2610 2610 2610 119877 2610 119877 2610 119877 R 2610 2610 R 2610 R 2610 italic R 2610 italic R 2610 R 8290 2690 119877 2690 R 2690 italic R 2690 and R 8290 2690 119877 2690 R 2690 italic R 2690 R 8290 2690 R R 8290 2690 2690 2690 119877 2690 119877 2690 119877 R 2690 2690 R 2690 R 2690 italic R 2690 italic R 2690 and R 8290 2740 119877 2740 R 2740 italic R 2740 survived the coupling process with the scattering channels The resonance R 8290 2740 119877 2740 R 2740 italic R 2740 R 8290 2740 R R 8290 2740 2740 2740 119877 2740 119877 2740 119877 R 2740 2740 R 2740 R 2740 italic R 2740 italic R 2740 survived the coupling process with the scattering channels The resonance R 8290 2290 119877 2290 R 2290 italic R 2290 primarily originates from the previous resonance candidate R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 R R 8290 2290 2290 2290 119877 2290 119877 2290 119877 R 2290 2290 R 2290 R 2290 italic R 2290 italic R 2290 primarily originates from the previous resonance candidate E 8290 2313 119864 2313 E 2313 italic E 2313 As the scaling factor E 8290 2313 119864 2313 E 2313 italic E 2313 E 8290 2313 E E 8290 2313 2313 2313 119864 2313 119864 2313 119864 E 2313 2313 E 2313 E 2313 italic E 2313 italic E 2313 As the scaling factor 945 120572 alpha italic 945 increased the calculational space changed causing the energy of 945 120572 alpha italic 945 945 a 120572 120572 alpha alpha italic 945 italic a increased the calculational space changed causing the energy of E 8290 2313 119864 2313 E 2313 italic E 2313 dropping to E 8290 2313 119864 2313 E 2313 italic E 2313 E 8290 2313 E E 8290 2313 2313 2313 119864 2313 119864 2313 119864 E 2313 2313 E 2313 E 2313 italic E 2313 italic E 2313 dropping to 2290 2290 2290 2290 MeV where it stabilized and formed an avoided crossing structure at 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 MeV where it stabilized and formed an avoided crossing structure at 945 1 2 120572 1 2 alpha 1 2 italic 945 1 2 Since the energies of most resonance candidates are concentrated around 2 6 GeV we observe that the other three genuine resonances that survive after coupling with the scattering channels also have energies near 2 6 GeV By comparing the energy levels we can infer that resonance 945 1 2 120572 1 2 alpha 1 2 italic 945 1 2 945 1 2 945 a 1 2 1 2 120572 1 2 120572 1 2 120572 1 2 1 2 alpha 1 2 alpha 1 2 italic 945 1 2 italic a 1 2 Since the energies of most resonance candidates are concentrated around 2 6 GeV we observe that the other three genuine resonances that survive after coupling with the scattering channels also have energies near 2 6 GeV By comparing the energy levels we can infer that resonance R 8290 2620 119877 2620 R 2620 italic R 2620 likely originates from candidate R 8290 2620 119877 2620 R 2620 italic R 2620 R 8290 2620 R R 8290 2620 2620 2620 119877 2620 119877 2620 119877 R 2620 2620 R 2620 R 2620 italic R 2620 italic R 2620 likely originates from candidate E 8290 2618 119864 2618 E 2618 italic E 2618 resonance E 8290 2618 119864 2618 E 2618 italic E 2618 E 8290 2618 E E 8290 2618 2618 2618 119864 2618 119864 2618 119864 E 2618 2618 E 2618 E 2618 italic E 2618 italic E 2618 resonance R 8290 2680 119877 2680 R 2680 italic R 2680 from candidate R 8290 2680 119877 2680 R 2680 italic R 2680 R 8290 2680 R R 8290 2680 2680 2680 119877 2680 119877 2680 119877 R 2680 2680 R 2680 R 2680 italic R 2680 italic R 2680 from candidate E 8290 2682 119864 2682 E 2682 italic E 2682 and resonance E 8290 2682 119864 2682 E 2682 italic E 2682 E 8290 2682 E E 8290 2682 2682 2682 119864 2682 119864 2682 119864 E 2682 2682 E 2682 E 2682 italic E 2682 italic E 2682 and resonance R 8290 2740 119877 2740 R 2740 italic R 2740 from candidate R 8290 2740 119877 2740 R 2740 italic R 2740 R 8290 2740 R R 8290 2740 2740 2740 119877 2740 119877 2740 119877 R 2740 2740 R 2740 R 2740 italic R 2740 italic R 2740 from candidate E 8290 2751 119864 2751 E 2751 italic E 2751 We also calculated the percentage contributions of each channel and the root mean square distance between quarks for these resonances as listed in Table E 8290 2751 119864 2751 E 2751 italic E 2751 E 8290 2751 E E 8290 2751 2751 2751 119864 2751 119864 2751 119864 E 2751 2751 E 2751 E 2751 italic E 2751 italic E 2751 We also calculated the percentage contributions of each channel and the root mean square distance between quarks for these resonances as listed in Table 6 The table reveals that each resonance has a significant K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT component From our previous bound state calculations we have already analyzed that the energy of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT component From our previous bound state calculations we have already analyzed that the energy of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT is lower than that of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT is lower than that of K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT indicating a strong attraction between K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT indicating a strong attraction between K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT Since these four resonances contain substantial colorful structure components their internal quark distances are small within 1 fm Additionally their decay widths are all less than K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT Since these four resonances contain substantial colorful structure components their internal quark distances are small within 1 fm Additionally their decay widths are all less than 10 10 10 10 MeV 10 10 10 10 10 10 10 10 10 10 10 10 10 MeV 
152,S5.T6, state 160 160 width 160 160 160 K 8290 K 8727 119870 superscript 119870 KK italic K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT 160 160 160 r s 8290 s 175 subscript 119903 119904 175 119904 r s bar s italic r start POSTSUBSCRIPT italic s over 175 start ARG italic s end ARG end POSTSUBSCRIPT 160 160 160 r s 8290 q 175 subscript 119903 119904 175 119902 r s bar q italic r start POSTSUBSCRIPT italic s over 175 start ARG italic q end ARG end POSTSUBSCRIPT 160 160 160 r q 175 8290 q 175 subscript 119903 175 119902 175 119902 r bar q bar q italic r start POSTSUBSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG end POSTSUBSCRIPT R 8290 2290 119877 2290 R 2290 italic R 2290 160 160 160 10 2 160 160 160 17 2 percent 17 2 17 2 17 2 8 0 percent 8 0 8 0 8 0 9 8 percent 9 8 9 8 9 8 36 4 percent 36 4 36 4 36 4 5 8 percent 5 8 5 8 5 8 22 4 percent 22 4 22 4 22 4 160 160 160 1 1 160 160 160 1 2 160 160 160 1 4 R 8290 2620 119877 2620 R 2620 italic R 2620 160 160 160 5 3 160 160 160 19 6 percent 19 6 19 6 19 6 24 8 percent 24 8 24 8 24 8 0 9 percent 0 9 0 9 0 9 36 3 percent 36 3 36 3 36 3 15 0 percent 15 0 15 0 15 0 3 2 percent 3 2 3 2 3 2 160 160 160 1 2 160 160 160 1 4 160 160 160 1 5 R 8290 2680 119877 2680 R 2680 italic R 2680 160 160 160 5 8 160 160 160 5 4 percent 5 4 5 4 5 4 25 5 percent 25 5 25 5 25 5 15 1 percent 15 1 15 1 15 1 28 9 percent 28 9 28 9 28 9 7 3 percent 7 3 7 3 7 3 17 6 percent 17 6 17 6 17 6 160 160 160 1 2 160 160 160 1 3 160 160 160 1 4 R 8290 2740 119877 2740 R 2740 italic R 2740 160 160 160 1 9 160 160 160 7 4 percent 7 4 7 4 7 4 14 5 percent 14 5 14 5 14 5 1 6 percent 1 6 1 6 1 6 35 1 percent 35 1 35 1 35 1 0 3 percent 0 3 0 3 0 3 40 1 percent 40 1 40 1 40 1 160 160 160 0 9 160 160 160 1 0 160 160 160 1 2 ,Table 6 Decay widths of the resonance states main components of the resonance states unit MeV and root mean square distances unit fm in s 8290 s 8290 q 175 8290 q 175 119904 119904 175 119902 175 119902 ss bar q bar q italic s italic s over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG system s 8290 s 8290 q 175 8290 q 175 119904 119904 175 119902 175 119902 ss bar q bar q italic s italic s over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG s 8290 s 8290 q 175 8290 q 175 s s 8290 s s 8290 q 175 q q 175 8290 q 175 q q 175 119904 119904 175 119902 175 119902 119904 119904 175 119902 175 119902 119904 s 119904 s 175 119902 175 119902 q 175 119902 175 119902 q ss bar q bar q ss bar q bar q italic s italic s over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG italic s italic s over start ARG italic q end ARG over start ARG italic q end ARG system ,In the T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system there are four colorful structures two diquark configurations T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system there are four colorful structures two diquark configurations s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT s 8290 s 6 0 8290 q 175 8290 q 175 6 175 1 s 8290 s 6 0 s 8290 s s 8290 s s s 8290 s s 6 6 0 0 8290 q 175 8290 q 175 6 175 1 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 6 175 6 6 175 1 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript subscript delimited 119904 119904 6 0 superscript superscript subscript delimited 119904 119904 6 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 6 6 0 0 superscript subscript delimited 175 119902 175 119902 175 6 1 superscript superscript subscript delimited 175 119902 175 119902 175 6 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 6 175 6 6 1 1 ss 6 0 bar q bar q bar 6 1 ss 6 0 bar q bar q bar 6 1 italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 6 end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 6 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT and s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT and two color octet states s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT s 8290 s 3 1 8290 q 175 8290 q 175 3 175 0 s 8290 s 3 1 s 8290 s s 8290 s s s 8290 s s 3 3 1 1 8290 q 175 8290 q 175 3 175 0 q 175 8290 q 175 q 175 8290 q 175 q 175 q q 175 8290 q 175 q q 175 3 175 3 3 175 0 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript subscript delimited 119904 119904 3 1 superscript superscript subscript delimited 119904 119904 3 subscript subscript delimited 119904 119904 delimited delimited 119904 119904 119904 s 119904 s 3 3 1 1 superscript subscript delimited 175 119902 175 119902 175 3 0 superscript superscript subscript delimited 175 119902 175 119902 175 3 subscript subscript delimited 175 119902 175 119902 delimited delimited 175 119902 175 119902 175 119902 175 119902 q 175 119902 175 119902 q 175 3 175 3 3 0 0 ss 3 1 bar q bar q bar 3 0 ss 3 1 bar q bar q bar 3 0 italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over 175 start ARG italic q end ARG over 175 start ARG italic q end ARG start POSTSUBSCRIPT over 175 start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT italic s italic s start POSTSUBSCRIPT 3 end POSTSUBSCRIPT start POSTSUPERSCRIPT 1 end POSTSUPERSCRIPT over start ARG italic q end ARG over start ARG italic q end ARG start POSTSUBSCRIPT over start ARG 3 end ARG end POSTSUBSCRIPT start POSTSUPERSCRIPT 0 end POSTSUPERSCRIPT and two color octet states K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT and K 8 8290 K 8727 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8 8290 K 8727 8 K 8 K K K 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript delimited superscript 119870 8 subscript delimited 119870 8 subscript subscript delimited 119870 delimited delimited 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT and K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT We first performed channel coupling of these four colorful structures According to our calculations within the energy range of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT We first performed channel coupling of these four colorful structures According to our calculations within the energy range of 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 1 4 2 8 2 8 2 8 2 8 GeV we identified 13 possible candidates of resonance in 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 2 8 GeV we identified 13 possible candidates of resonance in T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system These resonances denoted as T s 8290 s subscript 119879 119904 119904 T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT T s 8290 s T T s 8290 s s s 8290 s s subscript 119879 119904 119904 subscript 119879 119904 119904 subscript subscript 119879 T 119904 119904 119904 s 119904 s T ss T ss italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT italic T start POSTSUBSCRIPT italic s italic s end POSTSUBSCRIPT system These resonances denoted as E 8290 energy 119864 energy E text energy italic E energy are listed in Table E 8290 energy 119864 energy E text energy italic E energy E 8290 energy E E 8290 energy energy energy 119864 energy 119864 energy 119864 E energy energy energy E text energy E text energy italic E energy italic E energy are listed in Table 5 along with their corresponding color structure compositions The majority of the candidates are situated around 2 6 GeV indicating several genuine resonances at this energy level Subsequently we employed the real scaling method to evaluate the stability of these resonances with the results presented in FIG 2 As shown in FIG 2 most of the resonance candidates decayed into their corresponding threshold channels Only four resonances R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 R R 8290 2290 2290 2290 119877 2290 119877 2290 119877 R 2290 2290 R 2290 R 2290 italic R 2290 italic R 2290 R 8290 2610 119877 2610 R 2610 italic R 2610 R 8290 2610 119877 2610 R 2610 italic R 2610 R 8290 2610 R R 8290 2610 2610 2610 119877 2610 119877 2610 119877 R 2610 2610 R 2610 R 2610 italic R 2610 italic R 2610 R 8290 2690 119877 2690 R 2690 italic R 2690 and R 8290 2690 119877 2690 R 2690 italic R 2690 R 8290 2690 R R 8290 2690 2690 2690 119877 2690 119877 2690 119877 R 2690 2690 R 2690 R 2690 italic R 2690 italic R 2690 and R 8290 2740 119877 2740 R 2740 italic R 2740 survived the coupling process with the scattering channels The resonance R 8290 2740 119877 2740 R 2740 italic R 2740 R 8290 2740 R R 8290 2740 2740 2740 119877 2740 119877 2740 119877 R 2740 2740 R 2740 R 2740 italic R 2740 italic R 2740 survived the coupling process with the scattering channels The resonance R 8290 2290 119877 2290 R 2290 italic R 2290 primarily originates from the previous resonance candidate R 8290 2290 119877 2290 R 2290 italic R 2290 R 8290 2290 R R 8290 2290 2290 2290 119877 2290 119877 2290 119877 R 2290 2290 R 2290 R 2290 italic R 2290 italic R 2290 primarily originates from the previous resonance candidate E 8290 2313 119864 2313 E 2313 italic E 2313 As the scaling factor E 8290 2313 119864 2313 E 2313 italic E 2313 E 8290 2313 E E 8290 2313 2313 2313 119864 2313 119864 2313 119864 E 2313 2313 E 2313 E 2313 italic E 2313 italic E 2313 As the scaling factor 945 120572 alpha italic 945 increased the calculational space changed causing the energy of 945 120572 alpha italic 945 945 a 120572 120572 alpha alpha italic 945 italic a increased the calculational space changed causing the energy of E 8290 2313 119864 2313 E 2313 italic E 2313 dropping to E 8290 2313 119864 2313 E 2313 italic E 2313 E 8290 2313 E E 8290 2313 2313 2313 119864 2313 119864 2313 119864 E 2313 2313 E 2313 E 2313 italic E 2313 italic E 2313 dropping to 2290 2290 2290 2290 MeV where it stabilized and formed an avoided crossing structure at 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 2290 MeV where it stabilized and formed an avoided crossing structure at 945 1 2 120572 1 2 alpha 1 2 italic 945 1 2 Since the energies of most resonance candidates are concentrated around 2 6 GeV we observe that the other three genuine resonances that survive after coupling with the scattering channels also have energies near 2 6 GeV By comparing the energy levels we can infer that resonance 945 1 2 120572 1 2 alpha 1 2 italic 945 1 2 945 1 2 945 a 1 2 1 2 120572 1 2 120572 1 2 120572 1 2 1 2 alpha 1 2 alpha 1 2 italic 945 1 2 italic a 1 2 Since the energies of most resonance candidates are concentrated around 2 6 GeV we observe that the other three genuine resonances that survive after coupling with the scattering channels also have energies near 2 6 GeV By comparing the energy levels we can infer that resonance R 8290 2620 119877 2620 R 2620 italic R 2620 likely originates from candidate R 8290 2620 119877 2620 R 2620 italic R 2620 R 8290 2620 R R 8290 2620 2620 2620 119877 2620 119877 2620 119877 R 2620 2620 R 2620 R 2620 italic R 2620 italic R 2620 likely originates from candidate E 8290 2618 119864 2618 E 2618 italic E 2618 resonance E 8290 2618 119864 2618 E 2618 italic E 2618 E 8290 2618 E E 8290 2618 2618 2618 119864 2618 119864 2618 119864 E 2618 2618 E 2618 E 2618 italic E 2618 italic E 2618 resonance R 8290 2680 119877 2680 R 2680 italic R 2680 from candidate R 8290 2680 119877 2680 R 2680 italic R 2680 R 8290 2680 R R 8290 2680 2680 2680 119877 2680 119877 2680 119877 R 2680 2680 R 2680 R 2680 italic R 2680 italic R 2680 from candidate E 8290 2682 119864 2682 E 2682 italic E 2682 and resonance E 8290 2682 119864 2682 E 2682 italic E 2682 E 8290 2682 E E 8290 2682 2682 2682 119864 2682 119864 2682 119864 E 2682 2682 E 2682 E 2682 italic E 2682 italic E 2682 and resonance R 8290 2740 119877 2740 R 2740 italic R 2740 from candidate R 8290 2740 119877 2740 R 2740 italic R 2740 R 8290 2740 R R 8290 2740 2740 2740 119877 2740 119877 2740 119877 R 2740 2740 R 2740 R 2740 italic R 2740 italic R 2740 from candidate E 8290 2751 119864 2751 E 2751 italic E 2751 We also calculated the percentage contributions of each channel and the root mean square distance between quarks for these resonances as listed in Table E 8290 2751 119864 2751 E 2751 italic E 2751 E 8290 2751 E E 8290 2751 2751 2751 119864 2751 119864 2751 119864 E 2751 2751 E 2751 E 2751 italic E 2751 italic E 2751 We also calculated the percentage contributions of each channel and the root mean square distance between quarks for these resonances as listed in Table 6 The table reveals that each resonance has a significant K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT component From our previous bound state calculations we have already analyzed that the energy of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT component From our previous bound state calculations we have already analyzed that the energy of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT is lower than that of K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT is lower than that of K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT indicating a strong attraction between K 8727 8290 K 8727 superscript 119870 superscript 119870 K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT K 8727 8290 K 8727 K 8727 K K 8727 8290 K 8727 K K 8727 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript 119870 superscript superscript 119870 K superscript 119870 superscript superscript 119870 K K K K K italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT indicating a strong attraction between K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT Since these four resonances contain substantial colorful structure components their internal quark distances are small within 1 fm Additionally their decay widths are all less than K 8727 8 8290 K 8727 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT K 8727 8 8290 K 8727 8 K 8727 8 K 8727 K 8727 K K 8727 8 8 8290 K 8727 8 K 8727 K 8727 K K 8727 8 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 subscript delimited superscript 119870 8 subscript subscript delimited superscript 119870 delimited delimited superscript 119870 superscript superscript 119870 K 8 8 K 8 K 8 K 8 K 8 italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT 8727 end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT italic K start POSTSUPERSCRIPT end POSTSUPERSCRIPT start POSTSUBSCRIPT 8 end POSTSUBSCRIPT Since these four resonances contain substantial colorful structure components their internal quark distances are small within 1 fm Additionally their decay widths are all less than 10 10 10 10 MeV 10 10 10 10 10 10 10 10 10 10 10 10 10 MeV 
153,S3.T1.5.1, Setting SAM 160 19 HQ SAM 160 18 PerSAM 160 40 Prompt Surrogate ADE COCO CITY SA 1B ADE COCO CITY SA 1B ADE COCO CITY SA 1B POINT Clean 65 39 62 79 50 70 77 21 63 39 65 38 50 25 72 89 64 61 62 91 51 22 77 93 ADE 0 43 3 21 0 02 5 81 0 99 6 04 6 82 7 75 0 37 3 25 7 95 1 54 COCO 0 42 1 16 0 76 2 46 0 69 2 23 3 19 3 46 0 01 0 05 2 56 0 07 CITY 10 54 22 23 0 07 22 11 9 95 25 12 3 01 20 82 0 93 5 57 0 43 1 19 SA 1B 0 24 0 91 0 04 0 14 1 20 5 74 2 81 0 75 0 05 0 07 2 54 0 05 AVG 2 91 6 88 0 22 7 63 3 21 9 78 3 96 8 20 0 34 2 24 3 37 0 71 BOX Clean 74 59 79 00 64 60 89 41 72 95 81 19 62 00 86 80 72 30 78 83 64 46 89 32 ADE 4 87 10 74 1 64 14 81 3 77 12 95 19 16 20 90 2 32 8 82 19 33 14 06 COCO 1 51 2 97 1 96 9 22 0 27 9 38 9 74 20 97 0 41 1 38 12 04 3 20 CITY 17 39 26 43 0 33 16 10 4 43 20 60 3 82 27 66 3 09 13 40 2 49 35 25 SA 1B 16 81 27 38 9 19 5 19 10 16 24 90 18 06 1 01 5 49 17 12 16 46 0 81 AVG 10 15 16 88 3 28 11 33 4 66 16 96 12 70 17 64 2 83 10 18 12 58 13 33 ,Table 1 The mIoU of DarkSAM under different settings Values covered by gray denote the clean mIoU others denote adversarial mIoU ADE20K MS COCO CITYSCAPES abbreviated as ADE COCO CITY respectively Bolded values indicate the best results ,The experiments in Tab 1 show that DarkSAM can effectively fool these prompt guided image segmentation models with an average mIoU reduction of more than 60 percent6060 60 across 96969696 different experimental settings The results in Tab 1 also indicate that box prompts not only yield higher segmentation accuracy but also demonstrate greater robustness For adversaries the choice of surrogate datasets has a minor impact on crafting UAPs yet they consistently facilitate excellent attack performance Notably DarkSAM demonstrates a distinct advantage when the SA 1B dataset the training data for SAM is employed as the surrogate dataset In addition to the above quantitative experimental results we also present qualitative findings Specifically we provide the visualization of SAM segmentation results for adversarial examples made by DarkSAM using point and box prompts across four different datasets in Fig 5 These results include masks of objects in images output by SAM under point box and segment everything prompt modes From Fig 5 we can see that SAM successfully segments benign images across three types of prompt modes but it is unable to segment adversarial examples i e the output masks are close to dark The qualitative results further corroborate the powerful attack capability of DarkSAM We study the attack transferability of DarkSAM across data domain prompt types and models respectively 1 Cross domain The results in Tab 1 demonstrate DarkSAM s excellent cross domain transferability where UAPs generated with the surrogate dataset ADE20K achieve a high ASR on datasets from various different domains We also explore the role of the frequency attack i e JfasubscriptJfa mathcal J fa caligraphic J start POSTSUBSCRIPT italic f italic a end POSTSUBSCRIPT denoted as FA in enhancing cross domain transferability As shown in Fig 4 a frequency attack can effectively improve the attack performance based on the spatial attack i e JsasubscriptJsa mathcal J sa caligraphic J start POSTSUBSCRIPT italic s italic a end POSTSUBSCRIPT denoted as SA 2 Cross prompt We examine the performance of DarkSAM across various types of prompts As demonstrated in the last three columns of Fig 5 UAPs created based on both point and box prompts perform well under the segment everything mode Additionally we provide results of transferability experiments between point and box prompts in Tab 2 This includes testing UAPs created with point prompts in the box prompt setting and vice versa Based on the observed results it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information 3 Cross model We use UAPs created with points and boxes based on SAM to attack HQ SAM and PER SAM The results in Fig 4 b e showcase DarkSAM s exceptional transferability across different models 
154,S4.T2.4.2, Surrogate BOX 8594 8594 rightarrow 8594 POINT POINT 8594 8594 rightarrow 8594 BOX ADE COCO CITY SA 1B ADE COCO CITY SA 1B ADE 63 01 55 61 49 72 66 67 47 00 35 52 56 15 40 80 COCO 64 95 61 69 49 98 75 09 19 95 25 27 44 60 53 01 CITY 48 31 30 48 50 30 55 74 17 36 10 94 55 43 20 69 SA 1B 52 47 36 05 47 12 66 20 31 16 17 45 58 21 62 00 AVG 57 19 45 96 49 28 65 93 28 87 22 30 53 60 44 13 ,Table 2 The ASR of the cross prompt transferability study on SAM BOX rightarrow POINT indicates that adversarial examples created using box are tested in point mode Others stand the same meaning ,We study the attack transferability of DarkSAM across data domain prompt types and models respectively 1 Cross domain The results in Tab 1 demonstrate DarkSAM s excellent cross domain transferability where UAPs generated with the surrogate dataset ADE20K achieve a high ASR on datasets from various different domains We also explore the role of the frequency attack i e JfasubscriptJfa mathcal J fa caligraphic J start POSTSUBSCRIPT italic f italic a end POSTSUBSCRIPT denoted as FA in enhancing cross domain transferability As shown in Fig 4 a frequency attack can effectively improve the attack performance based on the spatial attack i e JsasubscriptJsa mathcal J sa caligraphic J start POSTSUBSCRIPT italic s italic a end POSTSUBSCRIPT denoted as SA 2 Cross prompt We examine the performance of DarkSAM across various types of prompts As demonstrated in the last three columns of Fig 5 UAPs created based on both point and box prompts perform well under the segment everything mode Additionally we provide results of transferability experiments between point and box prompts in Tab 2 This includes testing UAPs created with point prompts in the box prompt setting and vice versa Based on the observed results it is discernible that UAPs crafted using box prompts generally demonstrate better transferability compared to those using point prompts This increased efficacy can likely be attributed to the box prompts offering more integral and detailed prompt information 3 Cross model We use UAPs created with points and boxes based on SAM to attack HQ SAM and PER SAM The results in Fig 4 b e showcase DarkSAM s exceptional transferability across different models 
155,S4.T3.2.2, Method POINT 8594 8594 rightarrow 8594 POINT BOX 8594 8594 rightarrow 8594 BOX ADE COCO CITY SA 1B ADE COCO CITY SA 1B UAP 160 27 1 62 0 47 8 13 5 28 0 28 1 29 1 76 UAPGD 160 9 4 85 1 52 11 52 10 04 0 97 0 45 2 22 3 11 SSP 160 32 0 67 0 09 5 90 4 08 0 91 1 20 SegPGD 160 11 4 24 1 44 11 48 8 92 0 89 0 51 2 10 3 46 Attack SAM 160 39 2 91 1 36 13 20 9 54 0 51 0 36 1 90 3 12 Ours 64 96 61 63 50 63 77 07 69 72 76 03 64 27 84 22 ,Table 3 The ASR of comparison study,To comprehensively demonstrate the superiority of our proposed method we compare DarkSAM with popular UAP schemes including UAP 27 UAPGD 9 and SSP 32 We also consider the state of the art adversarial attack against traditional segmentation models SegPGD 11 and the latest sample wise attack against SAM Attack SAM 39 For a fair comparison we adapt them to a UAP optimization strategy and keep other settings consistent with DarkSAM We select SAM as the victim model and assess the effectiveness of these UAP methods across four datasets using the same dataset for both generating and testing the UAPs The results in Tab 3 indicate that DarkSAM outperforms all methods with a considerable margin The negative experimental values indicate that the attack does not work at all This phenomenon may stem from counterproductive perturbations that inadvertently cause the input samples to resemble the training set used by SAM paradoxically enhancing accuracy and resulting in negative ASR values We also provide visualizations of the segmentation results of the adversarial examples made by these methods using box prompts in Fig A10 obtained in point box and segment everything modes respectively The results further demonstrate the superiority of DarkSAM 
156,A3.T1.2.2, Model Surrogate POINT 8594 8594 rightarrow 8594 POINT BOX 8594 8594 rightarrow 8594 BOX ADE COCO CITY SA 1B ADE COCO CITY SA 1B MobileSAM 160 37 Clean 63 77 63 08 51 13 76 82 72 69 79 15 64 3 89 14 ADE 0 82 2 48 0 47 3 99 0 98 4 99 1 34 6 02 COCO 0 10 0 41 0 03 0 74 1 60 6 38 1 33 8 12 CITY 16 73 31 85 0 10 41 47 26 81 49 38 0 81 48 74 SA 1B 0 06 0 34 2 3e 6 1 8e 5 4 67 14 89 0 84 3 03 AVG 4 43 8 77 0 15 11 55 8 52 18 91 1 08 16 48 ,Table A1 The mIoU of DarkSAM on MobileSAM Values covered by gray denote the clean mIoU others denote adversarial mIoU ADE20K MS COCO CITYSCAPES abbreviated as ADE COCO CITY respectively Bolded values indicate the best results ,We evaluate the attack performance of DarkSAM against another SAM s variant model MobileSAM 37 on four datasets All experimental settings are kept consistent with Sec 4 2 of the manuscript The results in Tab A1 demonstrate the effectiveness of DarkSAM against MobileSAM further proving its strong attack capability Notably in line with the conclusions in Sec 4 2 of the manuscript the choice of surrogate datasets has a certain impact on the attack performance SA 1B serves as a notably superior surrogate dataset while CITYSCAPES exhibits comparatively lower performance in certain scenarios This discrepancy may be attributed to CITYSCAPES limited scope which solely encompasses the urban street scene consequently restricting the transferability of the generated UAPs Hence adversaries gain an advantage by opting for a semantically diverse dataset encompassing a wide range of categories objects and scenes to augment the attack performance of UAPs 
157,A4.T2.4.1, Prompt Surrogate ADE COCO CITY SA 1B Point ADE 43 51 49 03 23 86 43 77 SA 1B 34 87 38 84 42 59 48 03 Box ADE 49 79 48 69 47 42 43 63 SA 1B 62 56 66 68 51 91 52 29 ,Table A2 The ASR of DarkSAM on SAM L,We present both quantitative and qualitative results of DarkSAM on SAM with a ViT L backbone denoted as SAM L The quantitative findings in Table A2 illustrate the effectiveness of DarkSAM in deceiving SAM L Notably these results indicate that SAM L exhibits greater robustness compared to SAM B SAM with a ViT B backbone due to its more intricate network architecture Additionally we offer visualization results of DarkSAM s attacks on SAM L under point box and segment everything modes Figs A1 and A2 demonstrate that adversarial examples generated based on point prompts effectively mislead SAM L Similarly adversarial examples crafted using box prompts also prove to be successful in deceiving SAM L as depicted in Figs A3 and A4 
158,A4.T3.2.2, Model POINT amp BOX 8594 8594 rightarrow 8594 POINT POINT amp BOX 8594 8594 rightarrow 8594 BOX ADE COCO CITY SA 1B ADE COCO CITY SA 1B SAM 160 19 64 49 61 31 50 74 76 42 73 05 71 87 63 59 86 45 HQ SAM 160 18 62 49 64 43 49 09 72 26 70 65 70 08 57 49 84 28 PerSAM 160 40 63 53 62 41 50 01 76 86 70 13 76 54 62 03 85 63 MobileSAM 160 37 63 18 62 47 50 88 76 06 70 90 75 61 63 38 85 41 ,Table A3 The ASR of DarkSAM using five points and five boxes,2 The mixed use of point and box prompts in the shadow target strategy Tab 1 of the manuscript showcases the impressive attack capabilities of UAPs generated by DarkSAM utilizing ten randomly selected points and boxes Building upon this we delve into the effects of employing a hybrid approach of points and boxes as prompts in the shadow target strategy for DarkSAM In this method we craft UAPs using a balanced mix of five random points and five boxes The outcomes as detailed in Tab A3 reveal that UAPs constructed with this mixed approach maintain robust attack performance This finding accentuates the adaptability and effectiveness of our proposed shadow target strategy 
159,S4.T1, Dataset Nodes Edges Lables Metrics Amazon CoBuyPhoto 7 650 238 163 8 ACC amp F1 Amazon CoBuyComputer 13 752 491 722 10 ACC amp F1 Flickr 89 250 899 756 7 ACC amp F1 Reddit 232 965 114 848 857 41 ACC amp F1 Ogbn products 2 449 029 126 067 309 47 ACC amp F1 Dataset Nodes Edges Node Deg Metric Ogbl citation2 2 927 963 30 561 187 20 7 MRR Ogbl collab 235 868 1 285 465 8 2 Hits 100 Amazon CoBuyPhoto Amazon CoBuyComputer ,TABLE I Statistics of the node classification above and link prediction below datasets used in this paper ,Datasets To verify the generalization ability of the model across different datasets we evaluate the PromptGCN model on five large scale public datasets about node classification tasks AmazonCoBuyPhoto 1 1 1 11https github com shchur gnn benchmark datasets AmazonCoBuyComputer1 Flickr 2 2 2 22https github com GraphSAINT GraphSAINT Reddit 3 3 3 33http snap stanford edu graphsage and Ogbn products 4 4 4 44https ogb stanford edu docs nodeprop ogbn products Additionally we evaluate the PromptGCN model on two large scale public datasets about link prediction tasks Ogbl citation2 5 5 5 55https ogb stanford edu docs linkprop ogbl citation2 and Obgl collab 6 6 6 66https ogb stanford edu docs linkprop ogbl collab The datasets detail are shown in Table I Evaluation indicators This paper evaluates the PromptGCN model using several classical evaluation metrics As shown in Table I 1 For the node classification task Accuracy and marco F1 are used as an evaluation metric on all datasets 2 For the link prediction task we adopt the same evaluation metrics as 28 The Obgl citation2 dataset emphasizes the precise ordering of predictions and therefore the 28 The Obgl citation2 dataset emphasizes the precise ordering of predictions and therefore the MRR metric is used In contrast the Obgl collab dataset focuses on the coverage of predictions and therefore the Hits 100 metric is adopted 
160,S4.T2, Models Amazon CoBuyPhoto Amazon CoBuyComputer Flickr Reddit Ogbn products Metrics ACC 8593 F1 8593 ACC 8593 F1 8593 ACC 8593 F1 8593 ACC 8593 F1 8593 ACC 8593 F1 8593 Full batch GCN 4 0 9385 0 9327 0 9116 0 9051 0 5226 0 2390 0 9442 0 9152 0 7560 0 3484 GCN Ours 0 9411 0 9291 0 9102 0 9111 0 5314 0 2438 0 9440 0 9101 0 7405 0 3336 GAT 5 0 9509 0 9458 0 9193 0 9092 0 5162 0 2413 0 9401 0 9002 0 7552 0 3077 GAT Ours 0 9483 0 9414 0 9196 0 9091 0 5310 0 2528 0 9424 0 9066 0 7427 0 3121 GCNII 6 0 9326 0 9289 0 9065 0 8979 0 4892 0 2292 0 9482 0 9131 0 7326 0 3217 GCNII Ours 0 9359 0 9304 0 9127 0 9092 0 4945 0 2445 0 9527 0 9303 0 7433 0 3665 Subgraph Sampling ClusterGCN 14 0 9518 0 9430 0 9107 0 9031 0 4848 0 1619 0 9346 0 9031 0 7552 0 3360 ClusterGCN Is 0 9529 0 9433 0 9065 0 8952 0 4923 0 1805 0 9323 0 8945 0 7365 0 3116 ClusterGCN Ours 0 9582 0 9497 0 9294 0 9233 0 5036 0 2191 0 9404 0 9151 0 7590 0 3421 GraphSAINT 15 0 9735 0 9650 0 9315 0 9182 0 4760 0 1658 0 9639 0 8992 0 7271 0 3336 GraphSAINT Is 0 9740 0 9681 0 9325 0 9205 0 4856 0 1751 0 9622 0 8873 0 6986 0 3163 GraphSAINT Ours 0 9763 0 9716 0 9373 0 9291 0 5021 0 2349 0 9688 0 9116 0 7324 0 3375 LMC 16 0 9000 0 8910 0 8400 0 8372 0 5392 0 2936 0 9544 0 9342 0 7409 0 3519 LMC Is 0 8875 0 8933 0 7750 0 7731 0 5361 0 2909 0 9532 0 9344 0 7418 0 3593 LMC Ours 0 9187 0 9090 0 8500 0 8493 0 5387 0 2966 0 9542 0 9352 0 7469 0 3434 Amazon CoBuyPhoto Amazon CoBuyComputer Full batch Subgraph Sampling ,TABLE II Performance comparison among PromptGCN and the baselines on the node classification task at 3 layers The performance of PromptGCN is validated in three backbone models respectively Bold indicates the best results while underlined indicates the suboptimal results denotes the higher score the better performance ,This section compares the performance of PromptGCN with baselines on seven large scale datasets Table II and Table III show that the performance comparison among PromptGCN and the baselines on the node classification and link prediction tasks at 3 layers respectively The experimental results are statistical results of multiple experiments We apply PromptGCN to two representative subgraph sampling methods i e ClusterGCN 14 and GraphSAINT 14 and GraphSAINT 15 and two latest methods i e LMC 15 and two latest methods i e LMC 16 and ELPH 16 and ELPH 28 From the experimental results we can draw the following conclusions 28 From the experimental results we can draw the following conclusions 1 PromptGCN is applied to node classification Table II and link prediction Table III tasks respectively Experiments show that PromptGCN finally resembles full batch performance on all datasets 2 Table II presents the overall performance of PromptGCN on the node classification task Compared to the three backbone subgraph sampling models PromptGCN achieves competitive metrics across almost all datasets For instance on the Flickr dataset PromptGCN improves performance by 5 48 Additionally we selected the largest dataset Ogbn products for visualization experiments As shown in Fig 4 experiments across different layers reveal that PromptGCN achieves the highest test accuracy 3 Although the memory consumption of PromptGCN is slightly larger than that of the backbone models ClusterGCN and GraphSAINT it remains within acceptable limits Moreover Table II illustrated that PromptGCN finally resembles full batch performance and outperforms subgraph sampling models 
161,S4.T3, Models Ogbl citation2 Ogbl collab Metrics MRR 8593 Hits 100 8593 Full Batch GCN 4 0 8432 0 4557 GCN Ours OOM 0 4795 GAT 5 0 8434 0 4322 GAT Ours OOM 0 4228 GCNII 6 0 8435 0 4664 GCNII Ours OOM 0 4548 Subgraph Sampling ClusterGCN 14 0 8076 0 4889 ClusterGCN Is 0 7323 0 3779 ClusterGCN Ours 0 8118 0 4988 GraphSAINT 15 0 7933 0 4745 GraphSAINT Is 0 7765 0 4642 GraphSAINT Ours 0 8023 0 5289 ELPH 28 0 8756 0 7094 ELPH Is 0 8442 0 6878 ELPH Ours 0 8642 0 7114 Full Batch Subgraph Sampling ,TABLE III Performance comparison between PromptGCN and the baseline on the link prediction task at 3 layers The performance of PromptGCN is validated in three backbone models respectively Bold indicates the best results while underlined indicates the suboptimal results denotes the higher score the better performance ,This section compares the performance of PromptGCN with baselines on seven large scale datasets Table II and Table III show that the performance comparison among PromptGCN and the baselines on the node classification and link prediction tasks at 3 layers respectively The experimental results are statistical results of multiple experiments We apply PromptGCN to two representative subgraph sampling methods i e ClusterGCN 14 and GraphSAINT 14 and GraphSAINT 15 and two latest methods i e LMC 15 and two latest methods i e LMC 16 and ELPH 16 and ELPH 28 From the experimental results we can draw the following conclusions 28 From the experimental results we can draw the following conclusions 1 PromptGCN is applied to node classification Table II and link prediction Table III tasks respectively Experiments show that PromptGCN finally resembles full batch performance on all datasets 3 To evaluate the performance of PromptGCN on different downstream tasks we applied it to a link prediction task Table III presents the overall performance of PromptGCN for this task On the Ogbl collab dataset PromptGCN improves the backbone performance by 2 02 percent 2 02 2 02 2 02 The strong performance of PromptGCN across various tasks demonstrates that our method is adaptable to different backbone models and downstream tasks Experimental results on two tasks show that PromptGCN performs less impressively with the latest backbone models This suggests that these advanced neural network models are more complex and require more sophisticated and well designed prompts 2 02 percent 2 02 2 02 2 02 2 02 2 02 2 02 percent 2 02 percent 2 02 percent percent 2 02 2 02 2 02 2 02 2 02 2 02 The strong performance of PromptGCN across various tasks demonstrates that our method is adaptable to different backbone models and downstream tasks Experimental results on two tasks show that PromptGCN performs less impressively with the latest backbone models This suggests that these advanced neural network models are more complex and require more sophisticated and well designed prompts 
162,S4.T4, Layer Models Amazon CoBuyPhoto Amazon CoBuyComputer Flickr Reddit Ogbn products 3 layer GCN 4 0 99 1 81 9 41 26 182 ClusterGCN 14 0 25 0 38 1 86 5 15 27 78 ClusterGCN Is 0 67 1 11 4 08 12 13 31 23 ClusterGCN Ours 0 25 1 12 4 27 8 83 37 42 GraphSAINT 15 0 45 0 52 0 58 0 97 5 01 GraphSAINT Is 1 32 1 49 1 29 2 03 5 97 GraphSAINT Ours 1 13 1 15 1 36 1 46 7 69 4 layer GCN 4 1 18 2 15 11 1 32 23 243 ClusterGCN 14 0 33 0 51 2 49 6 72 34 57 ClusterGCN Is 0 69 1 14 4 11 12 83 39 44 ClusterGCN Ours 0 69 1 13 4 91 13 22 45 43 GraphSAINT 15 0 59 0 68 0 78 1 25 6 63 GraphSAINT Is 1 28 1 51 1 33 2 13 7 60 GraphSAINT Ours 1 29 1 51 1 43 1 68 9 39 5 layer GCN 4 1 37 2 49 14 1 37 21 292 ClusterGCN 14 0 40 0 61 3 13 8 33 42 81 ClusterGCN Is 0 71 1 15 4 60 12 92 47 71 ClusterGCN Ours 0 78 1 22 5 55 15 25 53 21 GraphSAINT 15 0 72 0 84 0 99 1 53 8 24 GraphSAINT Is 1 30 1 53 1 46 2 15 9 23 GraphSAINT Ours 1 39 1 67 1 76 1 88 10 91 Amazon CoBuyPhoto Amazon CoBuyComputer ,TABLE IV Comparison of memory consumption between PromptGCN and baselines at different numbers of layers measured in 10 2 superscript 10 2 10 2 10 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT MB 10 2 superscript 10 2 10 2 10 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 10 2 10 10 2 2 superscript 10 2 superscript 10 2 superscript superscript 10 10 2 2 10 2 10 2 10 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT 10 start POSTSUPERSCRIPT 2 end POSTSUPERSCRIPT MB Bold indicates where memory consumption is lower than the full batch method The lower metric the smaller memory consumption ,The deeper aggregation layers of full batch GCN significantly increase the memory consumption In Table IV we compare the memory consumption of PromptGCN GraphGCN GraphSAINT and GCN across different layers From the experimental results we can draw the following conclusions 
163,S4.T5, Attachment patterns Amazon CoBuyPhoto Amazon CoBuyComputer Flickr Reddit Ogbn products Concatenate 0 9582 0 9294 0 5036 0 9404 0 7590 Point wise Addition 0 9535 0 9223 0 5012 0 9376 0 7482 Point wise Multiplication 0 9524 0 9114 0 4961 0 9303 0 7522 Point wise Weighted Addition 945 120572 alpha italic 945 0 2 0 9549 0 9206 0 5036 0 9384 0 7500 Point wise Weighted Addition 945 120572 alpha italic 945 0 4 0 9535 0 9173 0 5011 0 9411 0 7462 Point wise Weighted Addition 945 120572 alpha italic 945 0 6 0 9522 0 9276 0 5036 0 9399 0 7546 Point wise Weighted Addition 945 120572 alpha italic 945 0 8 0 9575 0 9247 0 5028 0 9408 0 7578 Attachment patterns Amazon CoBuyPhoto Amazon CoBuyComputer ,TABLE V Effect of different attachment modes of ClusterGCN Ours on test accuracy Integrated prompt with node features in different attachment methods including Concatenate Point wise Addition Point wise Multiplication and Point wise Weighted Addition Bold indicates the best results Use ACC as the evaluation metric and the higher score the better performance ,Impact of Different Prompt Attachment Methods We integrated Prompts with node features using various attachment methods including concatenate point wise addition point wise multiplication and point wise weighted addition Table V presents the experimental results on the five datasets indicating that 1 The concatenate scheme achieves optimal results because it maximizes the retention of the independence between prompts and node embedding features 2 Other attachment methods also contribute to model performance We find that using the concatenate operation to attach prompt embeddings to node features maximize the delivery of global information Therefore in this paper the concatenate operation is used as the main attachment method 
164,S4.T6, Number of subgraphs Models Amazon CoBuyPhoto Amazon CoBuyComputer Flickr Reddit Ogbn products 100 ClusterGCN 14 0 9464 0 8960 0 4838 0 9303 0 7658 ClusterGCN Ours 0 9503 0 9076 0 4924 0 9327 0 7707 20 ClusterGCN 14 0 9507 0 9102 0 4864 0 9365 0 7683 ClusterGCN Ours 0 9533 0 9160 0 5026 0 9384 0 7629 10 ClusterGCN 14 0 9481 0 9113 0 4869 0 9352 0 7552 ClusterGCN Ours 0 9604 0 9184 0 5018 0 9392 0 7590 5 ClusterGCN 14 0 9518 0 9107 0 4848 0 9346 0 7455 ClusterGCN Ours 0 9582 0 9294 0 5036 0 9404 0 7530 Number of subgraphs Amazon CoBuyPhoto Amazon CoBuyComputer ,TABLE VI Test Accuracy of ClusterGCN and ClusterGCN Ours at different number of subgraphs and bold indicates the best results Use ACC as the evaluation metric and the higher score the better performance ,Impact of Different Numbers of Subgraphs To evaluate the performance of PromptGCN with varying numbers of subgraphs we conducted experiments by adjusting the number of subgraphs in the ClusterGCN model Table VI presents the ACC across five datasets The results show that 1 The ClusterGCN results confirm our hypothesis from the introduction that the subgraph receptive field is negatively correlated with the number of subgraphs i e as more subgraphs are partitioned more global information is lost which significantly impacts the prediction performance of downstream tasks 2 PromptGCN consistently outperforms ClusterGCN across all subgraph configurations demonstrating that the prompting strategy effectively expands the subgraph receptive field and captures global information 
165,Sx1.T0, Mechanism Definition Potential Mitigations Personal Engagement Advocacy Industry actors directly participate in formal policy making processes 8212 e g interacting directly with policymakers or regulators to provide information or convince them of a particular point of view 8226 Increase transparency requirements 8226 Build robust civil society institutions Procedural obstruction Industry actors intentionally impede policymaker or regulator action through procedural interference 8226 N A additional research needed Incentive Shaping Donations gifts and bribes Industry actors make financial contributions to elected officials 8217 campaigns or give personal gifts to elected officials or their staff 8226 Increase transparency requirements 8226 Build robust civil society institutions Private threats Industry actors make explicit or implicit threats of litigatory reputational or other negative consequences to prevent policy enactment or enforcement 8226 N A to the United States Revolving door Industry actors hire government officials regulators policymakers or their staff or employees of industry actors leave to work for government officials regulators or policymakers 8226 Strengthen and enforce government ethics policies such as conflict of interest reviews 8226 Fund and provide AI specific training for government ethics offices 8226 Invest in regulator salaries work environments and professional development to make government careers more desirable Information Capture Agenda setting Industry actors emphasize or de emphasize particular perspectives or data set priorities in policy conversations or frame regulatory problems in ways that favor industry actors Regulators may then adopt such views 8212 e g goals norms practices activities and models of risks and markets 8226 Increase access for non industry stakeholders to policy processes particularly at early stages of policy development to address all information capture mechanisms 8226 Consider consumer empowerment programs to enable civic participation to address all information capture mechanisms 8226 Institute reporting and monitoring requirements to raise regulatory visibility and verify industry information to address all information capture mechanisms Information management Industry actors selectively share control access to withhold or provide misleading or false information to policymakers or regulators Information overload Industry actors inundate policymakers or regulators with similar information or communications supporting their points of view which challenges the ability of regulators to process and interpret the information ,Table 0 Mechanisms of industry influence in the policy process cont d on next page ,Industry actors exert influence on policymakers through particular mechanisms to achieve that policy outcome We identify 15 mechanisms through which industry actors can influence policy These mechanisms include advocacy revolving door employees shuttling between industry and government agenda setting cultural capture and other mechanisms as defined in Table 0 Mechanisms of industry influence in the policy process cont d on next page Policy outcomes that arise absent industry influence even those which may benefit industry do not reflect capture To conclude we explore potential measures to mitigate capture Systemic changes are needed to protect the AI governance ecosystem from undue industry influence building technical capacity within governments and civil society e g promoting access requirements providing funding independent of industry and creating public AI infrastructure could be a first step towards building resilience to capture Procedural and institutional safeguards may also be effective against many different types of capture examples include building regulatory capacity in government empowering watchdogs conducting independent review of regulatory rules and forming advisory boards or public advocates Other mitigation measures that are specific to different types of industry influence are outlined in Table 0 Mechanisms of industry influence in the policy process cont d on next page 2 minute read Read Table 0 Mechanisms of industry influence in the policy process cont d on next page in full Browse the interview results in Figure 1 interview responses to the goals of AI regulation Figure 2 interview responses to actors influencing AI policy and Figure 3 interview responses to the mechanisms of industry influence in AI policy 10 minute read Start with the Executive Summary Skip Table 0 Mechanisms of industry influence in the policy process cont d on next page but read through the definitions in Table 2 Finish with Section 6 on preventing and mitigating capture Policymakers Start with the Executive Summary particularly Table 0 Mechanisms of industry influence in the policy process cont d on next page Browse the interview results in Figure 1 interview responses to the goals of AI regulation Figure 2 interview responses to actors influencing AI policy and Figure 3 interview responses to the mechanisms of industry influence in AI policy Then read Section 6 on preventing and mitigating capture 
166,Sx1.T0a, Mechanism Definition Potential Mitigations Cultural Capture Group identity Policymakers or regulators may be 8220 more likely to adopt positions advanced by people whom they perceive as being their in group 8221 Kwak 2013 8226 N A additional research needed Relationship networks Policymakers or regulators may be 8220 more likely to adopt positions advanced by people who are in their social networks 8221 Kwak 2013 Status Policymakers or regulators may be 8220 more likely to adopt positions advanced by people whom they perceive to be of higher status in social economic intellectual or other terms 8221 Kwak 2013 Indirect Capture Academic capture Industry actors influence academic actors who may then influence policymakers or regulators 8226 Provide funding sources independent of industry 8226 Increase non industry career opportunities 8226 Ensure academic access to compute and data resources Private regulator capture Industry actors influence private organizations that serve regulatory functions 8212 e g auditors or standards setting bodies 8226 Provide funding sources independent of industry Public relations Industry actors engage in direct public communications which may then influence policymakers or regulators directly or via shaping public opinion 8226 Increase transparency requirements 8226 Build robust civil society institutions Media capture Industry actors influence journalists or outputs from media channels which may then influence policymakers or regulators directly or via shaping public opinion 8226 Increase transparency requirements 8226 Build robust civil society institutions ,Table 0 Mechanisms of industry influence in the policy process cont d ,
167,S2.T1, Category Examples Policy content 8226 Regulations are too weak to protect the public or are nonexistent Carpenter 2013a Papyshev and Yarime 2022 8226 Regulation creates high barriers to entry that protect market incumbents Stigler 1971 Barrett 2004 8226 Product standards are set to favor particular industry players Berman 2017 8226 Regulations make suboptimal value trade offs between e g safety and justice Guha et 160 al 2024 Wong 2023 Policy enforcement 8226 Policies are not enforced or exceptions are created for particular companies Teachout and Khan 2014 8226 Enforcement is biased toward a subset of firms Mariniello Neven and Padilla 2015 Governance structures 8226 Agencies lack funding to enact or enforce policies Shapiro 2012 Neudert 2023 8226 A lack of uniform rules allows regulated entities to engage in regulatory arbitrage Etzioni 2009 8226 An agency 8217 s dual mandate results in the agency achieving only one mandate Carrigan 2013 Rex 2020 8226 Suboptimal federal policies preempt state policies Carpenter 2013a Policy content Policy enforcement Governance structures ,Table 1 Examples of outcomes of regulatory capture,Resolving the question of which outcomes are properly considered capture of AI policy is beyond the scope of this article because the goals of general purpose AI regulation are contested and sometimes conflicting However discourse about capture in AI policy can benefit from understanding how these models of capture operate We categorize the ways in which policy outcomes can diverge from the public interest as affecting the content of government policies the enforcement of government policies or the institutional structures of regulation Examples of outcomes in each category are outlined in Table 1 
168,S2.T2, Mechanism Definition Examples Personal Engagement Advocacy Industry actors directly participate in formal policy making processes 8212 e g interacting directly with policymakers or regulators to provide information or convince them of a particular point of view Activities include lobbying private meetings speaking events public hearings constituent engagement court filings amicus briefs Procedural obstruction Industry actors intentionally impede policymaker or regulator action through procedural interference An industry actor participates in a standards setting committee and repeatedly stalls the conversation an industry actor files multiple lawsuits against a regulatory agency to prevent enforcement an industry actor files many requests for reconsideration or to otherwise slow down policy enforcement Incentive Shaping Donations gifts and bribes Industry actors make financial contributions to elected officials 8217 campaigns or give personal gifts to elected officials or their staff A company donates money to a Congress member 8217 s campaign a company gives free vacations to an agency staff member Private threats Industry actors make explicit or implicit threats of litigatory reputational or other negative consequences to prevent policy enactment or enforcement Regulators decline to investigate a company because they believe that the company would sue a policymaker stops advocating for a policy because they are wary of negative press coverage a company threatens to release material that would portray a policymaker in a negative light Revolving door Industry actors hire government officials regulators policymakers or their staff or employees of industry actors leave to work for government officials regulators or policymakers A company hires a legislator 8217 s chief of staff a general counsel of a company is nominated for a political appointment Information Capture Agenda Setting Industry actors emphasize or de emphasize particular perspectives or data set priorities in policy conversations or frame regulatory problems in ways that favor industry actors Regulators may then adopt such views 8212 e g goals norms practices activities and models of risks and markets A company frames industry regulation as a question of regulating downstream users or upstream producers but not of the company itself a company tells policymakers or regulators that particular policy goals are more important than other ones when thinking about regulating the industry which then results in industry biased regulation many companies coordinate to repeat the same message to policymakers so that policymakers perceive a 8220 united front 8221 of industry voices on a particular issue Information management Industry actors selectively share control access to withhold or provide misleading or false information to policymakers or regulators A company fails to report important information about its product or business practices to regulators policymakers a company makes a presentation to a policymaker regulator in which they highlight the benefits of their technology but fail to discuss its risks Information overload Industry actors inundate policymakers or regulators with similar information or communications supporting their points of view which challenges the ability of regulators to process and interpret the information Industry actors organize a comment submission drive and overwhelm the notice and comment process with comments favorable to the industry position industry actors send or organize a barrage of phone calls letters or other communications to a policymaker to create the illusion of support for their position ,Table 2 Mechanisms of industry influence in the policy process cont d on next page ,10 minute read Start with the Executive Summary Skip Table 0 Mechanisms of industry influence in the policy process cont d on next page but read through the definitions in Table 2 Finish with Section 6 on preventing and mitigating capture Industry actors can seek the outcomes outlined above by exerting influence on the policy process capture occurs when these actors succeed Therefore policy failures absent corporate involvement are not capture and neither is corporate influence that does not result in an outcome contravening the public interest We distinguish between direct and indirect mechanisms of capture and definitions for all mechanisms are provided in Table 2 The interview protocol was designed to elicit information about industry actors preferred policy outcomes that could constitute capture as well as what mechanisms of influence industry actors are currently using to facilitate those outcomes Interviewees were first asked about the public interest goals of AI regulation the types of industry actors involved in AI policy and the policy goals of those actors The first author then presented interviewees with a table of influence mechanisms an early version of Table 2 asked interviewees to list any additional mechanisms of influence and asked which mechanisms were currently most relevant to AI policy Where interviewees indicated that industry actors used a specific mechanism to influence policy they asked follow up questions asking for examples of such influence what mitigation measures were in place to curb it whether and why those measures were effective and whether similar dynamics existed in other industries The full interview protocol is contained in Appendix A The first author transcribed the interviews with the assistance of a private OpenAI Whisper instance then de identified the interview transcripts following Saunders Kitzinger and Kitzinger 2015 and Saunders Kitzinger and Kitzinger 2015 and Stam and Diaz 2023 In preliminary analysis the first and second authors deductively developed codes for the goals of AI regulation the types of actors involved in AI regulation and the mechanisms through which industry actors influenced the policy process The third and fourth authors then independently coded the de identified transcripts and the first and second authors subsequently refined the codes and adjudicated any disagreements in coding using an open discussion method Stam and Diaz 2023 In preliminary analysis the first and second authors deductively developed codes for the goals of AI regulation the types of actors involved in AI regulation and the mechanisms through which industry actors influenced the policy process The third and fourth authors then independently coded the de identified transcripts and the first and second authors subsequently refined the codes and adjudicated any disagreements in coding using an open discussion method Chinh et 160 al 2019 The final coding manual for influence mechanisms was substantially similar to the table presented in Table Chinh et al 2019 The final coding manual for influence mechanisms was substantially similar to the table presented in Table 2 Agreement between coders was very high 10 10 10 1010We do not provide quantitative inter rater reliability metrics as ease of coding was relatively high McDonald Schoenebeck and Forte 2019 and as there were few disagreements between coders McDonald Schoenebeck and Forte 2019 and as there were few disagreements between coders Figure 3 displays the influence mechanisms from Section 2 2 along with the number of interviews in which experts indicated each mechanism to be important in AI policy Extended definitions and examples are in Table 2 Our discussion proceeds according to these categories of mechanisms We then presented interviewees with a version of Table 2 19 19 19 1919This table was adapted throughout our interview process with new examples clarifications and mechanisms based on previous interviewee questions and suggestions We asked a set of questions about mechanisms of industry influence in AI policy Which of the mechanisms listed in Table 2 are currently most relevant to AI policy to your knowledge These could be mechanisms that industry is currently using or likely to use in the future to influence policy 
169,S2.T2a, Mechanism Definition Examples Cultural Capture Group identity Policymakers or regulators may be 8220 more likely to adopt positions advanced by people whom they perceive as being their in group 8221 Kwak 2013 A regulator was formerly a business executive and identifies with employees in that industry as people of the same trade a company sends a lobbyist of the same gender and ethnic background as a legislator to speak to them in the hopes that the legislator would be more sympathetic Relationship networks Policymakers or regulators may be 8220 more likely to adopt positions advanced by people who are in their social networks 8221 Kwak 2013 A legislator has a relative who works in a particular industry and the legislator adopts their relative 8217 s views about regulating that industry after speaking to them a regulator regularly plays golf with trade association executives and the regulator begins to adopt industry friendly views after discussing policy issues with those executives Status Policymakers or regulators may be 8220 more likely to adopt positions advanced by people whom they perceive to be of higher status in social economic intellectual or other terms 8221 Kwak 2013 A policymaker adopts the views of a someone testifying at a hearing because of their status as a technical expert a legislator wishes to associate with CEOs in an industry that many people consider 8220 hot 8221 and 8220 the next big thing 8221 and adopts industry friendly views as a result Indirect Capture Academic Capture Industry actors influence academic actors who may then influence policymakers or regulators A company funds an academic 8217 s research a company donates a large sum to a think tank a company donates to a university to set up a research lab Private regulator capture Industry actors influence private organizations that serve regulatory functions 8212 e g auditors or standards setting bodies A company actively participates in standards setting and the standards are then adopted by regulators a company develops a close relationship with its auditors leading to ineffective audits Public relations Industry actors engage in direct public communications which may then influence policymakers or regulators directly or via shaping public opinion A company puts out a press release or runs an advertising campaign supporting or opposing a regulation Media Capture Industry actors influence journalists or outputs from media channels which may then influence policymakers or regulators directly or via shaping public opinion A company puts out paid media pieces advocating for its policy stances ,Table 2 Mechanisms of industry influence in the policy process cont d ,
170,S3.T3, ID Type Role R1 Government A congressional staffer R2 Government A former congressional staffer R3 Academia research An academic in a university studying technology policy R4 Academia research An academic who has worked on AI in different sectors R5 Academia research An AI ethics researcher R6 Academia research A policy analyst working on technology issues R7 Academia research An expert at a technical research organization R8 Civil society An executive at a US advocacy group that works on technology issues R9 Civil society An executive at a US think tank working on technology policy issues R10 Civil society An expert at a US think tank R11 Civil society An economist at a US think tank R12 Civil society A leader at a US think tank working on technology policy issues R13 Civil society Grantmakers at a philanthropic foundation focused on technology R14 EU civil society A researcher at a technology policy think tank R15 EU civil society A policy executive at a think tank R16 EU civil society An employee of an EU think tank working on technology policy R17 UK civil society A policy expert at a UK think tank ,Table 3 Overview of interviews with interview IDs and descriptions of experts roles,Personal engagement Industry actors directly participate in formal policy making processes Mechanisms in this category are advocacy Barkow 2010 Etzioni 2009 de 160 Figueiredo and Richter 2014 Godwin Ainsworth and Godwin 2013 and Barkow 2010 Etzioni 2009 de Figueiredo and Richter 2014 Godwin Ainsworth and Godwin 2013 and procedural obstruction R16 4 4 4 These citations indicate expert interviews See Table 3 R16 4 4 4 44These citations indicate expert interviews See Table 3 We conducted 17 semi structured interviews with AI policy experts 7 7 7 77We conducted one group interview so we spoke with more than 17 experts in total Group interviewees were not assigned distinct IDs in Table 3 An expert interview method is appropriate for our research questions because most information about industry influence is non public Additionally political influence and regulatory capture in particular is difficult to measure quantitatively and our data captures many informal interactions and processes Soest 2023 Interviews were anonymous to enable more in depth conversations about the policy process The study protocol was approved by the Human Subjects Protection Committee at RAND Soest 2023 Interviews were anonymous to enable more in depth conversations about the policy process The study protocol was approved by the Human Subjects Protection Committee at RAND We primarily recruited experts located in the United States but also included some based in the United Kingdom and the European Union To maintain sample diversity and reduce bias we purposively invited experts with diverse demographic organizational and ideological backgrounds We did not contact any experts currently affiliated with companies that develop general purpose AI models Experts backgrounds are described in Table 3 8 8 8 88Given the small size of our population of interest demographics are not reported to protect interviewee confidentiality All interviews were conducted online by the first author in January February 2024 and lasted 40 60 minutes each At the beginning of each interview the first author described to interviewees the goals of this study and detailed our practices for protecting interviewee confidentiality They then verbally re obtained consent to record the interviews to use the descriptions reported in Table 3 and to use transcribed quotes 9 9 9 99When interviewees did not consent to recording analysis was performed with the first author s contemporaneous interview notes 
171,S4.T4, Actor Definition AI deployers Companies that are deploying AI products or services AI developers Companies that are building AI models or products Cloud companies Companies that provide virtual computing services but do not primarily manufacture hardware Data input providers Companies that provide datasets data labeling services or technical infrastructure or services to AI developers Hardware producers Companies that produce computing hardware Hired lobbyists External lobbyists paid to advocate on behalf of industry actors listed elsewhere in this table Industry groups Membership organizations consisting of industry actors listed elsewhere in this table Venture capital Financial firms that provide funding to early stage AI developers or deployers ,Table 4 Definitions for industry actors participating in the policy process,Similarly interviewees identified many actors in the AI industry who are participating in the policy process Figure 2 presents a frequency chart of the number of interviews that mentioned each type of actor as participating in AI policy Any mentions of specific companies or industry actors were re coded into one of the categories below Actor categories definitions and examples are presented in Table 4 
172,A2.T5, AI Governance Keywords Capture Keywords 8220 artificial intelligence 8221 8220 regulatory capture 8221 8220 AI governance 8221 8220 industry capture 8221 8220 AI policy 8221 8220 agency capture 8221 8220 AI ethics 8221 8220 corporate capture 8221 ,Table 5 Search terms for scoping review,Using the search terms in Table 5 we queried the ACM Digital Library IEEE Xplore arXiv 20 20 20 2020Although preprints on arXiv are not peer reviewed a not insignificant proportion of academic literature appears on arXiv before formal publication as do various articles in the gray literature e g reports by think tanks or advocacy institutions and Google Scholar for articles containing terms in Table 5 Searches were conducted in October 2023 and again in January February 2024 Both articles in the academic and gray literature were included Using the search terms in Table 5 we queried the ACM Digital Library IEEE Xplore arXiv 20 20 20 2020Although preprints on arXiv are not peer reviewed a not insignificant proportion of academic literature appears on arXiv before formal publication as do various articles in the gray literature e g reports by think tanks or advocacy institutions and Google Scholar for articles containing terms in Table 5 Searches were conducted in October 2023 and again in January February 2024 Both articles in the academic and gray literature were included The search string was constructed to return any article that contained an exact match of any one term from both columns in Table 5 In other words articles needed to contain one term related to AI policy and one term related to regulatory capture All searches were full text so the terms could be contained anywhere in the text of the article or in any metadata field e g title abstract 
173,A2.T6, Inclusion Criteria The article describes influence related to AI policy or regulation The article describes influence exerted on policymakers or on an entity that may influence policymakers ,Table 6 Inclusion criteria for scoping review,The search returned n 255 119899 255 n 255 italic n 255 unique articles in the English language The second author filtered these results by reading the titles and abstracts of all articles Filtering was conducted using the inclusion and exclusion criteria in Tables n 255 119899 255 n 255 italic n 255 n 255 n n 255 255 119899 255 119899 255 119899 n 255 255 n 255 n 255 italic n 255 italic n 255 unique articles in the English language The second author filtered these results by reading the titles and abstracts of all articles Filtering was conducted using the inclusion and exclusion criteria in Tables 6 and 7 
174,A2.T7, Exclusion Criteria The article only discusses influence by non industry or non corporate actors The article includes neither a mechanism nor a concrete outcome of capture The article is a thesis or dissertation ,Table 7 Exclusion criteria for scoping review,The search returned n 255 119899 255 n 255 italic n 255 unique articles in the English language The second author filtered these results by reading the titles and abstracts of all articles Filtering was conducted using the inclusion and exclusion criteria in Tables n 255 119899 255 n 255 italic n 255 n 255 n n 255 255 119899 255 119899 255 119899 n 255 255 n 255 n 255 italic n 255 italic n 255 unique articles in the English language The second author filtered these results by reading the titles and abstracts of all articles Filtering was conducted using the inclusion and exclusion criteria in Tables 6 and 7 
175,A2.T8, Articles Included n 120 119899 120 n 120 italic n 120 Abdu Pasquetto and Jacobs 2023 Abebe et 160 al 2022 AI Governance Alliance 2024 Alaga and Schuett 2023 Allen 2019 Almada and Petit 2023 Anderljung et 160 al 2023a Attard Frost and Widder 2023 Badran 2021 Bajohr 2023 Bannerman et 160 al 2020 Bender and Grimsson 160 II 2024 Berman Goyal and Madaio 2024 Bova Stefano and Han 2024 Brandusescu 2021 Broughel 2023 Browne Drage and McInerney 2024 Brynjolfsson and Ng 2023 Bryson and Malikova 2021 Bryson 2020 Casper et 160 al 2024 Cath and Keyes 2022 Chan et 160 al 2024 Chan Bradley and Rajkumar 2023 Chesterman 2021b Chilson and Rinehart 2024 Chomanski 2021 Cihon Maas and Kemp 2020a b Clarke and Whittlestone 2022 Cui et 160 al 2024 de 160 Laat 2021 Vries Kanevskaia and Jager 2023 Dempsey et 160 al 2024 Derczynski et 160 al 2023 Dickens 2021 Ebers 2022 Egan and Heim 2023 Erman and Furendal 2024 Evans et 160 al 2021 Fagleman Griffiths and Mcateer 2023 Frazier 2023 Friedman et 160 al 2022 Gaske 2023b Gazendam and Dawson 2023 Gilbert et 160 al 2022 Giraudo Fosch Villaronga and Malgieri 2023 Goanta et 160 al 2023 Gornet 2023 Greenleaf Clarke and Lindsay 2019 Guha et 160 al 2024 Guihot Matthew and Suzor 2017 Haataja and Bryson 2022 Hacker 2023 Hadfield and Clark 2023 Himmelreich and Lim 2022 Hu 2021 Hua and Belfield 2023 Jiang et 160 al 2023 Jing Berger and Becerra 160 Sandoval 2023 Katyal 2022 Khan 2023 Koene et 160 al 2019 Kolt 2023 Lam et 160 al 2024 Laux Wachter and Mittelstadt 2021 Leslie et 160 al 2022 L 233 vesque 2021 Liesenfeld Lopez and Dingemanse 2023 Luetz 2023 Lupo 2023 Marcus 2023a Margulies 2023 M 252 gge 2023 Narayanan and Tan 2023 Nemitz 2023 O 8217 Shaughnessy et 160 al 2023 Ochigame 2019 Papyshev and Yarime 2022 Paul 2022 Peng Lin and Streinz 2021 Pizzi Romanoff and Engelhardt 2020 Png 2022 Raji Costanza Chock and Buolamwini 2023 Ramdas 2022 Roberts et 160 al 2021 2023 Sanchez Graells 2024b 2023d 2023c 2023a 2023b Sarel 2023 Sch 228 ferling 2023 SRI 2023 Seger et 160 al 2023 Selbst 2021 Solow Niederman 2019 Straub et 160 al 2023 Stuurman and Lachaud 2022 Taeihagh 2021 Tafani 2022 Tartaro 2023a b Taylor and Dencik 2020 Th 246 nnes et 160 al 2023 Timcke 2023 Trager et 160 al 2023 Veale 2020 Veale Matus and Gorwa 2023 Vipra and Korinek 2023 Weil 2024 Westgarth et 160 al 2022 Whittaker 2021 Widder West and Whittaker 2023 Won 2021 W 246 rsd 246 rfer 2023 Wouters 2022 Vallor 2022 Young Katell and Krafft 2022 Excluded n 135 119899 135 n 135 italic n 135 Abdalla et 160 al 2023 Abou Zeid Bayingana and Amazouz 2022 Ajena et 160 al 2022 Barabas 2023 Baumberger 2023 Bechara et 160 al 2021 Bedford et 160 al 2022 Bennett 2023 Bietti 2023 Boffel 2023 Brandusescu and Sieber 2023 Bremmer and Suleyman 2023 Brownsword 2019 Carlizzi and Quattrone 2023 Carter 2023 Cebulla 2023 Chan Papyshev and Yarime 2022 Charisi and Dignum 2024 Charlesworth 2021 Charlesworth et 160 al 2023 Chauhan 2023 Chesterman 2021a 2023 Chinen 2023 Cohen and Jackson 2019 Correa et 160 al 2023 Couldry and Mejias 2019 Critch and Russell 2023 Cu 233 llar and Huq 2022 Dancy and Workman 2023 Edwards 2022 Eliot and Murakami 160 Wood 2022 Fahey 2022b a Fenwick and Vermeulen 2020a b 2021 Findlay and Seah 2020 Findlay et 160 al 2022 Findlay Seah and Wong 2023 Ford and Clifford 2021 Fraser and Bello 160 y Villarino 2023 Gans 2024 Gantzias 2021 Gaske 2023a Gegenhuber et 160 al 2022 Geiger et 160 al 2023 Gilbert 2021 Goodlad 2023 Goodman Gerstel and Risberg 2019 Gottardo 2023 Gurumurthy and Bharthur 2019 Hacohen 2022 Hawking 2021 Hermstr 252 wer and Langenbach 2023 Hilty Hoffmann and Scheuerer 2020 Himmelreich 2023 Ho Marcus and Ray 2021 Huang and Ma 2023 Iliadis and Ford 2023 Ilie and Welch 2014 Kaplan 2008 Keller and Magalh 227 es 2023 Killian 2021 Klaessig 2021 Kleizen 2020 Knaack 2022 Konya et 160 al 2023 Ku 378 niacki et 160 al 2022 Larsen 2022 Lee Hilty and Liu 2021 Lie 2023 Lin and Jackson 2023 London and Danks 2018 McGraw and Mandl 2021 McInerney and Drage 2024 Meghani 2021 Mehmood Naseer and Chen 2024 Me 223 mer and Degeling 2023 Moberg and Gill Pedro 2024 Moitra et 160 al 2022 Neudert 2023 Neumann 2018 Neves 2023 Olteanu et 160 al 2023 Ong 2024 Opoku 2019 Outeda and Cacheda 2023 Pacione and Teixeira 2023 Papyshev and Yarime 2023 Pavel et 160 al 2022 Pavlopoulou 2022 Plantinga et 160 al 2024 Polishchuk 2023 Rainie Anderson and Vogels 2021 Rawat Prerna and Singh 2024 Ren 2022 Roberts et 160 al 2023 Sanchez Graells 2024a Sandoval et 160 al 2023 Sastry et 160 al 2024 Sengupta 2022 Scherz 2024 Shneiderman 2020 Shrier and Pentland 2022 Sifat 2023 Sitaraman and Eyre 2023 Sivan Sevilla and Sharvit 2021 Southgate et 160 al 2019 Southgate 2021 Stahl et 160 al 2022 Steed and Acquisti 2024 Stix and Maas 2021 Stockbauer 2021 Sun and Guo 2013 Tambini 2021 2023 Taylor 2021 Thierer and Haaland 2021 Timmers 2021 Turner 2019 2018 van 160 der Merwe and Al 160 Achkar 2022 Varoglu Gokten and Ozdogan 2021 Verma 2023 Vescent and Blakley 2018 Viljoen 2021 Wedam 2023 Widder et 160 al 2023 Xenidis 2024 Yaghmai 2021 Yang et 160 al 2024 Zhang Ong and Findlay 2023 Zumbansen 2022 2023 ,Table 8 A complete list of the 255 articles resulting from our search including the final 120 articles in our review,A list of the articles remaining after filtering based on the inclusion and exclusion criteria may be found in Table 8 Note that our method led us to be over inclusive about which articles remained in the final review As long as any part of the text satisfied the inclusion criteria we included the article in our final review Oftentimes the included articles contained only a few sentences to a paragraph of relevant discussion 
